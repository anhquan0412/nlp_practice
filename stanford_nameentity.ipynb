{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To FIND and CLASSIFY names in text\n",
    "\n",
    "![](images/ner_intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hard task because\n",
    "- An official name can sound like typical words: \"First National Bank\", \"Future School\"\n",
    "- Hard to determine class of entity: \"Charles Schwab\" can be referred to a person or an organization\n",
    "\n",
    "=> ambiguous, dependent on context\n",
    "\n",
    "Overall, single word classification is always hard, since words can have double meaning, such as \"sanction\" or \"Paris\" (Paris city, or Paris Hilton?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a window containing neighboring words to classify the center word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Use word2vector to convert each word in a FIXED SIZE window to a vector\n",
    "- Concatenate those vectors. x_window will be a vector of size (windows size * word vector length,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ner_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classifier with unnormalized score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still use a fixed window and concat vector x_window size size (windows size * word vector length,)\n",
    "\n",
    "Task: classify whether center word is a location or not (binary classification)\n",
    "- Will go over all positions in corpus, but only correct position will get high score. Correct position = position that has actual NER Location at center\n",
    "\n",
    "E.g.: Not all museums in Paris are amazing\n",
    "- One true window: 'museums in **Paris** are amazing'\n",
    "- Wrong window: 'all museums **in** Paris are'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net feed forward for this architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming word-vector-length is 4. We will have 3-layer neural net as described below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ner_neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a is R(8x1). Output of s is R(2x1), or binary outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons of updating word vectors using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are doing binary classification for movie review for example, and feeding a pre-trained w2v to start with, some words representation will be updated (thanks to SGD) in a way that break the original word relationship\n",
    "- Words like: \"TV\",\"telly\",\"television\" are close to each other, but if 'television' is in test set, it will be further away from 'TV' and 'telly' in train set which got updated. Things will got worst if TV and telly are key words to determine negativity of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ner_updatew2v_or_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- use transfer learning (w2v pre-trained?) Yes, but if we already have big corpuses around +100 millions of words of data (probably from machine translation tasks), then I guess it's fine to start with random initialization of word vectors\n",
    "\n",
    "about fine-tuning w2v pre-trained: \n",
    "- if small data (~ 100k words) => don't, since small data is easy to overfit, can't generalize well\n",
    "- large dataset (>1 mil words): yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
