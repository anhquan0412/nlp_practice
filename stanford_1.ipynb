{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2&t=2433s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this course: to represent the meaning of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet library is an okay to process words using computers:\n",
    "    - extract synonym sets and hypernyms ('is a; relationships)\n",
    "- Wordnet however requires human labor to create and adapt => lot of missing words, some incorrect word synonyms or interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag-of-words (one-hot representation) cannot capture similarity between words (since each word vector are orthogonal, aka dot product is 0, aka different position of '1' => no natural notion of similarity\n",
    "- WordNet can help, but it has its cons (see above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of all the cons above, **vector needs to learn to encode the similarity themselves**\n",
    "\n",
    "- Word meaning is given by its context: other words that frequently appear close-by (position before and after)\n",
    "- With this, you can represent a word using a vector with positive/negative values(not one-hot), aka word vectors, aka word embeddings. Length of these vectors can go from 50 to 300, depends on the computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "- for each word in a text (center word c), look at the context words (o) around c.\n",
    "- Calculate **P(o|c)** using **word vectors' similarity** for c and o\n",
    "- Try to maximize this probability, aka **by adjusting word vector matrix** => we need to keep updating this matrix using some metric/loss (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/word2vec.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
