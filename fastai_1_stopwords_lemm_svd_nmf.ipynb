{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych /home/quantran/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38816\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0],newsgroups_train.filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# although we only have 4 topics, we want to see what the model will split out if we adjust # of topics\n",
    "num_of_topics = 6\n",
    "num_top_words=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "Is going out of style. Only suitable when you have a very simple model and want to focus more on important words, so you sacrifice the need for stop words.\n",
    "\n",
    "Neural network, on the other hand, can handle stop words, and you should include stop words for language model to capture all the information it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both involve generating the ROOT form of the word\n",
    "\n",
    "- Lemmatization: apply language rules. Result tokens are actual words, e.g. foot and footing are 2 different words, but feet and foot are 1 word (no prob)\n",
    "- Stemming: just chop of the end of the words. Faster. Result tokens are not words. E.g. foot and footing become 1 word: foot, but foot and feet are 2 different words (b/c of different endings). Universe and university will become 1 word (which should not be!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list= [\"fly\", \"flies\", \"flying\",\n",
    "            \"organize\", \"organizes\", \"organizing\",\n",
    "            \"universe\", \"university\",\n",
    "           \"foot\",\"feet\",\"footing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly',\n",
       " 'flies',\n",
       " 'flying',\n",
       " 'organize',\n",
       " 'organizes',\n",
       " 'organizing',\n",
       " 'universe',\n",
       " 'university',\n",
       " 'foot',\n",
       " 'feet',\n",
       " 'footing']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lookup(word) for word in word_list]\n",
    "# spacy lemmatizer aint do shit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data).todense() \n",
    "# (documents, vocab)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26576,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brow', 'brown', 'browning', 'browns', 'browse', 'browsing',\n",
       "       'bruce', 'bruces', 'bruise', 'bruised'], dtype='<U80')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[5000:5010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/svd_fb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is exact decomposition => one solution for the other 3 matrices\n",
    "\n",
    "Columns of U (each value of column is for each doc) and rows of Vt (each value of row is for each vocab) are **orthonormal** (perpendicular aka each dot product is 0 AND each vectors are unit vectors aka length = 1)\n",
    "\n",
    "Note that set of orthonormal vectors are **linearly independent**, meaning no vector of the set can be calculated using other vectors in that same set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vocab = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full SVD, both U and V are square matrices, where the extra columns in U (or V, depends on your setup) form an orthonormal basis (but zero out when multiplied by extra rows of zeros in S).\n",
    "\n",
    "Those made-up columns don't come from the original matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 2.32 s, total: 2min 41s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%time U, s, Vh = linalg.svd(doc_vocab, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 2034) (2034,) (26576, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape, s.shape, Vh.shape) # every matrix is square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Selection_033.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 1.13 s, total: 37.4 s\n",
      "Wall time: 9.96 s\n"
     ]
    }
   ],
   "source": [
    "%time U, s, Vh = linalg.svd(doc_vocab, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 2034) (2034,) (2034, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 2034)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(s).shape # to be consistent with the graph above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U: doc vs \"topics\" (# of topics is automatically generated in this case, aka unsupervised)\n",
    "- s: diagonal matrix showing important value of each topic\n",
    "- Vh: topics vs vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([433.92698542, 291.51012741, 240.71137677, 220.00048043])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:4] # diagonal matrix showing important values in DESCENDING ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([433.92698542, 291.51012741, 240.71137677, 220.00048043])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.diag(s[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the 3 matrices are the decomposition of the main one\n",
    "temp = U@ np.diag(s) @ Vh\n",
    "np.allclose(temp,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check U and V are orthonormal\n",
    "\n",
    "np.allclose(U@U.T, np.eye(U.shape[0]))\n",
    "# because U dot itself is 1 b/c unit vector, \n",
    "# and U dot other vectors is 0 due to being perpendicular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(Vh@Vh.T, np.eye(Vh.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26576, 2034), (2034, 26576))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vh.T.shape,Vh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwell into 'topics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Vh is topics vs vocab matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00940972, -0.0114532 , -0.00002169, ..., -0.00000572,\n",
       "        -0.00001144, -0.00109243],\n",
       "       [-0.00356688, -0.01769167, -0.00003045, ..., -0.00000773,\n",
       "        -0.00001546, -0.0018549 ],\n",
       "       [ 0.00094971, -0.02282845, -0.00002339, ..., -0.0000122 ,\n",
       "        -0.0000244 ,  0.00150538],\n",
       "       ...,\n",
       "       [-0.00218087, -0.04322025, -0.00012552, ...,  0.00003759,\n",
       "         0.00007518,  0.00160907],\n",
       "       [-0.00039196,  0.00494894,  0.00000309, ..., -0.00001321,\n",
       "        -0.00002643, -0.00015038],\n",
       "       [ 0.00306552, -0.01437264, -0.00000405, ..., -0.00003597,\n",
       "        -0.00007193,  0.00056218]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vh[:10] # vocab value of first 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_top_words=8\n",
    "\n",
    "def show_topics(a):\n",
    "    # get indices of 8 highest vocab value for each topic, and get the words associated with each index\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = [top_words(t) for t in a]\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: critus ditto propagandist surname galacticentric kindergarten surreal imaginative\n",
      "Topic 1: jpeg gif file color quality image jfif format\n",
      "Topic 2: graphics edu pub mail 128 3d ray ftp\n",
      "Topic 3: jesus god matthew people atheists atheism does graphics\n",
      "Topic 4: image data processing analysis software available tools display\n",
      "Topic 5: god atheists atheism religious believe religion argument true\n",
      "Topic 6: space nasa lunar mars probe moon missions probes\n",
      "Topic 7: image probe surface lunar mars probes moon orbit\n",
      "Topic 8: argument fallacy conclusion example true ad argumentum premises\n",
      "Topic 9: space larson image theory universe physical nasa material\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(show_topics(Vh[:10])): \n",
    "    # 8 highest vocab value (from Vh) for each of 10 topics\n",
    "    print(f'Topic {i}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's unsupervised, now we get an idea of what each topic is based on the vocabulary associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to SVD, but instead of constraining our vetors to be orthogonal, we will constrain them to be **non-negative** (note that SVD allows negative value in its matrices, and **we are not sure how to interpret them**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is a factorization of a non-negative data set $V$:\n",
    "\n",
    "$V$ = $W$$H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **SVD is exact factorizaion, NMF is NON-exact => many factorization solution** (variations are based on different constraints on NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nmf_doc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,n=doc_vocab.shape\n",
    "d=5  # num topics\n",
    "m,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=d, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(doc_vocab) # doc vs topic\n",
    "H1 = clf.components_ # topic vs vocab (or words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034, 5), (5, 26576))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape,H1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image gif file color images format quality',\n",
       " 'edu graphics pub mail 128 ray ftp send',\n",
       " 'space launch satellite nasa commercial satellites year market',\n",
       " 'jesus god people matthew atheists does atheism said',\n",
       " 'image data available software processing ftp edu analysis']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and using NMF on TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF = (# occurrences of term t in document) / (# of words in documents)\n",
    "\n",
    "IDF = log(# of documents / # documents with term t in it)\n",
    "\n",
    "TF-IDF = TF * IDF, and it will return a matrix of doc vs vocab\n",
    "\n",
    "=> the vocab will be important to a document if it appears several time in that document, but rarely appears in other documents\n",
    "\n",
    "\n",
    "This is a better version of bag-of-words. But similarly to BOW, it doesn't take into acount order of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data) # (documents, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_ # topic vs vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people don think just like objective say morality',\n",
       " 'graphics thanks files image file program windows know',\n",
       " 'space nasa launch shuttle orbit moon lunar earth',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens',\n",
       " 'god jesus bible believe christian atheism does belief']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the normal SVD spit out too many topics. In NMF you can adjust the # of topics so it will only calculate the subset of topics you are interested in.\n",
    "\n",
    "Truncated SVD will try to achieve what NMF does: We are just interested in the vectors corresponding to the largest singular values (from the diagonal matrix) so we will throw away smallest singular values, effectively throwing away rows and columns of the other 2 matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cons of using classical decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrices are sometimes too big\n",
    "- Input data are often inaccurate/missing => limit the precision and quality of the matrices\n",
    "- Expensive computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.63 s, sys: 586 ms, total: 6.22 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time u, s, v = decomposition.randomized_svd(doc_vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034, 10), (10,), (10, 26576))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape,s.shape,v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg image edu file graphics images gif data',\n",
       " 'jpeg gif file color quality image jfif format',\n",
       " 'space jesus launch god people satellite matthew atheists',\n",
       " 'jesus god matthew people atheists atheism does graphics',\n",
       " 'image data processing analysis software available tools display',\n",
       " 'jesus matthew prophecy messiah psalm isaiah david said',\n",
       " 'launch commercial satellite market image services satellites launches',\n",
       " 'data available nasa ftp grass anonymous contact gov',\n",
       " 'argument fallacy conclusion example true ad argumentum premises',\n",
       " 'probe data surface moon mars probes lunar launch']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(v[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on randomized SVD, check out my PyBay 2017 talk: https://www.youtube.com/watch?v=7i6kBz1kZ-A&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=7\n",
    "\n",
    "For significantly more on randomized SVD, check out the Computational Linear Algebra course: https://github.com/fastai/numerical-linear-algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Randomized SVD matches error rate of SVD with faster runtime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
