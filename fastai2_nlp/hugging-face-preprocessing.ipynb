{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-layer, 768-hidden, 12-heads, 117M parameters.\n",
    "# OpenAI GPT-2 English model\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 281, 1672, 286, 2420, 11, 428, 318, 1194, 1672, 286, 2420, 13, 1058, 828, 1058, 14]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode('This is an example of text, this is another example of text. :), :/')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example of text, this is another example of text. :), :/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ' is', ' an', ' example', ' of', ' text', ',', ' this', ' is', ' another', ' example', ' of', ' text', '.', ' :', '),', ' :', '/']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode([i]) for i in ids])\n",
    "# ',' and ', ' are tokenized differently. No decoding for emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace preprocessing (tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Hello I'm a single sentence\",\n",
    "                    \"And anot`her sentence\",\n",
    "                    \"And the very very last one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[15496, 314, 1101, 257, 2060, 6827], [1870, 1194, 6827], [1870, 262, 845, 845, 938, 530]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "batch = tmp_token(text)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827],\n",
      "        [ 1870,  1194,  6827, 50256, 50256, 50256],\n",
      "        [ 1870,   262,   845,   845,   938,   530]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(text,padding=True,truncation=True,max_length=100,return_tensors=\"pt\")\n",
    "# with padding. Default GPT2 padding is to the right\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I'm a single sentence\n",
      "And another sentence<|endoftext|><|endoftext|><|endoftext|>\n",
      "And the very very last one\n"
     ]
    }
   ],
   "source": [
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257],\n",
      "        [ 1870,  1194,  6827, 50256],\n",
      "        [ 1870,   262,   845,   845]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]])}\n",
      "Hello I'm a\n",
      "And another sentence<|endoftext|>\n",
      "And the very very\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(text,padding=True,truncation=True,max_length=4,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "# truncation is also to the right\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pair of sentences (useful for BERT), but we will play around with truncation and max_length here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
    "                             \"And I should be encoded with the second sentence\",\n",
    "                             \"And I go with the very last one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827,    40,  1101,   257,  6827,\n",
      "           326,  2925,   351,   262,   717,  6827],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827, 50256, 50256, 50256, 50256],\n",
      "        [ 1870,   262,   845,   845,   938,   530,  1870,   314,   467,   351,\n",
      "           262,   845,   938,   530, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "torch.Size([3, 16])\n",
      "Hello I'm a single sentenceI'm a sentence that goes with the first sentence\n",
      "And another sentenceAnd I should be encoded with the second sentence<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "And the very very last oneAnd I go with the very last one<|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,    40,  1101,   257,  6827,   326,  2925,   351,   262,\n",
      "           717,  6827],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827],\n",
      "        [ 1870,   262,   845,   845,  1870,   314,   467,   351,   262,   845,\n",
      "           938,   530]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([3, 12])\n",
      "Hello II'm a sentence that goes with the first sentence\n",
      "And another sentenceAnd I should be encoded with the second sentence\n",
      "And the very veryAnd I go with the very last one\n",
      "torch.Size([3, 12])\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,truncation='only_first',max_length=12,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "# truncate only the first sentence. Still truncate from the right\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))\n",
    "\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827,    40,  1101,   257,  6827,\n",
      "           326,  2925],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827],\n",
      "        [ 1870,   262,   845,   845,   938,   530,  1870,   314,   467,   351,\n",
      "           262,   845]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([3, 12])\n",
      "Hello I'm a single sentenceI'm a sentence that goes\n",
      "And another sentenceAnd I should be encoded with the second sentence\n",
      "And the very very last oneAnd I go with the very\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,truncation='only_second',max_length=12,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "# truncate only the second sentence. Still truncate from the right\n",
    "\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827,    40,  1101,   257,  6827,\n",
      "           326,  2925],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827],\n",
      "        [ 1870,   262,   845,   845,   938,   530,  1870,   314,   467,   351,\n",
      "           262,   845]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([3, 12])\n",
      "Hello I'm a single sentenceI'm a sentence that goes\n",
      "And another sentenceAnd I should be encoded with the second sentence\n",
      "And the very very last oneAnd I go with the very\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,truncation='longest_first',max_length=12,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "# truncate the longest sentence of the two. Still truncate from the right\n",
    "\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also work with pre-tokenized inputs (where sentence has already split into words), good for NER or POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[18435, 314, 1101, 257, 2060, 6827, 314, 1101, 257, 6827, 326, 2925, 351, 262, 717, 6827], [843, 1194, 6827, 843, 314, 815, 307, 30240, 351, 262, 1218, 6827], [843, 262, 845, 845, 938, 530, 843, 314, 467, 351, 262, 845, 938, 530]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      " Hello I'm a single sentence I'm a sentence that goes with the first sentence\n",
      " And another sentence And I should be encoded with the second sentence\n",
      " And the very very last one And I go with the very last one\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights,add_prefix_space=True)\n",
    "\n",
    "batch_sentences = [[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],\n",
    "                   [\"And\", \"another\", \"sentence\"],\n",
    "                   [\"And\", \"the\", \"very\", \"very\", \"last\", \"one\"]]\n",
    "batch_of_second_sentences = [[\"I'm\", \"a\", \"sentence\", \"that\", \"goes\", \"with\", \"the\", \"first\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"should\", \"be\", \"encoded\", \"with\", \"the\", \"second\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"go\", \"with\", \"the\", \"very\", \"last\", \"one\"]]\n",
    "batch = tmp_token(batch_sentences, batch_of_second_sentences, is_split_into_words=True)\n",
    "print(batch)\n",
    "# print(batch['input_ids'].shape)\n",
    "\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
