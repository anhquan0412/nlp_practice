{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#GPT2-own-tokenizer-(use-Byte-level-BPE)\" data-toc-modified-id=\"GPT2-own-tokenizer-(use-Byte-level-BPE)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>GPT2 own tokenizer (use Byte level BPE)</a></span></li><li><span><a href=\"#HuggingFace-preprocessing-(tokenizer)\" data-toc-modified-id=\"HuggingFace-preprocessing-(tokenizer)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>HuggingFace preprocessing (tokenizer)</a></span></li><li><span><a href=\"#Tokenizers-in-details-(Subword-tokenization)\" data-toc-modified-id=\"Tokenizers-in-details-(Subword-tokenization)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenizers in details (Subword tokenization)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-tokenizer\" data-toc-modified-id=\"Pre-tokenizer-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pre-tokenizer</a></span></li><li><span><a href=\"#Byte-pair-encoding-(BPE)\" data-toc-modified-id=\"Byte-pair-encoding-(BPE)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Byte-pair encoding (BPE)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Byte-level-BPE\" data-toc-modified-id=\"Byte-level-BPE-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Byte-level BPE</a></span></li></ul></li><li><span><a href=\"#WordPiece\" data-toc-modified-id=\"WordPiece-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>WordPiece</a></span></li><li><span><a href=\"#Unigram\" data-toc-modified-id=\"Unigram-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Unigram</a></span></li><li><span><a href=\"#SentencePiece\" data-toc-modified-id=\"SentencePiece-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>SentencePiece</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-layer, 768-hidden, 12-heads, 117M parameters.\n",
    "# OpenAI GPT-2 English model\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 own tokenizer (use Byte level BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 281, 1672, 286, 2420, 11, 428, 318, 1194, 1672, 286, 2420, 13, 1058, 828, 1058, 14]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode('This is an example of text, this is another example of text. :), :/')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example of text, this is another example of text. :), :/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ' is', ' an', ' example', ' of', ' text', ',', ' this', ' is', ' another', ' example', ' of', ' text', '.', ' :', '),', ' :', '/']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode([i]) for i in ids])\n",
    "# ',' and ', ' are tokenized differently. No decoding for emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'t\",\n",
       " 'ƒ†you',\n",
       " 'ƒ†love',\n",
       " 'ƒ†√∞≈Å',\n",
       " '¬§',\n",
       " 'ƒπ',\n",
       " 'ƒ†Transformers',\n",
       " '?',\n",
       " 'ƒ†We',\n",
       " 'ƒ†sure',\n",
       " 'ƒ†do',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " 'iscover',\n",
       " 'ƒ†discovering',\n",
       " 'ƒ†discovered',\n",
       " 'ƒ†disco',\n",
       " 'ƒ†disc',\n",
       " 'ƒ†disk',\n",
       " 'ƒ†discord',\n",
       " 'ƒ†disconnect',\n",
       " 'ƒ†disconnected',\n",
       " 'ƒ†disconnect',\n",
       " 'ing',\n",
       " 'ƒ†red',\n",
       " 'is',\n",
       " 'ƒ†rad',\n",
       " 'ish']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"discover discovering discovered disco disc disk discord disconnect disconnected disconnecting redis radish\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace preprocessing (tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[15496, 314, 1101, 257, 2060, 6827], [1870, 1194, 6827], [1870, 262, 845, 845, 938, 530]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "batch = tmp_token(text)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827],\n",
      "        [ 1870,  1194,  6827, 50256, 50256, 50256],\n",
      "        [ 1870,   262,   845,   845,   938,   530]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(text,padding=True,truncation=True,max_length=100,return_tensors=\"pt\")\n",
    "# with padding. Default GPT2 padding is to the right\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I'm a single sentence\n",
      "And another sentence<|endoftext|><|endoftext|><|endoftext|>\n",
      "And the very very last one\n"
     ]
    }
   ],
   "source": [
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257],\n",
      "        [ 1870,   281,   313,    63],\n",
      "        [ 1870,   262,   845,   845]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])}\n",
      "Hello I'm a\n",
      "And anot`\n",
      "And the very very\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(text,padding=True,truncation=True,max_length=4,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "# truncation is also to the right\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pair of sentences (useful for BERT), but we will play around with truncation and max_length here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
    "                             \"And I should be encoded with the second sentence\",\n",
    "                             \"And I go with the very last one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827,    40,  1101,   257,  6827,\n",
      "           326,  2925,   351,   262,   717,  6827],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827, 50256, 50256, 50256, 50256],\n",
      "        [ 1870,   262,   845,   845,   938,   530,  1870,   314,   467,   351,\n",
      "           262,   845,   938,   530, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "torch.Size([3, 16])\n",
      "Hello I'm a single sentenceI'm a sentence that goes with the first sentence\n",
      "And another sentenceAnd I should be encoded with the second sentence<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "And the very very last oneAnd I go with the very last one<|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,    40,  1101,   257,  6827,   326,  2925,   351,   262,\n",
      "           717,  6827],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827],\n",
      "        [ 1870,   262,   845,   845,  1870,   314,   467,   351,   262,   845,\n",
      "           938,   530]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([3, 12])\n",
      "Hello II'm a sentence that goes with the first sentence\n",
      "And another sentenceAnd I should be encoded with the second sentence\n",
      "And the very veryAnd I go with the very last one\n",
      "torch.Size([3, 12])\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,truncation='only_first',max_length=12,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "# truncate only the first sentence. Still truncate from the right\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))\n",
    "\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827,    40,  1101,   257,  6827,\n",
      "           326,  2925],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827],\n",
      "        [ 1870,   262,   845,   845,   938,   530,  1870,   314,   467,   351,\n",
      "           262,   845]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([3, 12])\n",
      "Hello I'm a single sentenceI'm a sentence that goes\n",
      "And another sentenceAnd I should be encoded with the second sentence\n",
      "And the very very last oneAnd I go with the very\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,truncation='only_second',max_length=12,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "# truncate only the second sentence. Still truncate from the right\n",
    "\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,   314,  1101,   257,  2060,  6827,    40,  1101,   257,  6827,\n",
      "           326,  2925],\n",
      "        [ 1870,  1194,  6827,  1870,   314,   815,   307, 30240,   351,   262,\n",
      "          1218,  6827],\n",
      "        [ 1870,   262,   845,   845,   938,   530,  1870,   314,   467,   351,\n",
      "           262,   845]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([3, 12])\n",
      "Hello I'm a single sentenceI'm a sentence that goes\n",
      "And another sentenceAnd I should be encoded with the second sentence\n",
      "And the very very last oneAnd I go with the very\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "tmp_token.pad_token = tmp_token.eos_token\n",
    "batch = tmp_token(batch_sentences,batch_of_second_sentences,padding=True,truncation='longest_first',max_length=12,return_tensors=\"pt\")\n",
    "print(batch)\n",
    "print(batch['input_ids'].shape)\n",
    "# truncate the longest sentence of the two. Still truncate from the right\n",
    "\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also work with pre-tokenized inputs (where sentence has already split into words), good for NER or POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[18435, 314, 1101, 257, 2060, 6827, 314, 1101, 257, 6827, 326, 2925, 351, 262, 717, 6827], [843, 1194, 6827, 843, 314, 815, 307, 30240, 351, 262, 1218, 6827], [843, 262, 845, 845, 938, 530, 843, 314, 467, 351, 262, 845, 938, 530]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      " Hello I'm a single sentence I'm a sentence that goes with the first sentence\n",
      " And another sentence And I should be encoded with the second sentence\n",
      " And the very very last one And I go with the very last one\n"
     ]
    }
   ],
   "source": [
    "tmp_token = GPT2TokenizerFast.from_pretrained(pretrained_weights,add_prefix_space=True)\n",
    "\n",
    "batch_sentences = [[\"Hello\", \"I'm\", \"a\", \"single\", \"sentence\"],\n",
    "                   [\"And\", \"another\", \"sentence\"],\n",
    "                   [\"And\", \"the\", \"very\", \"very\", \"last\", \"one\"]]\n",
    "batch_of_second_sentences = [[\"I'm\", \"a\", \"sentence\", \"that\", \"goes\", \"with\", \"the\", \"first\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"should\", \"be\", \"encoded\", \"with\", \"the\", \"second\", \"sentence\"],\n",
    "                             [\"And\", \"I\", \"go\", \"with\", \"the\", \"very\", \"last\", \"one\"]]\n",
    "batch = tmp_token(batch_sentences, batch_of_second_sentences, is_split_into_words=True)\n",
    "print(batch)\n",
    "# print(batch['input_ids'].shape)\n",
    "\n",
    "for i in batch['input_ids']:\n",
    "    print(tmp_token.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers in details (Subword tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "Subword tokenization algorithms rely on the principle that \n",
    "- frequently used words should not be split into smaller subwords\n",
    "- rare words should be decomposed into meaningful subwords. \n",
    "\n",
    "E.g.: For instance \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annoying', '##ly']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('annoyingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'new', 'gp', '##u', '!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I have a new GPU!\")\n",
    "# \"##\" means that the rest of the token should be attached to the previous one, \n",
    "# without space (for decoding or reversal of the tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discover',\n",
       " 'discovering',\n",
       " 'discovered',\n",
       " 'disco',\n",
       " 'disc',\n",
       " 'disk',\n",
       " 'disco',\n",
       " '##rd',\n",
       " 'disco',\n",
       " '##nne',\n",
       " '##ct',\n",
       " 'disconnected',\n",
       " 'disco',\n",
       " '##nne',\n",
       " '##ting',\n",
       " 'red',\n",
       " '##is',\n",
       " 'ra',\n",
       " '##dis',\n",
       " '##h']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"discover discovering discovered disco disc disk discord disconnect disconnected disconnecting redis radish\")\n",
    "# common words are kept, rare words are broken down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72eb4ebdee1f42c9b039a5f30bdeeb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=798011.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‚ñÅDon',\n",
       " \"'\",\n",
       " 't',\n",
       " '‚ñÅyou',\n",
       " '‚ñÅlove',\n",
       " '‚ñÅ',\n",
       " 'ü§ó',\n",
       " '‚ñÅ',\n",
       " 'Transform',\n",
       " 'ers',\n",
       " '?',\n",
       " '‚ñÅWe',\n",
       " '‚ñÅsure',\n",
       " '‚ñÅdo',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅdiscover',\n",
       " '‚ñÅdiscovering',\n",
       " '‚ñÅdiscovered',\n",
       " '‚ñÅdisco',\n",
       " '‚ñÅdisc',\n",
       " '‚ñÅdisk',\n",
       " '‚ñÅdiscord',\n",
       " '‚ñÅdisconnect',\n",
       " '‚ñÅdisconnected',\n",
       " '‚ñÅdisc',\n",
       " 'onne',\n",
       " 'ting',\n",
       " '‚ñÅred',\n",
       " 'is',\n",
       " '‚ñÅ',\n",
       " 'rad',\n",
       " 'ish']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"discover discovering discovered disco disc disk discord disconnect disconnected disconnecting redis radish\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-tokenizer: splits the training data into words. \n",
    "\n",
    "- Pretokenization can be as simple as space tokenization, e.g. GPT-2, Roberta. \n",
    "- More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses Spacy and ftfy, to count the frequency of each word in the training corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-tokenized. After this, **a set of unique words has been created** and the **frequency of each word it occurred in the training data has been determined.** \n",
    "- **BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words** \n",
    "- Learns **merge rules** to form a new symbol from two symbols of the base vocabulary.: Most frequent ngram pairs ‚Ü¶ a new ngram\n",
    "- It does so **until the vocabulary has attained the desired vocabulary size.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g.:\n",
    "\n",
    "- ```(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)```\n",
    "\n",
    "- the base vocabulary is [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]\n",
    "    - ```(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)```\n",
    "- BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently.\n",
    "    - ```(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)```\n",
    "- BPE then identifies the next most common symbol pair.\n",
    "    - ```(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)```\n",
    "    \n",
    "    \n",
    "At this stage, the vocabulary is ```[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]``` (3 merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unknown word**\n",
    "\n",
    "For instance, the word \"bug\" would be tokenized to ```[\"b\", \"ug\"]``` but \"mug\" would be tokenized as ```[\"<unk>\", \"ug\"]``` since the symbol ```\"m\"``` is not in the base vocabulary. \n",
    "\n",
    "(In general, single letters such as ```\"m\"``` are not replaced by the ```\"<unk>\"``` symbol because the training data usually includes at least one occurrence of each letter, but it is likely to happen for very special characters like emojis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Size of vocabulary**\n",
    "\n",
    "As mentioned earlier, the vocabulary size, i.e. **the base vocabulary size + the number of merges**, is a **hyperparameter to choose**. For instance GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base vocabulary that includes **all possible base characters can be quite large** if e.g. **all unicode characters are considered as base characters**. \n",
    "\n",
    "To have a better base vocabulary, **GPT-2 uses bytes as the base vocabulary**, which is a clever trick to **force the base vocabulary to be of size 256** while ensuring that every base character is included in the vocabulary.\n",
    "\n",
    "E.g. With some additional rules to deal with punctuation, the **GPT2‚Äôs tokenizer can tokenize every text without the need for the ```<unk>``` symbol**. GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordPiece is the subword tokenization algorithm used for **BERT, DistilBERT, and Electra.**\n",
    "- very similar to BPE: WordPiece first initializes the vocabulary to include every character present in the training data and progressively learn a given number of merge rules. \n",
    "- **In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that MAXIMIZES THE LIKELIHOOD OF THE TRAINING DATA once added to the vocabulary.**\n",
    "    - maximizing the likelihood of the training data is equivalent to **finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs.** E.g. ```\"u\"```, followed by ```\"g\"``` would have only been merged if the probability of ```\"ug\"``` divided by ```\"u\"```, ```\"g\"``` would have been greater than for any other symbol pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unigram initializes its base vocabulary to a large number of symbols (such as all pre-tokenized words and the most common substrings) and progressively trims down each symbol to obtain a smaller vocabulary.\n",
    "- not used directly for any of the models in the transformers, **but it‚Äôs used in conjunction with SentencePiece.**\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- At each training step, the Unigram algorithm defines a **loss (often defined as the log-likelihood)** over the training data given the current vocabulary and **a unigram language model**. \n",
    "- Then, for each symbol in the vocabulary, the algorithm computes **how much the overall loss would increase if the symbol was to be removed from the vocabulary.** Unigram then **removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest**, i.e. those symbols that least affect the overall loss over the training data. \n",
    "- This process is repeated **until the vocabulary has reached the desired size**. \n",
    "- The Unigram algorithm **always keeps the base CHARACTERS** so that any word can be tokenized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to tokenize words after training**\n",
    "\n",
    "- the algorithm has several ways of tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary: ```[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],```, there are several way to tokenize the word ```hugs```\n",
    "- which one to choose? Unigram **saves the probability of each token in the training corpus on top of saving the vocabulary** so that the **probability of each possible tokenization can be computed after training**. The algorithm simply **picks the most likely tokenization in practice**, but also offers the possibility to sample a possible tokenization according to their probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of the words x1,‚Ä¶,xN and that the set of all possible tokenizations for a word xi is defined as S(xi), then the overall loss is defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All tokenization algorithms described so far** have the same problem: It is assumed that the input text **uses spaces to separate words.**\n",
    "\n",
    "- To solve this problem more generally, use SentencePiece (**language independent subword tokenizer**)\n",
    "- SentencePiece **treats the input as a raw input stream**, thus **including the space** in the set of characters to use. \n",
    "- It then uses the **BPE or unigram algorithm** to construct the appropriate vocabulary.\n",
    "\n",
    "In the example below the \"‚ñÅ\" character (for space) was included in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All transformers models in the library that use SentencePiece** use it in combination with **unigram**. \n",
    "- Examples of models using SentencePiece are **ALBERT, XLNet, Marian, and T5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅdiscover',\n",
       " '‚ñÅdiscovering',\n",
       " '‚ñÅdiscovered',\n",
       " '‚ñÅdisco',\n",
       " '‚ñÅdisc',\n",
       " '‚ñÅdisk',\n",
       " '‚ñÅdiscord',\n",
       " '‚ñÅdisconnect',\n",
       " '‚ñÅdisconnected',\n",
       " '‚ñÅdisconnect',\n",
       " 'ing',\n",
       " '‚ñÅred',\n",
       " 'is',\n",
       " '‚ñÅ',\n",
       " 'rad',\n",
       " 'ish']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\")\n",
    "\n",
    "tokenizer.tokenize(\"discover discovering discovered disco disc disk discord disconnect disconnected disconnecting redis radish\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "124px",
    "width": "320px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
