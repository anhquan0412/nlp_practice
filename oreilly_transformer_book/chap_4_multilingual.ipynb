{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Split-based-on-spoken-proportion\" data-toc-modified-id=\"Split-based-on-spoken-proportion-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Split based on spoken proportion</a></span></li><li><span><a href=\"#Extract-NER-tags\" data-toc-modified-id=\"Extract-NER-tags-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extract NER tags</a></span></li><li><span><a href=\"#Multilingual-Transformers-with-XLM-R\" data-toc-modified-id=\"Multilingual-Transformers-with-XLM-R-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Multilingual Transformers with XLM-R</a></span></li><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Piece-vs-WordPiece\" data-toc-modified-id=\"Sentence-Piece-vs-WordPiece-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Sentence Piece vs WordPiece</a></span></li><li><span><a href=\"#Tokenizer-Pipeline\" data-toc-modified-id=\"Tokenizer-Pipeline-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Tokenizer Pipeline</a></span></li><li><span><a href=\"#Sentencepiece-tokenizer\" data-toc-modified-id=\"Sentencepiece-tokenizer-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Sentencepiece tokenizer</a></span></li></ul></li><li><span><a href=\"#Transformers-for-NER-and-the-inner-working-of-HuggingFace-Transformers\" data-toc-modified-id=\"Transformers-for-NER-and-the-inner-working-of-HuggingFace-Transformers-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Transformers for NER and the inner working of HuggingFace Transformers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Body-and-Head\" data-toc-modified-id=\"Body-and-Head-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Body and Head</a></span></li><li><span><a href=\"#Create-custom-model\" data-toc-modified-id=\"Create-custom-model-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Create custom model</a></span></li><li><span><a href=\"#Adjust-config-file\" data-toc-modified-id=\"Adjust-config-file-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Adjust config file</a></span></li><li><span><a href=\"#Load-the-custom-model\" data-toc-modified-id=\"Load-the-custom-model-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Load the custom model</a></span></li></ul></li><li><span><a href=\"#Tokenization-for-NER\" data-toc-modified-id=\"Tokenization-for-NER-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Tokenization for NER</a></span></li><li><span><a href=\"#Define-metrics\" data-toc-modified-id=\"Define-metrics-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Define metrics</a></span></li><li><span><a href=\"#Finetuning-XLM-R\" data-toc-modified-id=\"Finetuning-XLM-R-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Finetuning XLM-R</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-up\" data-toc-modified-id=\"Setting-up-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Setting up</a></span></li><li><span><a href=\"#Error-analysis\" data-toc-modified-id=\"Error-analysis-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Error analysis</a></span></li></ul></li><li><span><a href=\"#Cross-lingual-transfer\" data-toc-modified-id=\"Cross-lingual-transfer-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Cross-lingual transfer</a></span></li><li><span><a href=\"#When-to-use-Zero-shot-transfer\" data-toc-modified-id=\"When-to-use-Zero-shot-transfer-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>When to use Zero-shot transfer</a></span></li><li><span><a href=\"#Fine-Tuning-on-Multiple-Languages-at-Once\" data-toc-modified-id=\"Fine-Tuning-on-Multiple-Languages-at-Once-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Fine-Tuning on Multiple Languages at Once</a></span><ul class=\"toc-item\"><li><span><a href=\"#Concatenate-dataset\" data-toc-modified-id=\"Concatenate-dataset-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Concatenate dataset</a></span></li><li><span><a href=\"#train-model\" data-toc-modified-id=\"train-model-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>train model</a></span></li><li><span><a href=\"#comparing-the-performance-of-fine-tuning-on-each-language-separately-against-multilingual-learning-on-all-the-corpora\" data-toc-modified-id=\"comparing-the-performance-of-fine-tuning-on-each-language-separately-against-multilingual-learning-on-all-the-corpora-12.3\"><span class=\"toc-item-num\">12.3&nbsp;&nbsp;</span>comparing the performance of fine-tuning on each language separately against multilingual learning on all the corpora</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual transformers: Like BERT, these models use **masked language modeling** as a pretraining objective, but they are trained **jointly on texts in over one hundred languages** (huge corpora across many languages)\n",
    "\n",
    "=> a model that is fine-tuned on one language can be applied to others without any further training\n",
    "\n",
    "=> these multilingual transformers enable zero-shot cross-lingual transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a single transformer model called XLM-RoBERTa (introduced in Chapter 3)1 can be fine-tuned to perform named entity recognition (NER) across several languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Zero-shot transfer or zero-shot learning usually refers to the task of training a model on one set of labels and then evaluating it on a different set of labels. In the context of transformers, zero-shot learning may also refer to situations where a language model like GPT-3 is evaluated on a downstream task that it wasn’t even fine-tuned on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this chapter let’s assume that we want to perform NER for a customer based in Switzerland, where there are four national languages (with English often serving as a bridge between them). Let’s start by getting a suitable multilingual corpus for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each article is annotated with \n",
    "- LOC (location), \n",
    "- PER (person), and \n",
    "- ORG (organization) tags \n",
    "- in the “inside-outside-beginning” (IOB2) format. \n",
    "    - In this format, a B- prefix indicates the beginning of an entity, and consecutive tokens belonging to the same entity are given an I- prefix. \n",
    "- An O tag indicates that the token does not belong to any entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a subset of the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark called WikiANN or PAN-X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_config_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af',\n",
       " 'PAN-X.ar',\n",
       " 'PAN-X.bg',\n",
       " 'PAN-X.bn',\n",
       " 'PAN-X.de',\n",
       " 'PAN-X.el',\n",
       " 'PAN-X.en',\n",
       " 'PAN-X.es',\n",
       " 'PAN-X.et',\n",
       " 'PAN-X.eu',\n",
       " 'PAN-X.fa',\n",
       " 'PAN-X.fi',\n",
       " 'PAN-X.fr',\n",
       " 'PAN-X.he',\n",
       " 'PAN-X.hi',\n",
       " 'PAN-X.hu',\n",
       " 'PAN-X.id',\n",
       " 'PAN-X.it',\n",
       " 'PAN-X.ja',\n",
       " 'PAN-X.jv',\n",
       " 'PAN-X.ka',\n",
       " 'PAN-X.kk',\n",
       " 'PAN-X.ko',\n",
       " 'PAN-X.ml',\n",
       " 'PAN-X.mr',\n",
       " 'PAN-X.ms',\n",
       " 'PAN-X.my',\n",
       " 'PAN-X.nl',\n",
       " 'PAN-X.pt',\n",
       " 'PAN-X.ru',\n",
       " 'PAN-X.sw',\n",
       " 'PAN-X.ta',\n",
       " 'PAN-X.te',\n",
       " 'PAN-X.th',\n",
       " 'PAN-X.tl',\n",
       " 'PAN-X.tr',\n",
       " 'PAN-X.ur',\n",
       " 'PAN-X.vi',\n",
       " 'PAN-X.yo',\n",
       " 'PAN-X.zh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a realistic Swiss corpus, we’ll sample the German (de), French (fr), Italian (it), and English (en) corpora from PAN-X according to their spoken proportions. This will create a language imbalance that is very common in real-world datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xtreme (/home/quan/.cache/huggingface/datasets/xtreme/PAN-X.vi/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318f4b6929b84d808b81f1b65124ee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = load_dataset(\"xtreme\", name=f\"PAN-X.vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Đồng', 'bằng', 'sông', 'Cửu', 'Long'],\n",
       " ['Trần', 'Trọng', 'Kim', ',', \"''Đường\", 'thi', \"''\", '.'],\n",
       " ['Gaushorn', '(', '213', ')'],\n",
       " ['10.17', '-', \"'\", \"''\", 'Wonder', 'Girls15th', '-', 'Nobody3rd', \"''\", \"'\"],\n",
       " ['đổi', 'CJ', 'E', '&', 'M', 'Pictures'],\n",
       " ['Đây',\n",
       "  'là',\n",
       "  'loài',\n",
       "  'bản',\n",
       "  'địa',\n",
       "  'của',\n",
       "  'Argentina',\n",
       "  ',',\n",
       "  'Brasil',\n",
       "  ',',\n",
       "  'và',\n",
       "  'Paraguay',\n",
       "  '.'],\n",
       " ['Sân',\n",
       "  'vận',\n",
       "  'động',\n",
       "  'chính',\n",
       "  'Khu',\n",
       "  'liên',\n",
       "  'hợp',\n",
       "  'thể',\n",
       "  'thao',\n",
       "  'quốc',\n",
       "  'gia',\n",
       "  'Lào'],\n",
       " ['Kia]]', ',', 'Mazda', ',', 'BMW'],\n",
       " [\"''\", \"'\", \"''\", \"'\", '-', 'Gustavo', 'Gianetti'],\n",
       " [\"''\", 'Senecio', 'nevadensis', \"''\", 'Boiss', '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['train']['tokens'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split based on spoken proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xtreme (/home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6843431a6aa34cd7842304ed8bf9d9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-ad1b311e95818edf.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-9e4b5e384626785e.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b80ca41f318cd7e7.arrow\n",
      "Reusing dataset xtreme (/home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549208eb24fe4a4a814f97a9138a4c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-222f2a739e50779b.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-ae79577dfb0e7498.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-0bc206f54324de18.arrow\n",
      "Reusing dataset xtreme (/home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82037e6940a2464384fb773dc208e689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-2a286f85a785394c.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-55894a1d8ab171ae.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b2a9c20bbec1f943.arrow\n",
      "Reusing dataset xtreme (/home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bd676e670f40bc96fb60c807124e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b91db9df81081a1f.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-dea8d95ed2e6a82a.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-56dba7892a30e39c.arrow\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "# Return a DatasetDict if a key doesn't exist\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # Shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['de', 'fr', 'it', 'en']),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['tokens', 'ner_tags', 'langs'],\n",
       "         num_rows: 12580\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['tokens', 'ner_tags', 'langs'],\n",
       "         num_rows: 6290\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['tokens', 'ner_tags', 'langs'],\n",
       "         num_rows: 6290\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_ch.keys(),panx_ch['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    "             index=[\"Number of training examples\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have more examples in German than all other languages combined, so we’ll use it as a starting point from which to perform zero-shot cross-lingual transfer to French, Italian, and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['2.000',\n",
       "  'Einwohnern',\n",
       "  'an',\n",
       "  'der',\n",
       "  'Danziger',\n",
       "  'Bucht',\n",
       "  'in',\n",
       "  'der',\n",
       "  'polnischen',\n",
       "  'Woiwodschaft',\n",
       "  'Pommern',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0],\n",
       " 'langs': ['de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_ch[\"de\"][\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None),\n",
       " 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_ch[\"de\"][\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# extract ner_tags\n",
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: function to put in `map` function for DatasetDict should have this signature\n",
    "\n",
    "`function(examples: Dict[str, List]) -> Dict[str, List]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tokens': ['2.000',\n",
       "   'Einwohnern',\n",
       "   'an',\n",
       "   'der',\n",
       "   'Danziger',\n",
       "   'Bucht',\n",
       "   'in',\n",
       "   'der',\n",
       "   'polnischen',\n",
       "   'Woiwodschaft',\n",
       "   'Pommern',\n",
       "   '.'],\n",
       "  'ner_tags': [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0],\n",
       "  'langs': ['de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de']},\n",
       " {'ner_tags_str': ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'I-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'B-LOC',\n",
       "   'I-LOC',\n",
       "   'O']})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from ner_tags to string\n",
    "panx_ch['de']['train'][0],create_tag_names(panx_ch['de']['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-55acd4c0c78d0f11.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b74f4b8beefbe2d4.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-0d2a1d8035c7d0e3.arrow\n"
     ]
    }
   ],
   "source": [
    "# go through the entire de\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 12580\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 6290\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 6290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "['Tokens', 'Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check that we don’t have any unusual imbalance in the tags, let’s calculate the frequencies of each entity across each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Transformers with XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remarkable feature of this approach is that **despite receiving no explicit information to differentiate among the languages, the resulting linguistic representations are able to generalize well across languages** for a variety of downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual transformer models are usually evaluated in three different ways:\n",
    "\n",
    "- en\n",
    "    - Fine-tune on the English training data and then evaluate on each language’s test set.\n",
    "\n",
    "- each\n",
    "    - Fine-tune and evaluate on monolingual test data to measure per-language performance.\n",
    "\n",
    "- all\n",
    "    - Fine-tune on all the training data to evaluate on all on each language’s test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLM-R uses only MLM as a pretraining objective for 100 languages, but is distinguished by the huge size of its pretraining corpus compared to its predecessors: Wikipedia dumps for each language and 2.5 terabytes of Common Crawl data from the web. \n",
    "\n",
    "The 'R' (RoBERTa) improved on several aspects of BERT, in particular by **removing the next sentence prediction task altogether**\n",
    "\n",
    "Language embeddings used in XLM is dropped. Use **Sentence Piece** tokenize raw texts directly\n",
    "\n",
    "Notable difference between XLM-R and RoBERTa is the **size of the respective vocabularies**: 250,000 tokens versus 55,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Piece vs WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]'],\n",
       " ['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens,xlmr_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLM-R uses `<s> and <\\s>` to denote the start and end of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not entirely accurate that tokenization is a single operation that transforms strings to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it “cleaner”**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencepiece tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on Unigram\n",
    "- Encodes each input text as sequence of **Unicode characters** => agnostic to accents, punctuation\n",
    "- whitespace characters == '_' (\\u2581) => dont have to rely on language-specific pretokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]'],\n",
       " ['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "\n",
    "bert_tokens,xlmr_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no whitespace between York and !, but WordPiece does not reflect this. In contrast..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for NER and the inner working of HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0403.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BERT paper,5 the authors assigned this label to the first subword (“Chr” in our example) and ignored the following subword (“##ista”). This is the convention we’ll adopt here, and we’ll indicate the ignored subwords with IGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body and Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers is organized around dedicated classes for each architecture and task. The model classes associated with different tasks are named according to a `<ModelName>For<Task>` convention, or `AutoModelFor<Task>` when using the AutoModel classes.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure is reflected in the HuggingFace Transformers code as well: the **body of a model is implemented in a class such as `BertModel` or `GPT2Model` that returns the hidden states of the last layer**. **Task-specific models** such as BertForMaskedLM or BertForSequenceClassification use the base model and **add the necessary head on top** of the hidden states, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building a custom token classification head for XLM-R\n",
    "\n",
    "(this is an exercise, as we already have a XLM-R for token classification called `XLMRobertaForTokenClassification` from HuggingFace Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel # body only\n",
    "\n",
    "#inherit this to load pretrained weight\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    # make sure standard XLM-R are used\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        # add_polling_layer to False\n",
    "        #    to ensure all hidden states are returned \n",
    "        #    and not only the one associated with the [CLS] token.\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights from RobertaPretrainedModel\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        # the only ones we need for now are input_ids and attention_mask\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output) # (bs,seq_len,num_labels)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None: # labels size: (bs,seq_len)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        # Use TokenClassifierOutput for familiar named tuple\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.features.features.ClassLabel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust config file\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xlm-roberta-base'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust config file\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"_name_or_path\": \"xlm-roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"XLMRobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PER\",\n",
       "    \"2\": \"I-PER\",\n",
       "    \"3\": \"B-ORG\",\n",
       "    \"4\": \"I-ORG\",\n",
       "    \"5\": \"B-LOC\",\n",
       "    \"6\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-LOC\": 5,\n",
       "    \"B-ORG\": 3,\n",
       "    \"B-PER\": 1,\n",
       "    \"I-LOC\": 6,\n",
       "    \"I-ORG\": 4,\n",
       "    \"I-PER\": 2,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.19.2\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250002\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jack Sparrow loves New York!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 21763, 37456, 15555, 5161, 7, 2356, 5753, 38, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or you can get input_ids with this\n",
    "xlmr_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0, 21763, 37456, 15555,  5161,     7,  2356,  5753,    38,     2]]),\n",
       " torch.Size([1, 10]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n",
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = xlmr_model(input_ids.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.1279, -0.5085, -0.0841,  0.3318, -0.0201,  0.3856, -0.2213],\n",
       "         [ 0.0337, -0.4387,  0.1413,  0.3597, -0.2307,  0.3686, -0.2843],\n",
       "         [ 0.1222, -0.5819,  0.1126,  0.3192, -0.2083,  0.2109, -0.1704],\n",
       "         [ 0.1401, -0.5656,  0.1057,  0.4588, -0.1804,  0.3483, -0.2686],\n",
       "         [ 0.1356, -0.4231,  0.1580,  0.3738, -0.2843,  0.3576, -0.3277],\n",
       "         [ 0.0839, -0.4747,  0.1817,  0.4780, -0.2855,  0.3366, -0.3124],\n",
       "         [-0.0013, -0.4648,  0.2464,  0.3286, -0.2638,  0.2610, -0.2975],\n",
       "         [ 0.0807, -0.4870,  0.2361,  0.3038, -0.2614,  0.3673, -0.4110],\n",
       "         [ 0.1728, -0.5386,  0.1194,  0.3795, -0.2315,  0.3302, -0.2537],\n",
       "         [ 0.1633, -0.5161, -0.1211,  0.3469,  0.0259,  0.2724, -0.2426]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 3, 3, 3, 3, 3, 5, 3, 3]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\n",
       "Tags    B-LOC  B-LOC  B-ORG  B-ORG  B-ORG  B-ORG  B-ORG  B-LOC  B-ORG  B-ORG"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of wrong prediction. Make sense as we are using a randomized head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper function to output NER prediction from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['2.000',\n",
       "  'Einwohnern',\n",
       "  'an',\n",
       "  'der',\n",
       "  'Danziger',\n",
       "  'Bucht',\n",
       "  'in',\n",
       "  'der',\n",
       "  'polnischen',\n",
       "  'Woiwodschaft',\n",
       "  'Pommern',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0],\n",
       " 'langs': ['de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de'],\n",
       " 'ner_tags_str': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "de_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2.000',\n",
       "  'Einwohnern',\n",
       "  'an',\n",
       "  'der',\n",
       "  'Danziger',\n",
       "  'Bucht',\n",
       "  'in',\n",
       "  'der',\n",
       "  'polnischen',\n",
       "  'Woiwodschaft',\n",
       "  'Pommern',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n",
    "words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot just use this `labels`, as after tokenization, words are broken into subwords (Einwohnern => ▁Einwohner + ▁n, and only ▁Einwohner has an 'Other' tag, and ▁n will have the 'Ignored' tag. We need to write some code for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlmr_tokenizer(words) \n",
    "# the tokenizer thought each word is a sentence => not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 70101, 176581, 19, 142, 122, 2290, 708, 1505, 18363, 18, 23, 122, 127474, 15439, 13787, 14, 15263, 18917, 663, 6947, 19, 6, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], \n",
    "                                 is_split_into_words=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1           2  3    4     5     6   7    8      9   ...   15  \\\n",
       "Tokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo   \n",
       "\n",
       "       16   17      18   19    20 21 22 23    24  \n",
       "Tokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_ids has mapped each subword to the corresponding index in the words sequence, so the first subword, “▁2.000”, is assigned the index 0, while “▁Einwohner” and “n” are assigned the index 1 (since “Einwohnern” is the second word in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2  3    4     5     6   7    8      9   ...  \\\n",
       "Tokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...   \n",
       "Word IDs  None       0           1  1    2     3     4   4    4      5  ...   \n",
       "\n",
       "           15 16   17      18   19    20  21  22  23    24  \n",
       "Tokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \n",
       "Word IDs    9  9    9       9   10    10  10  11  11  None  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that special tokens like `<s>` and `<\\s>` are mapped to None. Let’s set –100 as the label for these special tokens and the subwords we wish to mask during training\n",
    "\n",
    "Note: why -100? B/c PyTorch the **cross-entropy loss class torch.nn.CrossEntropyLoss has an attribute called ignore_index whose value is –100**. This index is ignored during training, so we can use it to ignore the tokens associated with consecutive subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IGN', 'O', 'O', 'IGN', 'O', 'O', 'B-LOC', 'IGN', 'IGN', 'I-LOC', 'IGN', 'O', 'O', 'B-LOC', 'IGN', 'B-LOC', 'IGN', 'IGN', 'IGN', 'I-LOC', 'IGN', 'IGN', 'O', 'IGN', 'IGN']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>...</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1           2     3    4     5      6     7     8   \\\n",
       "Tokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger   \n",
       "Word IDs   None       0           1     1    2     3      4     4     4   \n",
       "Label IDs  -100       0           0  -100    0     0      5  -100  -100   \n",
       "Labels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n",
       "\n",
       "              9   ...     15    16    17      18     19    20    21  22    23  \\\n",
       "Tokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .   \n",
       "Word IDs       5  ...      9     9     9       9     10    10    10  11    11   \n",
       "Label IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \n",
       "Labels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n",
       "\n",
       "             24  \n",
       "Tokens     </s>  \n",
       "Word IDs   None  \n",
       "Label IDs  -100  \n",
       "Labels      IGN  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to wrap up all this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With input `examples` is a batch from Transformers Dataset, such as panx_de['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [['2.000',\n",
       "   'Einwohnern',\n",
       "   'an',\n",
       "   'der',\n",
       "   'Danziger',\n",
       "   'Bucht',\n",
       "   'in',\n",
       "   'der',\n",
       "   'polnischen',\n",
       "   'Woiwodschaft',\n",
       "   'Pommern',\n",
       "   '.'],\n",
       "  ['Sie',\n",
       "   'geht',\n",
       "   'hinter',\n",
       "   'Walluf',\n",
       "   'nahtlos',\n",
       "   'in',\n",
       "   'die',\n",
       "   'Bundesautobahn',\n",
       "   '66',\n",
       "   'über',\n",
       "   '.']],\n",
       " 'ner_tags': [[0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0],\n",
       "  [0, 0, 0, 3, 0, 0, 0, 3, 4, 0, 0]],\n",
       " 'langs': [['de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de',\n",
       "   'de'],\n",
       "  ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']],\n",
       " 'ner_tags_str': [['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'I-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'B-LOC',\n",
       "   'I-LOC',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O']]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de['train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write a function that wraps the `map`, which accepts a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-12ac6485871107b9.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-aeaec806b256ea55.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-17f3d8dcfe5f1b02.arrow\n"
     ]
    }
   ],
   "source": [
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12580\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de_encoded['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 70101, 176581, 19, 142, 122, 2290, 708, 1505, 18363, 18, 23, 122, 127474, 15439, 13787, 14, 15263, 18917, 663, 6947, 19, 6, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, -100, 0, 0, 5, -100, -100, 6, -100, 0, 0, 5, -100, 5, -100, -100, -100, 6, -100, -100, 0, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(panx_de_encoded['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common to report results for precision, recall, and F1-score. The only subtlety is that all words of an entity need to be predicted correctly in order for a prediction to be counted as correct\n",
    "\n",
    "We will use library called seqeval for this. \n",
    "\n",
    "```\n",
    "seqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labeling and so on.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       1.00      1.00      1.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that can take the outputs of the model and convert them into the lists that seqeval expects, and ignoring the -100 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tune our base model on the German subset of PAN-X and then evaluate its zero-shot cross-lingual performance on French, Italian, and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, \n",
    "    log_level=\"error\", \n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, \n",
    "    disable_tqdm=False,\n",
    "#     logging_steps=logging_steps, \n",
    "    push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a data collator so we can pad each input sequence to the largest sequence length in a batch => **pad the labels along with the inputs**\n",
    "\n",
    "The label sequences are padded with the value –100, which, as we’ve seen, is ignored by PyTorch loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass all this information together with the encoded datasets to the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_de_encoded[\"train\"],\n",
    "                  eval_dataset=panx_de_encoded[\"validation\"],\n",
    "                  tokenizer=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manhquan0412\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">xlm-roberta-base-finetuned-panx-de</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/anhquan0412/huggingface\" target=\"_blank\">https://wandb.ai/anhquan0412/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/anhquan0412/huggingface/runs/bqg0jsbt\" target=\"_blank\">https://wandb.ai/anhquan0412/huggingface/runs/bqg0jsbt</a><br/>\n",
       "                Run data is saved locally in <code>/home/quan/kwon/nlp/oreilly_transformer_book/wandb/run-20220629_165522-bqg0jsbt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/torch/nn/modules/module.py:998: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1575' max='1575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1575/1575 02:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.167598</td>\n",
       "      <td>0.819439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.132570</td>\n",
       "      <td>0.851298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.134445</td>\n",
       "      <td>0.863394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1575, training_loss=0.1581088135734437, metrics={'train_runtime': 132.7652, 'train_samples_per_second': 284.261, 'train_steps_per_second': 11.863, 'total_flos': 863012377186080.0, 'train_loss': 0.1581088135734437, 'epoch': 3.0})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁ist</td>\n",
       "      <td>▁ein</td>\n",
       "      <td>▁Informati</td>\n",
       "      <td>ker</td>\n",
       "      <td>▁bei</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁Kaliforni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4     5           6    7     8        9   \\\n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google   \n",
       "Tags      O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n",
       "\n",
       "         10          11     12    13  \n",
       "Tokens  ▁in  ▁Kaliforni     en  </s>  \n",
       "Tags      O       B-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where training can fail\n",
    "- We might accidentally mask too many tokens and also mask some of our labels to get a really promising loss drop.\n",
    "\n",
    "- The compute_metrics() function might have a bug that overestimates the true performance.\n",
    "\n",
    "- We might include the zero class or O entity in NER as a normal class, which will heavily skew the accuracy and F1-score since it is the majority class by a large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at validation set's loss and predictions\n",
    "\n",
    "**This is how a batch goes through several steps (data collator => model => loss calculation)**. We will use this function and **map** together on the **encoded validation datasetdict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model\n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        # logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, 7), # 7 is num_labels\n",
    "                         labels.view(-1), reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6290\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = panx_de_encoded[\"validation\"]\n",
    "valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function forward_pass_with_label at 0x7f82620d9af0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-1c80317fa3b1799d.arrow\n"
     ]
    }
   ],
   "source": [
    "valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df = valid_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 3, -100, 4, 4, 4, -100]</td>\n",
       "      <td>[0.0, 0.026550876, 0.0, 0.021719921, 0.0201879...</td>\n",
       "      <td>[4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, 3, -100, -10...</td>\n",
       "      <td>[0.0, 0.00015174192, 0.0, 0.0, 0.0, 0.0, 0.754...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 6, 6, 6, 6, 6, 4, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...</td>\n",
       "      <td>[0.0, 0.00029583368, 9.4766896e-05, 0.00013314...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 5, -100, 0, 0, -100]</td>\n",
       "      <td>[0.0, 0.00016497205, 0.00011824862, 0.00016711...</td>\n",
       "      <td>[0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...</td>\n",
       "      <td>[0.0, 0.000116699084, 0.00010918975, 0.0001089...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0                 [0, 10699, 11, 15, 16104, 1388, 2]   \n",
       "1  [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n",
       "2  [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n",
       "3     [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n",
       "4  [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0                              [1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                     [-100, 3, -100, 4, 4, 4, -100]   \n",
       "1  [-100, 0, -100, -100, -100, -100, 3, -100, -10...   \n",
       "2  [-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...   \n",
       "3               [-100, 0, 0, 0, 5, -100, 0, 0, -100]   \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...   \n",
       "\n",
       "                                                loss  \\\n",
       "0  [0.0, 0.026550876, 0.0, 0.021719921, 0.0201879...   \n",
       "1  [0.0, 0.00015174192, 0.0, 0.0, 0.0, 0.0, 0.754...   \n",
       "2  [0.0, 0.00029583368, 9.4766896e-05, 0.00013314...   \n",
       "3  [0.0, 0.00016497205, 0.00011824862, 0.00016711...   \n",
       "4  [0.0, 0.000116699084, 0.00010918975, 0.0001089...   \n",
       "\n",
       "                                     predicted_label  \n",
       "0  [4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 3, 6, 6, 6, 6, 6, 4, 6, 6, ...  \n",
       "2  [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]</td>\n",
       "      <td>[0.0, 0.026550876, 0.0, 0.021719921, 0.0201879...</td>\n",
       "      <td>[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[&lt;s&gt;, ▁Ham, a, ▁(, ▁Unternehmen, ▁), &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, IGN, IGN, IGN, IGN, B-ORG, IGN, IGN, ...</td>\n",
       "      <td>[0.0, 0.00015174192, 0.0, 0.0, 0.0, 0.0, 0.754...</td>\n",
       "      <td>[O, O, O, O, O, O, B-ORG, I-LOC, I-LOC, I-LOC,...</td>\n",
       "      <td>[&lt;s&gt;, ▁WE, ITE, RL, EIT, UNG, ▁Luz, ky, j, ▁a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, O, B-ORG, IGN, IGN, O, IGN, O, ...</td>\n",
       "      <td>[0.0, 0.00029583368, 9.4766896e-05, 0.00013314...</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁entdeckt, ▁und, ▁gehört, ▁der, ▁Spek, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, O, O, B-LOC, IGN, O, O, IGN]</td>\n",
       "      <td>[0.0, 0.00016497205, 0.00011824862, 0.00016711...</td>\n",
       "      <td>[O, O, O, O, B-LOC, I-LOC, O, O, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁**, ▁', ▁'', ▁, Bretagne, ▁'', ▁', &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, O, O, O, O, O, IGN, O, O, O, B-ORG...</td>\n",
       "      <td>[0.0, 0.000116699084, 0.00010918975, 0.0001089...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-...</td>\n",
       "      <td>[&lt;s&gt;, ▁Nach, ▁einem, ▁Jahr, ▁bei, ▁diesem, ▁Ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0                 [0, 10699, 11, 15, 16104, 1388, 2]   \n",
       "1  [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n",
       "2  [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n",
       "3     [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n",
       "4  [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0                              [1, 1, 1, 1, 1, 1, 1]   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \\\n",
       "0        [IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]   \n",
       "1  [IGN, O, IGN, IGN, IGN, IGN, B-ORG, IGN, IGN, ...   \n",
       "2  [IGN, O, O, O, O, B-ORG, IGN, IGN, O, IGN, O, ...   \n",
       "3              [IGN, O, O, O, B-LOC, IGN, O, O, IGN]   \n",
       "4  [IGN, O, O, O, O, O, O, O, IGN, O, O, O, B-ORG...   \n",
       "\n",
       "                                                loss  \\\n",
       "0  [0.0, 0.026550876, 0.0, 0.021719921, 0.0201879...   \n",
       "1  [0.0, 0.00015174192, 0.0, 0.0, 0.0, 0.0, 0.754...   \n",
       "2  [0.0, 0.00029583368, 9.4766896e-05, 0.00013314...   \n",
       "3  [0.0, 0.00016497205, 0.00011824862, 0.00016711...   \n",
       "4  [0.0, 0.000116699084, 0.00010918975, 0.0001089...   \n",
       "\n",
       "                                     predicted_label  \\\n",
       "0  [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]   \n",
       "1  [O, O, O, O, O, O, B-ORG, I-LOC, I-LOC, I-LOC,...   \n",
       "2     [O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O]   \n",
       "3                [O, O, O, O, B-LOC, I-LOC, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-...   \n",
       "\n",
       "                                        input_tokens  \n",
       "0         [<s>, ▁Ham, a, ▁(, ▁Unternehmen, ▁), </s>]  \n",
       "1  [<s>, ▁WE, ITE, RL, EIT, UNG, ▁Luz, ky, j, ▁a,...  \n",
       "2  [<s>, ▁entdeckt, ▁und, ▁gehört, ▁der, ▁Spek, t...  \n",
       "3    [<s>, ▁**, ▁', ▁'', ▁, Bretagne, ▁'', ▁', </s>]  \n",
       "4  [<s>, ▁Nach, ▁einem, ▁Jahr, ▁bei, ▁diesem, ▁Ve...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2tag[-100] = \"IGN\"\n",
    "df[\"input_tokens\"] = df[\"input_ids\"].apply(\n",
    "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df[\"labels\"] = df[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df['loss'] = df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "df['predicted_label'] = df.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pd series explode to transform each element of list-like to row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IGN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10699</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.026551</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>IGN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.02172</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16104</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids attention_mask labels      loss predicted_label  input_tokens\n",
       "0         0              1    IGN       0.0           I-ORG           <s>\n",
       "0     10699              1  B-ORG  0.026551           B-ORG          ▁Ham\n",
       "0        11              1    IGN       0.0           I-ORG             a\n",
       "0        15              1  I-ORG   0.02172           I-ORG            ▁(\n",
       "0     16104              1  I-ORG  0.020188           I-ORG  ▁Unternehmen"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens = df.apply(pd.Series.explode)\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10699</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.02</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16104</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.02</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1388</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56530</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.00</td>\n",
       "      <td>O</td>\n",
       "      <td>▁WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83982</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.75</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁Luz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>1.22</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>▁a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids attention_mask labels  loss predicted_label  input_tokens\n",
       "0     10699              1  B-ORG  0.03           B-ORG          ▁Ham\n",
       "0        15              1  I-ORG  0.02           I-ORG            ▁(\n",
       "0     16104              1  I-ORG  0.02           I-ORG  ▁Unternehmen\n",
       "0      1388              1  I-ORG  0.03           I-ORG            ▁)\n",
       "1     56530              1      O  0.00               O           ▁WE\n",
       "1     83982              1  B-ORG  0.75           B-ORG          ▁Luz\n",
       "1        10              1  I-ORG  1.22           I-LOC            ▁a"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "df_tokens.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " group it by the input tokens and aggregate the losses for each token with the count, mean, and sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>▁</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁von</td>\n",
       "      <td>▁und</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁/</td>\n",
       "      <td>▁)</td>\n",
       "      <td>▁A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6066</td>\n",
       "      <td>1388</td>\n",
       "      <td>989</td>\n",
       "      <td>808</td>\n",
       "      <td>1171</td>\n",
       "      <td>246</td>\n",
       "      <td>2898</td>\n",
       "      <td>163</td>\n",
       "      <td>246</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>187.39</td>\n",
       "      <td>129.18</td>\n",
       "      <td>117.35</td>\n",
       "      <td>111.48</td>\n",
       "      <td>89.77</td>\n",
       "      <td>79.45</td>\n",
       "      <td>74.62</td>\n",
       "      <td>73.77</td>\n",
       "      <td>73.15</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0       1       2       3      4      5      6      7  \\\n",
       "input_tokens       ▁    ▁der     ▁in    ▁von   ▁und     ▁(    ▁''     ▁/   \n",
       "count           6066    1388     989     808   1171    246   2898    163   \n",
       "mean            0.03    0.09    0.12    0.14   0.08   0.32   0.03   0.45   \n",
       "sum           187.39  129.18  117.35  111.48  89.77  79.45  74.62  73.77   \n",
       "\n",
       "                  8      9  \n",
       "input_tokens     ▁)     ▁A  \n",
       "count           246    125  \n",
       "mean            0.3   0.43  \n",
       "sum           73.15  53.18  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- White space: highest lost but low average loss =>model do fine on this\n",
    "- Words like “in”, “von”, “der”, and “und” appear relatively frequently. They often appear together with named entities and are sometimes part of them, which explains why the model might mix them up.\n",
    "- Parentheses, slashes, and capital letters at the beginning of words are rarer but have a relatively high average loss. We will investigate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2683</td>\n",
       "      <td>1462</td>\n",
       "      <td>3820</td>\n",
       "      <td>3172</td>\n",
       "      <td>2893</td>\n",
       "      <td>4139</td>\n",
       "      <td>43648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>1685.01</td>\n",
       "      <td>880.02</td>\n",
       "      <td>1763.91</td>\n",
       "      <td>1076.03</td>\n",
       "      <td>778.81</td>\n",
       "      <td>755.25</td>\n",
       "      <td>1329.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       1        2        3       4       5        6\n",
       "labels    B-ORG   I-LOC    I-ORG    B-LOC   B-PER   I-PER        O\n",
       "count      2683    1462     3820     3172    2893    4139    43648\n",
       "mean       0.63     0.6     0.46     0.34    0.27    0.18     0.03\n",
       "sum     1685.01  880.02  1763.91  1076.03  778.81  755.25  1329.84"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B⁠-⁠ORG has the highest average loss, which means that determining the beginning of an organization poses a challenge to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the model seems to struggle the most on the ORG entities, probably because these are the **least common in the training data** and many **organization names are rare in XLM-R’s vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n",
    "                      tags.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confuse the B-ORG and I-ORG entities the most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let’s move on and **look at sequences with high losses**. For this calculation, we’ll revisit our “unexploded” DataFrame and **calculate the total loss by summing over the loss per token**. To do this, let’s first write a function that helps us display the token sequences with the labels and the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels,\n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total_loss\"] = df[\"loss\"].apply(sum)\n",
    "# look at top 3 sentences with highest CE loss\n",
    "df_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>[0, 5106, 1019, 5, 19838, 5106, 152, 75198, 27...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, B-ORG, IGN, IGN, I-ORG, I-ORG, I-ORG, I-...</td>\n",
       "      <td>[0.0, 9.5688, 0.0, 0.0, 8.366348, 9.808468, 10...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...</td>\n",
       "      <td>[&lt;s&gt;, ▁'', 8, ., ▁Juli, ▁'', ▁:, ▁Protest, cam...</td>\n",
       "      <td>84.777438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3558</th>\n",
       "      <td>[0, 242, 5106, 18141, 15020, 5106, 242, 242, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[IGN, O, O, O, IGN, O, O, B-LOC, I-LOC, I-LOC,...</td>\n",
       "      <td>[0.0, 0.00027998342, 0.0003777029, 1.6939306, ...</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[&lt;s&gt;, ▁', ▁'', ▁Τ, Κ, ▁'', ▁', ▁', ▁'', ▁T, ▁'...</td>\n",
       "      <td>75.252002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>[0, 14098, 145704, 19335, 157955, 91969, 3674,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-PER, I-PER, I-PER, IGN, I-PER, IGN, I-...</td>\n",
       "      <td>[0.0, 6.0487304, 5.533558, 5.7249274, 0.0, 5.5...</td>\n",
       "      <td>[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-O...</td>\n",
       "      <td>[&lt;s&gt;, ▁United, ▁Nations, ▁Multi, dimensional, ...</td>\n",
       "      <td>59.884397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_ids  \\\n",
       "5009  [0, 5106, 1019, 5, 19838, 5106, 152, 75198, 27...   \n",
       "3558  [0, 242, 5106, 18141, 15020, 5106, 242, 242, 5...   \n",
       "4671  [0, 14098, 145704, 19335, 157955, 91969, 3674,...   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "5009  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3558  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4671   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                                 labels  \\\n",
       "5009  [IGN, B-ORG, IGN, IGN, I-ORG, I-ORG, I-ORG, I-...   \n",
       "3558  [IGN, O, O, O, IGN, O, O, B-LOC, I-LOC, I-LOC,...   \n",
       "4671  [IGN, B-PER, I-PER, I-PER, IGN, I-PER, IGN, I-...   \n",
       "\n",
       "                                                   loss  \\\n",
       "5009  [0.0, 9.5688, 0.0, 0.0, 8.366348, 9.808468, 10...   \n",
       "3558  [0.0, 0.00027998342, 0.0003777029, 1.6939306, ...   \n",
       "4671  [0.0, 6.0487304, 5.533558, 5.7249274, 0.0, 5.5...   \n",
       "\n",
       "                                        predicted_label  \\\n",
       "5009  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...   \n",
       "3558  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n",
       "4671  [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-O...   \n",
       "\n",
       "                                           input_tokens  total_loss  \n",
       "5009  [<s>, ▁'', 8, ., ▁Juli, ▁'', ▁:, ▁Protest, cam...   84.777438  \n",
       "3558  [<s>, ▁', ▁'', ▁Τ, Κ, ▁'', ▁', ▁', ▁'', ▁T, ▁'...   75.252002  \n",
       "4671  [<s>, ▁United, ▁Nations, ▁Multi, dimensional, ...   59.884397  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁''</td>\n",
       "      <td>8</td>\n",
       "      <td>.</td>\n",
       "      <td>▁Juli</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁:</td>\n",
       "      <td>▁Protest</td>\n",
       "      <td>camp</td>\n",
       "      <td>▁auf</td>\n",
       "      <td>▁dem</td>\n",
       "      <td>▁Gelände</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Republika</td>\n",
       "      <td>n</td>\n",
       "      <td>ischen</td>\n",
       "      <td>▁Gar</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>9.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.37</td>\n",
       "      <td>9.81</td>\n",
       "      <td>10.11</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.48</td>\n",
       "      <td>9.64</td>\n",
       "      <td>8.55</td>\n",
       "      <td>7.13</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2      3      4      5         6     7      8      9   \\\n",
       "tokens    ▁''     8     .  ▁Juli    ▁''     ▁:  ▁Protest  camp   ▁auf   ▁dem   \n",
       "labels  B-ORG   IGN   IGN  I-ORG  I-ORG  I-ORG     I-ORG   IGN  I-ORG  I-ORG   \n",
       "preds       O     O     O      O      O      O         O     O      O      O   \n",
       "losses   9.57  0.00  0.00   8.37   9.81  10.11      7.05  0.00   9.48   9.64   \n",
       "\n",
       "              10     11          12     13      14     15     16    17  \n",
       "tokens  ▁Gelände   ▁der  ▁Republika      n  ischen   ▁Gar     de  </s>  \n",
       "labels     I-ORG  I-ORG       I-ORG    IGN     IGN  I-ORG    IGN   IGN  \n",
       "preds          O      O       B-ORG  I-ORG   I-ORG  I-ORG  I-ORG     O  \n",
       "losses      8.55   7.13        5.06   0.00    0.00   0.01   0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁'</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁Τ</td>\n",
       "      <td>Κ</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>▁'</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁T</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>ri</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>k</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>ala</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.45</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.27</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.59</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2     3     4     5      6      7      8      9   \\\n",
       "tokens    ▁'   ▁''     ▁Τ     Κ   ▁''    ▁'     ▁'    ▁''     ▁T    ▁''   \n",
       "labels     O     O      O   IGN     O     O  B-LOC  I-LOC  I-LOC  I-LOC   \n",
       "preds      O     O  B-ORG     O     O     O      O      O      O      O   \n",
       "losses  0.00  0.00   1.69  0.00  0.00  0.00  10.45   9.37   8.27   7.14   \n",
       "\n",
       "           10    11     12     13    14     15     16    17    18  \n",
       "tokens     ▁'    ri    ▁''     ▁'     k    ▁''     ▁'   ala  </s>  \n",
       "labels  I-LOC   IGN  I-LOC  I-LOC   IGN  I-LOC  I-LOC   IGN   IGN  \n",
       "preds       O     O      O      O     O      O      O     O     O  \n",
       "losses   7.22  0.00   7.37   7.23  0.00   8.59   7.92  0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁United</td>\n",
       "      <td>▁Nations</td>\n",
       "      <td>▁Multi</td>\n",
       "      <td>dimensional</td>\n",
       "      <td>▁Integra</td>\n",
       "      <td>ted</td>\n",
       "      <td>▁Stabil</td>\n",
       "      <td>ization</td>\n",
       "      <td>▁Mission</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁Central</td>\n",
       "      <td>▁African</td>\n",
       "      <td>▁Republic</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>6.05</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.09</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1       2            3         4      5        6   \\\n",
       "tokens  ▁United  ▁Nations  ▁Multi  dimensional  ▁Integra    ted  ▁Stabil   \n",
       "labels    B-PER     I-PER   I-PER          IGN     I-PER    IGN    I-PER   \n",
       "preds     B-ORG     I-ORG   I-ORG        I-ORG     I-ORG  I-ORG    I-ORG   \n",
       "losses     6.05      5.53    5.72         0.00      5.58   0.00     5.45   \n",
       "\n",
       "             7         8      9      10        11        12         13     14  \n",
       "tokens  ization  ▁Mission    ▁in   ▁the  ▁Central  ▁African  ▁Republic   </s>  \n",
       "labels      IGN     I-PER  I-PER  I-PER     I-PER     I-PER      I-PER    IGN  \n",
       "preds     I-ORG     I-ORG  I-ORG  I-ORG     I-ORG     I-ORG      I-ORG  I-ORG  \n",
       "losses     0.00      5.15   5.30   5.53      5.24      5.09       5.23   0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some labels are wrong(United Nations is a person?)\n",
    "    - annotations for the PAN-X dataset were generated through an automated process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at parenthesis and slashes examples (where there's an opening parenthesis `(`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Ham</td>\n",
       "      <td>a</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2             3      4      5\n",
       "tokens   ▁Ham      a     ▁(  ▁Unternehmen     ▁)   </s>\n",
       "labels  B-ORG    IGN  I-ORG         I-ORG  I-ORG    IGN\n",
       "preds   B-ORG  I-ORG  I-ORG         I-ORG  I-ORG  I-ORG\n",
       "losses   0.03   0.00   0.02          0.02   0.03   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Kesk</td>\n",
       "      <td>kül</td>\n",
       "      <td>a</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁Mart</td>\n",
       "      <td>na</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7\n",
       "tokens  ▁Kesk    kül      a     ▁(  ▁Mart     na     ▁)   </s>\n",
       "labels  B-LOC    IGN    IGN  I-LOC  I-LOC    IGN  I-LOC    IGN\n",
       "preds   B-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC\n",
       "losses   0.03   0.00   0.00   0.02   0.02   0.00   0.02   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The location is inside the parentheses, so the parenthesis is labeled as LOC as well\n",
    "- we might want disconnect it from the original location in the annotations\n",
    "- Data is collected from wikipedia multiple languages, so **more often wikipedia will have article titles often containing some sort of explanation in parentheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-lingual transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have fine-tuned XLM-R on German, we can evaluate its ability to transfer to other languages via the predict() method of the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='419' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [263/263 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [de] dataset: 0.866\n"
     ]
    }
   ],
   "source": [
    "f1_scores = defaultdict(dict)\n",
    "f1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\n",
    "print(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on French\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁est</td>\n",
       "      <td>▁informatic</td>\n",
       "      <td>ien</td>\n",
       "      <td>▁chez</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁en</td>\n",
       "      <td>▁Cali</td>\n",
       "      <td>for</td>\n",
       "      <td>nie</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3     4            5    6      7        8    9   \\\n",
       "Tokens  <s>  ▁Jeff    ▁De     an  ▁est  ▁informatic  ien  ▁chez  ▁Google  ▁en   \n",
       "Tags      O  B-PER  I-PER  I-PER     O            O    O      O    B-ORG    O   \n",
       "\n",
       "           10     11     12    13  \n",
       "Tokens  ▁Cali    for    nie  </s>  \n",
       "Tags    B-LOC  I-LOC  I-LOC     O  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\n",
    "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_ds = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1_score(trainer, panx_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b155fa8d1ee747f3.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-c96c80b619ee3678.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-c8b056551bc4d569.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [fr] dataset: 0.702\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\n",
    "print(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see a drop of about 15 points in the micro-averaged metrics, remember that our model has not seen a single labeled French example\n",
    "\n",
    "=> size of the performance drop is related to how “far away” the languages are from each other\n",
    "    - Indo-European languages, they technically belong to different language families: Germanic and Romance, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on Italian (also a Romance language, like French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-6b6e00b5f6b031e4.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-134dd5df028b892e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67288dc97da244b68612f15f167cf438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [it] dataset: 0.678\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\n",
    "print(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on English (Germanic language family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-98734a1056b4cd18.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-a6d97d034e66fcb1.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-58411fa278cc6fe8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de] model on [en] dataset: 0.597\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\n",
    "print(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use Zero-shot transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, how good are these results and how do they compare against an XLM-R model fine-tuned on a monolingual corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will explore this question for the **French** corpus by fine-tuning XLM-R on training sets of increasing size, which in practice can be useful for guiding decisions about whether to collect more labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we’ll keep the same hyperparameters from the fine-tuning run on the German corpus, except that we’ll tweak the logging_steps argument of Training​Ar⁠guments to account for the changing training set sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple function that takes a DatasetDict object corresponding to a monolingual corpus, downsamples it by num_samples, and fine-tunes XLM-R on that sample to return the metrics from the best epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n",
    "    valid_ds = dataset[\"validation\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "    training_args.logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "    trainer = Trainer(model_init=model_init, args=training_args,\n",
    "        data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
    "    trainer.train()\n",
    "#     if training_args.push_to_hub:\n",
    "#         trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "\n",
    "    f1_score = get_f1_score(trainer, test_ds)\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-7077f32b8a439dce.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-d233b9514858a421.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-271f03ea5f605d61.arrow\n"
     ]
    }
   ],
   "source": [
    "panx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with multiple sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-efb4c97d0cfc0aaf.arrow\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/torch/nn/modules/module.py:998: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.761700</td>\n",
       "      <td>1.308858</td>\n",
       "      <td>0.064724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.264300</td>\n",
       "      <td>1.144619</td>\n",
       "      <td>0.174922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.091400</td>\n",
       "      <td>1.032095</td>\n",
       "      <td>0.168708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.165449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_samples  f1_score\n",
       "0          250  0.165449"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = train_on_subset(panx_fr_encoded, 250)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_samples in [500, 1000, 2000, 4000]:\n",
    "    metrics_df = metrics_df.append(\n",
    "        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.165449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>0.643951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.759887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.798048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_samples  f1_score\n",
       "0          250  0.165449\n",
       "1          500  0.643951\n",
       "2         1000  0.759887\n",
       "3         2000  0.798048"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtCUlEQVR4nO3deXwV9b3/8dcnCUlICFsCiOxaCLIJEhC0KNQNrUK1brgvt1as23X5qddirb3trXWpraW22louYCvuRS+tVhpcgxIgsu8mJogm7CaBrN/fHzMJJysJ5OSc5Lyfj0cezJmZM/M5cw7fz8x3Zj5jzjlERCRyRYU6ABERCS0lAhGRCKdEICIS4ZQIREQinBKBiEiEUyIQEYlwQUsEZva8meWb2ZoGppuZ/dbMtpjZKjM7KVixiIhIw4J5RDAHmNrI9HOBwf7fTcAzQYxFREQaELRE4Jx7H9jdyCzTgbnOsxToama9gxWPiIjULyaE6+4D5Aa8zvPH7ag9o5ndhHfUQGJi4tihQ4e2SoAiIu3F8uXLdzrnetQ3LZSJoMmcc88CzwKkpaW5zMzMEEckItK2mFlOQ9NCedXQdqBfwOu+/jgREWlFoUwEC4Fr/KuHJgD7nHN1uoVERCS4gtY1ZGZ/AyYDKWaWB/wE6ADgnPsDsAg4D9gCFAPXBysWERFpWNASgXNuxmGmO+BHwVq/iIg0je4sFhGJcEoEIiIRTolARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREIpwSgYhIhFMiEBGJcEoEIiIRTolARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREIpwSgYhIhFMiEBGJcEoEIiIRTolARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREIpwSgYhIhFMiEBGJcEoEIiIRTolARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREIpwSgYhIhFMiEBGJcEoEIiIRTolARCTCBTURmNlUM9toZlvM7P56pvc3s3QzW2lmq8zsvGDGIyIidQUtEZhZNDAbOBcYBswws2G1Zvsx8JJzbgxwOfD7YMUjIiL1iwnisscDW5xz2wDM7EVgOrAuYB4HdPaHuwBfBjEeEZGw45zjQFkFRSUVFJWUU1RaTnGpN1xcWkFhSTnFJeUUlVZw+pAejOjTpcVjCGYi6APkBrzOA06uNc/DwDtmdhuQCJxZ34LM7CbgJoD+/fu3eKAiIk0R2GgXl5Z7jXdpeXWjXaPx9qcXl3qNeFFJOcX+/FXzFpWUU1xWgXNNW39SfEybSwRNMQOY45x7wswmAvPMbIRzrjJwJufcs8CzAGlpaU3cZCISyZxzHCyrrG6oAxvl4pJyv7H2G+aSgMbbn14U0LAXBczb1EY7OspIjI0mMS6GBP/fxNgYeneJJyE2hsS4aBJjY0iIiyExNrr636r5Eqqmx0bTKc57HRsdnN78YCaC7UC/gNd9/XGBbgSmAjjnMswsHkgB8oMYl4iEmcBGuzhgLzuwUS4u9RvvkoDGu7S8xvRDDXrzGu0oo7oBTow71Hgf0zmehLgYOsVFe413VYNd1Xj78yfExniNdUDDHxcThZkFd8O1kGAmgmXAYDMbhJcALgeuqDXPF8AZwBwzOwGIBwqCGJOIHCXnHCXllTUb5apukoBGuWoPvLC+LpHSmg16UWk5lUfQaAfuNfdKiich5VAD3Snu0F52Qo0G/tDeeFtstIMhaInAOVduZrcCbwPRwPPOubVm9giQ6ZxbCNwNPGdm/4l34vg655qaw0XkcKoa7Qb7rqv3vutpvGt1iQT2cTer0Q5osKsa3p5J8SQk+10eAXvVhxromo13VWOfGBcT8Y12MFhba3fT0tJcZmZmqMMQaXG1G+2aDXTdE4+H6/Ou2gOvaGKrbX6jfajvulaXR60GPTGgu6Tma2++Tmq0w4qZLXfOpdU3LdQni0XapKpGO7Cro/Zede0+7pqvAy8NPPJGO6FWI5zSKZb+cQn1nnRMrHVSsrqR96fHd1CjHamUCCTi5O4upqCwpN696tp93A1e192MRhsI2Gs+tHfdPTGWft0T6vRhBzbw1VeM1Oou6dghWo22tBglAmn3DpZV8Onnu0nfmE/6hnyydxU3On99e83dE2Pp1y2hzh541by1rxgJ7D6Jj4kmKkqNtoQvJQJpl7bvPUD6hnyWbMznoy27OFBWQVxMFKccn8z1pw6if3JCnf7wxNgYOnZQoy2RR4lA2oWyikqW5+whfWM+SzYUsPHrbwDo260jl6T1ZUpqTyYcl0zH2OgQRyoSfpQIpM3K/+Yg720sYMnGAt7fXMA3B8vpEG2MG9idB8eewJShPTm+R6L60kUOQ4lA2oyKSseqvL2kbywgfUM+q7fvA6BX5zi+O7I3k1N7cuq3kkmK7xDiSEXaFiUCCWt7i0t5b5O31//epgJ2F5USZXBS/27ce04qk1N7MKx3Z+31ixwFJQIJK8451u3YzxJ/r3/FF3uodNA9MZbTh/RgcmoPTh/Sg64JsaEOVaTdUCKQkCssKefDzTtZsjGf9I35fL2/BICRfbpw63cGMyW1B6P6diVaV/OIBIUSgbQ65xxbC4pI3+A1/Muyd1NW4UiKi+G0qr3+1B70TIoPdagiEUGJQFrFwbIKMrbu8m7q2phP7u4DAKT2SuKGbw9iSmpPxg7oRocg1VsXkYYpEUjQ5O4urr6b9+Otuygpr6Rjh2hO/VYyN59+PJNTe9Kna8dQhykS8ZQIpMWUlleSme2Vcvj3hny2FhQBMDA5gStO7s+U1J6MH9Sd+A66qUsknLS9RLBxI0yeXHPcpZfCLbdAcTGcd17d91x3nfe3cydcfHHd6TNnwmWXQW4uXH113el33w0XXOCt+4c/rDv9xz+GM8+ErCy4886603/xCzjlFPj4Y/iv/6o7/amnYPRoePdd+O//rjv9j3+E1FR480144om60+fNg379YMECeOaZutNfeQVSUmDOHO+vtkWLICEBfv97eOmlutOXLPH+ffxxeOutGpO+SkphyawnSd+Yz4drv6SIaGIryzl5fy5X7t3GFPYw6Jd/8WZ+4AHIyKi57L59Yf58b/jOO71tGGjIEHj2WW/4pptg06aa00eP9rYfwFVXQV5ezekTJ8L//I83/P3vw65dNaefcQbMmuUNn3suHDhQc/r558M993jDtX93oN9eCH97dOwI//iHN/yzn8HixTWnJyfDq696w/rt1Z0eoO0lAgmpcoysTseS3m0Q6V2PY11iL3htNcd2iWd6zG6mrP2AU/Z9QWJlmfeG5OTQBiwih6UH08hh7Sos4f3NBaRv8G7q2negjOgoI21AN6YM7cmU1J4M6dVJN3WJhDE9mEaapbLSsfbL/fzbv7zzs7y9OAcpneI4a1gvpqT25NuDU+jSUaUcRNoDJQIBYP/BMj7YtNOr3rmxgJ2FJZjBiX27cucZQ5gytAcjju2iEs0i7ZASQYRyzrHp68Lqyzszc/ZQUeno0rEDpw3pwXeG9uC0wT1I7hQX6lBFJMiUCCJIcWk5H2/Zxb835rNkQz5f7jsIwLDenbn59OOYktqT0f26EqObukQiihJBO/f5zkOlHD7ZtpvSikoSY6P59uAUbj9jMJNTe3JMF5VyEIlkSgTtTODzeZdsLODznd5NXcf3SOSaiQP4ztCepA3sTmyM9vpFxKNE0A5s33vAq9y5oYCPtuys9XzegUwe0pP+yQmhDlNEwpQSQRtUVlHJipw9fl+/ns8rIkdHiaCNqO/5vDFRxvhBej6viBwdJYIw1dDzeXsmxXHeiN5MGarn84pIy1AiCEOl5ZVM+92HbPjqGz2fV0SCTokgDP1z7Vds+OobHjh3KJem9aNbop7PKyLBo0QQhuZlZDMgOYEfTDpOJR1EJOh0MXmYWb9jP8uy93DVyQOUBESkVSgRhJm5GTnExURxSVrfUIciIhFCiSCM7DtQxhsrtzN99LF0TdB5ARFpHUoEYeTV5XkcKKvgmokDQx2KiESQoCYCM5tqZhvNbIuZ3d/APJea2TozW2tmfw1mPOGsstIxf2kOY/p3ZUSfLqEOR0QiSNCuGjKzaGA2cBaQBywzs4XOuXUB8wwGHgBOdc7tMbOewYon3H20dSfbdhbx68tODHUoIhJhgnlEMB7Y4pzb5pwrBV4Eptea5wfAbOfcHgDnXH4Q4wlrczNySE6M5byRvUMdiohEmGAmgj5AbsDrPH9coCHAEDP7yMyWmtnU+hZkZjeZWaaZZRYUFAQp3NDZvvcAi9d/zWXj+hEXo0JxItK6Qn2yOAYYDEwGZgDPmVnX2jM55551zqU559J69OjRuhG2gheW5gBw5YQBIY5ERCJRkxKBmX3bzK73h3uY2aAmvG070C/gdV9/XKA8YKFzrsw59zmwCS8xRIyS8goWLMvljBN60adrx1CHIyIR6LCJwMx+AtyHd1IXoAMwvwnLXgYMNrNBZhYLXA4srDXPG3hHA5hZCl5X0bamBN5eLFq9g11FpVwzUUcDIhIaTTkiuBCYBhQBOOe+BJIO9ybnXDlwK/A2sB54yTm31sweMbNp/mxvA7vMbB2QDtzrnNvV/I/Rds3NyOG4lEROPT4l1KGISIRqyuWjpc45Z2YOwMwSm7pw59wiYFGtcQ8FDDvgLv8v4qzZvo+VX+zlofOHqa6QiIRMU44IXjKzPwJdzewHwLvAc8ENKzLMzcimY4dovj9WdYVEJHQaPSIw7wkoC4ChwH4gFXjIOfevVoitXdtbXMrfs77kopP60qWjnjImIqHTaCLwu4QWOedGAmr8W9DLmXmUlFfqJLGIhFxTuoZWmNm4oEcSQSorHfM/yWHcwG6c0LtzqMMRkQjXlJPFJwNXmlkO3pVDhnewMCqokbVj720uIGdXMXefnRrqUEREmpQIzgl6FBFmXkYOKZ3imDr8mFCHIiJy+K4h51wO0BW4wP/r6o+TI5C7u5j0jflcMb4fsTGhrvAhItK0O4vvAF4Aevp/883stmAH1l7NX5pDlBlXnKyTxCISHprSNXQjcLJzrgjAzB4FMoCngxlYe3SwrIIFmbmcPawXx3SJD3U4IiJA064aMqAi4HWFP06a6c3PvmRvcRlX65JREQkjTTki+AvwiZm97r/+HvDnoEXUjs1bmsPgnp2YeFxyqEMREal22ETgnHvSzJYA3/ZHXe+cWxnUqNqhrNy9rMrbxyPTh+PdsC0iEh4OmwjMbAKw1jm3wn/d2cxOds59EvTo2pG5GdkkxkZz4ZjaD2kTEQmtppwjeAYoDHhd6I+TJtpdVMpbq3Zw4Ul9SIpXXSERCS9NOlnsl4sGwDlXSdPOLYhvwbJcSssruWbiwFCHIiJSR1MSwTYzu93MOvh/dxBhTxE7GhWVjvlLczh5UHeG9Drs83xERFpdUxLBzcApeM8b3o5Xe+imYAbVnqRvyGf73gM6GhCRsNWUq4by8Z43LEdg7tIcenWO4+zhvUIdiohIvRo8IjCzH5jZYH/YzOx5M9tnZqvM7KTWC7Ht+nxnEe9vKmDG+P50iFZdIREJT421TncA2f7wDOBE4Di85wv/JrhhtQ/zl+YQE2VcMb5/qEMREWlQY4mg3DlX5g+fD8x1zu1yzr0LNPkB9pHqQGkFL2fmcs6IY+jZWXWFRCR8NZYIKs2st5nFA2fgPbS+SsfghtX2/T1rO/sPlnPNBNUVEpHw1tjJ4oeATCAaWOicWwtgZqejy0cb5ZxjbkYOqb2SGD+oe6jDERFpVIOJwDn3lpkNAJKcc3sCJmUClwU9sjZsxRd7WLdjP//9vRGqKyQiYa/Ry0edc+XAnlrjioIaUTswLyOHpLgY1RUSkTZB1zS2sJ2FJSxa/RXfH9uXxDhV4hCR8KdE0MIWLMultKKSq3SSWETaiCNKBGY2tKUDaQ/KKyp5YWkOp34rmW/17BTqcEREmuRIjwjeadEo2onFG/L5ct9Brp4wMNShiIg0WYOd2Gb224YmAV2DEk0bNy8jh2O7xHPmCT1DHYqISJM1djbzeuBuoKSeaTOCE07btbWgkA+37OSes4cQo7pCItKGNJYIlgFrnHMf155gZg8HLaI2al5GDh2ijcvGqa6QiLQtjSWCi4GD9U1wzg0KTjhtU1FJOa8uz+O8kb3pkRQX6nBERJqlsT6MTs654laLpA17I2s735SUc81EXTIqIm1PY4ngjaoBM3s1+KG0Tc455mXkMKx3Z07q3y3U4YiINFtjiSCwSM5xR7JwM5tqZhvNbIuZ3d/IfN83M2dmaUeynlBalr2HDV99wzUTB6iukIi0SY0lAtfAcJOYWTQwGzgXGAbMMLNh9cyXhPcQnE+au45wMDcjm87xMUwfrbpCItI2NZYITjSz/Wb2DTDKH95vZt+Y2f4mLHs8sMU5t805Vwq8CEyvZ76fAY/SwInpcJa//yD/XPMVl6T1o2NsdKjDERE5Ig0mAudctHOus3MuyTkX4w9Xve7chGX3AXIDXuf546r5zz7u55z7v8YWZGY3mVmmmWUWFBQ0YdWt42+f5lJe6VRXSETatJDd+WRmUcCTeDetNco596xzLs05l9ajR4/gB9cEZRWV/PXTHE4b0oNBKXpyp4i0XcFMBNuBfgGv+/rjqiQBI4AlZpYNTAAWtpUTxv9a9zVf7y/RoyhFpM0LZiJYBgw2s0FmFgtcDiysmuic2+ecS3HODXTODQSWAtOcc5lBjKnFzM3Ipk/XjkwZqrpCItK2BS0R+E83uxV4G1gPvOScW2tmj5jZtGCttzVs+voblm7bzVUTBhAdpUtGRaRtC+ojtJxzi4BFtcY91MC8k4MZS0ual5FDbEwUl43rd/iZRUTCnMpkNtM3B8t4bUUe54/qTffE2FCHIyJy1JQImun1ldspKq3gmokDQx2KiEiLUCJoBuccczNyGNW3C6P7dQ11OCIiLUKJoBkytu1iS34hV+uSURFpR5QImmFeRg5dEzpwwYnHhjoUEZEWo0TQRDv2HeCddV9zWVo/4juorpCItB9KBE30t0++oNKprpCItD9KBE1QWl7JXz/NZUpqT/p1Twh1OCIiLUqJoAn+ufYrdhaWcLUeRSki7ZASQRPMy8hmQHICpw8Oj8qnIiItSYngMNbv2M+y7D1cdfIAolRXSETaISWCw5ibkUNcTBSXpPUNdSgiIkGhRNCIfQfKeGPldqaPPpauCaorJCLtkxJBI15dnseBMtUVEpH2TYmgAZWVjvlLcxjTvysj+nQJdTgiIkGjRNCAj7buZNvOIq7RJaMi0s4pETRgbkYOyYmxnDeyd6hDEREJKiWCemzfe4DF67/msnH9iItRXSERad+UCOrxwtIcAK5UXSERiQBKBLWUlFewYFkuZ5zQiz5dO4Y6HBGRoFMiqGXR6h3sKirVSWIRiRhKBLXMzcjhuJRETj0+JdShiIi0CiWCAGu272PlF3u5aoLqColI5FAiCDA3I5uOHaL5/ljVFRKRyKFE4NtbXMrfs77ke2P60KVjh1CHIyLSapQIfC9n5lFSXqmTxCIScZQI8OsKfZLDuIHdOKF351CHIyLSqpQIgPc2F5Czq5irVWVURCKQEgEwLyOHlE5xTB1+TKhDERFpdRGfCHJ3F5O+MZ8rxvcjNibiN4eIRKCIb/nmL80hyowrTtZJYhGJTBGdCA6WVbAgM5ezh/XimC7xoQ5HRCQkIjoRvLVqB3uLy7hal4yKSASL6EQwLyObwT07MfG45FCHIiISMkFNBGY21cw2mtkWM7u/nul3mdk6M1tlZovNrNV2zT/L3ctnefu4euIAzFRXSEQiV9ASgZlFA7OBc4FhwAwzG1ZrtpVAmnNuFPAK8KtgxVPb3IwcEmOjuXBMn9ZapYhIWArmEcF4YItzbptzrhR4EZgeOINzLt05V+y/XAq0SrW33UWlvLnqSy46qS9J8aorJCKRLZiJoA+QG/A6zx/XkBuBf9Q3wcxuMrNMM8ssKCg46sBeysyltLxSJ4lFRAiTk8VmdhWQBjxW33Tn3LPOuTTnXFqPHj2Oal0VlY75S3OYcFx3hvRKOqpliYi0B8FMBNuBfgGv+/rjajCzM4EHgWnOuZIgxgPAko355O05wNUTBgZ7VSIibUIwE8EyYLCZDTKzWOByYGHgDGY2BvgjXhLID2Is1eZm5NCrcxxnD+/VGqsTEQl7QUsEzrly4FbgbWA98JJzbq2ZPWJm0/zZHgM6AS+bWZaZLWxgcS0ie2cR720qYMb4/nSIDoteMRGRkIsJ5sKdc4uARbXGPRQwfGYw11/b/KU5xEQZV4zv35qrFREJa0FNBOHkQGkFL2Xmcs6IY+jZWXWFJDKUlZWRl5fHwYMHQx2KtJL4+Hj69u1Lhw5NvzQ+YhLBws+2s/9gOddM0CWjEjny8vJISkpi4MCBuoM+Ajjn2LVrF3l5eQwaNKjJ74uYjvIByYlccXJ/xg/qHupQRFrNwYMHSU5OVhKIEGZGcnJys48AI+aIYMJxyUxQcTmJQEoCkeVIvu+IOSIQEZH6KRGISNC8/vrrjB49usZfVFQU//hHvdVkWtySJUs4//zzm/Wep556iuLi4nqnffDBBwwfPpzRo0dz4MCBlgixSTp16hTU5SsRiEjQXHjhhWRlZVX/3XLLLUyaNIlzzjmnSe93zlFZWRnkKGtqLBG88MILPPDAA2RlZdGxY8fq8eXl5a0VXlAoEYhEksmT6/79/vfetOLi+qfPmeNN37mz7rRm2LRpE4888gjz5s0jKspreh577DHGjRvHqFGj+MlPfgJAdnY2qampXHPNNYwYMYLc3FzuvfdeRowYwciRI1mwYEG9y3/vvfeqjzrGjBnDN998A0BhYSEXX3wxQ4cO5corr8Q5B8DixYsZM2YMI0eO5IYbbqCkpITf/va3fPnll0yZMoUpU6bUWP6f/vQnXnrpJWbNmsWVV17JkiVLmDRpEtOmTWPYsGEcPHiQ66+/npEjRzJmzBjS09MBmDNnDt/73vc466yzGDhwIL/73e948sknGTNmDBMmTGD37t11Psvnn3/OxIkTGTlyJD/+8Y9rTKtvmx0tJQIRCbqysjKuuOIKnnjiCfr3927ofOedd9i8eTOffvopWVlZLF++nPfffx+AzZs3c8stt7B27VoyMzPJysris88+49133+Xee+9lx44dddbx+OOPM3v2bLKysvjggw+q99hXrlzJU089xbp169i2bRsfffQRBw8e5LrrrmPBggWsXr2a8vJynnnmGW6//XaOPfZY0tPTqxvyKv/xH//BtGnTeOyxx3jhhRcAWLFiBb/5zW/YtGkTs2fPxsxYvXo1f/vb37j22murr95Zs2YNr732GsuWLePBBx8kISGBlStXMnHiRObOnVvns9xxxx3MnDmT1atX07t37+rxjW2zoxExVw2JCLBkScPTEhIan56S0vj0RsyaNYvhw4dz2WWXVY975513eOeddxgzZgzg7blv3ryZ/v37M2DAACZMmADAhx9+yIwZM4iOjqZXr16cfvrpLFu2jGnTptVYx6mnnspdd93FlVdeyUUXXUTfvt7jTcaPH189PHr0aLKzs0lKSmLQoEEMGTIEgGuvvZbZs2dz5513NutzjR8/vvp6/Q8//JDbbrsNgKFDhzJgwAA2bdoEwJQpU0hKSiIpKYkuXbpwwQUXADBy5EhWrVpVZ7kfffQRr776KgBXX3019913X6Pb7LTTTmtW3LUpEYhIUC1ZsoRXX32VFStW1BjvnOOBBx7ghz/8YY3x2dnZJCYmHna5s2fP5rnnngNg0aJF3H///Xz3u99l0aJFnHrqqbz99tsAxMXFVb8nOjq6RfvzmxJn7RiioqKqX0dFRTUYT32XgTa0zY6WuoZEJGj27NnD9ddfz9y5c0lKqvn8j3POOYfnn3+ewsJCALZv305+ft0ixJMmTWLBggVUVFRQUFDA+++/z/jx4/nRj35UfRL62GOPZevWrYwcOZL77ruPcePGsWHDhgbjSk1NJTs7my1btgAwb948Tj/9dACSkpKqzy80x6RJk6q7jDZt2sQXX3xBampqs5cD3tHNiy++CFC9TGj6NmsuHRGISND84Q9/ID8/n5kzZ9YY/8ADD3DZZZexfv16Jk6cCHiXSM6fP5/o6Oga81544YVkZGRw4oknYmb86le/4phjjqmzrqeeeor09HSioqIYPnw45557LhkZGfXGFR8fz1/+8hcuueQSysvLGTduHDfffDMAN910E1OnTq0+V9BUt9xyCzNnzmTkyJHExMQwZ86cGkcCzfGb3/yGK664gkcffZTp0w894ffss8+ud5v17NnziNZTxarOoLcVaWlpLjMzM9RhiLQJ69ev54QTTgh1GNLK6vvezWy5cy6tvvnVNSQiEuGUCEREIpwSgYhIhFMiEBGJcEoEIiIRTolARCTCKRGISFBFR0fXKEOdnZ3NKaec0iLL3rt3L7+vKpoXZJMnTyYYl67PmDGDUaNG8etf/7rFl91UuqFMRIKqY8eOZGVl1Rj38ccft8iyqxLBLbfc0iLLa21fffUVy5Ytq77DOVB5eTkxMa3TRCsRiESIn765lnVf7m/RZQ47tjM/uWB4s9/XqVMnCgsLWbJkCQ8//DApKSmsWbOGsWPHMn/+fMyM5cuXc9ddd1FYWEhKSgpz5sypUYkT4P7772fr1q2MHj2as846i+9+97s8/vjjvPXWWwDceuutpKWlcd111zFw4ECuvfZa3nzzTcrKynj55ZcZOnQoRUVF3HbbbaxZs4aysjIefvhhpk+fzoEDB7j++uv57LPPGDp0aIMPolm8eDH33HNP9R3KzzzzDHFxcQ2uL9DZZ5/N9u3bGT16NE8//TSzZs1i9OjR1YX27r777mZv2yOhriERCaoDBw5UdwtdeOGFdabXVya6rKyM2267jVdeeYXly5dzww038OCDD9Z57y9/+UuOP/54srKyeOyxxw4bS0pKCitWrGDmzJk8/vjjAPz85z/nO9/5Dp9++inp6ence++9FBUV8cwzz5CQkMD69ev56U9/yvLly+ssr6Fy1o2tL9DChQur4580aRIApaWlZGZmtloSAB0RiESMI9lzbwn1dQ0Fqq9MdNeuXVmzZg1nnXUWABUVFXWOBo7ERRddBMDYsWN57bXXAK+088KFC6sb6oMHD/LFF1/w/vvvc/vttwMwatQoRo0aVWd5GzdubLScdX3rO5zAUt2tRYlAREKqvjLRzjmGDx9ep2hcbm5udS3/m2++malTp9aYHhMTU+PRllUPhqm9rsBy1M45Xn311SOuFNqY+tZ3OE0tbd2S1DUkImEnNTWVgoKC6kRQVlbG2rVr6devX3Xp6ZtvvrlOyegBAwawbt06SkpK2Lt3L4sXLz7sus455xyefvrp6kdYrly5EoDTTjuNv/71r4D3hLH6HiDTWDnrtkSJQETCTmxsLK+88gr33XcfJ554IqNHj673SqPk5GROPfVURowYwb333ku/fv249NJLGTFiBJdeemn1k7waM2vWLMrKyhg1ahTDhw9n1qxZAMycOZPCwkJOOOEEHnroIcaOHVvnvYHlrEeOHElUVFR1Oeu2RGWoRdoxlaGOTCpDLSIizaJEICIS4ZQIRNq5ttb9K0fnSL5vJQKRdiw+Pp5du3YpGUQI5xy7du0iPj6+We/TfQQi7Vjfvn3Jy8ujoKAg1KFIK4mPj6++Qa+plAhE2rEOHTowaNCgUIchYS6oXUNmNtXMNprZFjO7v57pcWa2wJ/+iZkNDGY8IiJSV9ASgZlFA7OBc4FhwAwzG1ZrthuBPc65bwG/Bh4NVjwiIlK/YB4RjAe2OOe2OedKgReB6bXmmQ78rz/8CnCGmVkQYxIRkVqCeY6gD5Ab8DoPOLmheZxz5Wa2D0gGdgbOZGY3ATf5LwvNbOMRxpRSe9lhri3F25ZihbYVb1uKFdpWvG0pVji6eAc0NKFNnCx2zj0LPHu0yzGzzIZusQ5HbSnethQrtK1421Ks0LbibUuxQvDiDWbX0HagX8Drvv64eucxsxigC7AriDGJiEgtwUwEy4DBZjbIzGKBy4GFteZZCFzrD18M/NvpzhcRkVYVtK4hv8//VuBtIBp43jm31sweATKdcwuBPwPzzGwLsBsvWQTTUXcvtbK2FG9bihXaVrxtKVZoW/G2pVghSPG2uTLUIiLSslRrSEQkwikRiIhEuHaVCMysn5mlm9k6M1trZnf44x82s+1mluX/nRfwngf8EhcbzeycVo4328xW+zFl+uO6m9m/zGyz/283f7yZ2W/9WFeZ2UmtHGtqwPbLMrP9ZnZnuGxbM3vezPLNbE3AuGZvSzO71p9/s5ldW9+6ghjvY2a2wY/pdTPr6o8faGYHArbxHwLeM9b/DW3xP1OL35DZQKzN/t4PV3ImyPEuCIg128yy/PGh3rYNtVmt+9t1zrWbP6A3cJI/nARswitv8TBwTz3zDwM+A+KAQcBWILoV480GUmqN+xVwvz98P/CoP3we8A/AgAnAJyHcztHAV3g3qITFtgVOA04C1hzptgS6A9v8f7v5w91aMd6zgRh/+NGAeAcGzldrOZ/6n8H8z3RuK8XarO/d/9sKHAfE+vMMa61tW2v6E8BDYbJtG2qzWvW3266OCJxzO5xzK/zhb4D1eHcvN2Q68KJzrsQ59zmwBa80RigFlt34X+B7AePnOs9SoKuZ9Q5BfABnAFudczmNzNOq29Y59z7elWe1Y2jOtjwH+Jdzbrdzbg/wL2Bqa8XrnHvHOVfuv1yKd+9Ng/yYOzvnljqvNZjLoc8Y1Fgb0dD33pSSM0GP19+rvxT4W2PLaMVt21Cb1aq/3XaVCAKZV8l0DPCJP+pW/1Dq+arDLOovg9FY4mhpDnjHzJabV0YDoJdzboc//BXQyx8OdayBLqfmf6Rw3LbQ/G0ZDjFXuQFvz6/KIDNbaWbvmdkkf1wfvBirtHa8zfnew2XbTgK+ds5tDhgXFtu2VpvVqr/ddpkIzKwT8Cpwp3NuP/AMcDwwGtiBd2gYDr7tnDsJr0Lrj8zstMCJ/p5IWF3fa97NgdOAl/1R4bptawjHbdkQM3sQKAde8EftAPo758YAdwF/NbPOoYrP1ya+93rMoOZOTFhs23rarGqt8dttd4nAzDrgbdAXnHOvATjnvnbOVTjnKoHnONRF0ZQyGEHjnNvu/5sPvO7H9XVVl4//b344xBrgXGCFc+5rCN9t62vutgx5zGZ2HXA+cKXfAOB3s+zyh5fj9bUP8WML7D5qtXiP4HsPh20bA1wELKgaFw7btr42i1b+7barROD3//0ZWO+cezJgfGBf+oVA1dUEC4HLzXtAziBgMN4JotaINdHMkqqG8U4UrqFm2Y1rgb8HxHqNf9XABGBfwKFja6qxRxWO2zZAc7fl28DZZtbN7+o42x/XKsxsKvD/gGnOueKA8T3Me74HZnYc3rbc5se838wm+L/9awI+Y7Bjbe733pSSM8F2JrDBOVfd5RPqbdtQm0Vr/3Zb+ix4KP+Ab+MdQq0Csvy/84B5wGp//EKgd8B7HsTbC9hIEK4KaCTW4/CunPgMWAs86I9PBhYDm4F3ge7+eMN70M9W/7OkhWD7JuIVBewSMC4sti1ectoBlOH1j954JNsSr29+i/93fSvHuwWvn7fqt/sHf97v+7+RLGAFcEHActLwGuGtwO/wqwW0QqzN/t79/4ub/GkPtua29cfPAW6uNW+ot21DbVar/nZVYkJEJMK1q64hERFpPiUCEZEIp0QgIhLhlAhERCKcEoGISIRTIpAjYmbOzJ4IeH2PmT3cQsueY2YXt8SyDrOeS8xsvZmlB4wbaYcqUe42s8/94XebuMxpdpjKmmZ2rJm9crTx+8vqZWZvmdln5lWwXNQSy21kfQMtoKqntA9Be1SltHslwEVm9j/OuZ2hDqaKmcW4Q4XbDudG4AfOuQ+rRjjnVuOVTcDM5gBvOedqNNqNrcN5j2Bt9EYp59yXeM/obgmP4BUb+40f26gWWq5EEB0RyJEqx3t+6n/WnlB7j97MCv1/J/uFvf5uZtvM7JdmdqWZfWpe3ffjAxZzppllmtkmMzvff3+0eTX7l5lX7OyHAcv9wMwWAuvqiWeGv/w1ZvaoP+4hvJt5/mxmjx3uw5rZEjN7yrznRtxhZheY2SfmFSt718x6+fNdZ2a/C9gOvzWzj/3Pe7E/vnqv2p//NTP7p3l15H8VsM4b/c//qZk9V7XcWnoTUBzNObfKf28nM1tsZiv8zz49YN0b/Ng2mdkLZnammX3kr3+8P9/DZjbPzDL88T+oZ5s09H30NrP3/SOpNXaokJuEKR0RyNGYDawKbLya4ETgBLwywduAPznnxpv3QI7bgDv9+Qbi1a85Hkg3s2/h3ea/zzk3zszigI/M7B1//pOAEc4rfVzNzI7Fq+0/FtiDV+31e865R8zsO3g19TObGHuscy7NX243YIJzzpnZf+CVhri7nvf0xks4Q/GOFOrrEhqNV3WyBNhoZk8DFcAs/3N9A/wb7y702mYDC8zsVrw7UP/iH3EcBC50zu03sxRgqZ8oAb4FXIJ3J+oy4Ao/xmnAf3Go5PEovJr3icBKM/u/Wuu+kfq/j4uAt51zPzevfENCPXFLGFEikCPmNzJzgduBA0182zLn10gys61AVUO+GpgSMN9LzitottnMtuE1pGcDowKONrrg1YYpBT6tnQR844AlzrkCf50v4D245I0mxhtoQcBwX7wGuDfeg1bqWzfAG/7nWFd11FCPxc65fX586/Ae+JMCvOec2+2PfxmvGFoNzrm3zauRMxWvIOBKMxsB7AV+YV5F20q8ksRV6//c7wLDzNb663dmthovAVf5u3PuAHDAvPMo4/FKIFRp6PtYBjxvXjG1N5xzge+RMKREIEfrKbwaLX8JGFeO3+1oZlF4DWWVkoDhyoDXldT8PdaufeLw6qzc5pyrUUzLzCYDRUcSfDMFruNp4Enn3EJ//Q838J7Az9vQow4D56mgmf8v/WTxV7wSym/hJbokoAcw1jlXZmbZQHw962vudxCo3u8DwE9A3wXmmNmTzrm5zflM0rp0jkCOit8IvYTXTVAlG68rBrzuhg5HsOhLzCzKP29wHF4Bs7eBmf6eJmY2xLzKrY35FDjdzFL8booZwHtHEE9tXThU5jcYzzZehhd3N/PKJ3+/vpnM7DtmluAPJ+F1pX3hx5fvJ4EpeEcZzTXdzOLNLBmY7McUqN7vw8wG4D385TngT3jdWxLGdEQgLeEJ4NaA188Bfzezz4B/cmR761/gNeKd8SpGHjSzP+F1XawwMwMKOMzjA51zO8y7nDMdbw/2/5xzLVFO+GHgZTPbg9d/P6gFllnNObfdzH6Btw12AxuAffXMOhb4nZlVHYX9yTm3zMw+B970u3sy/fc31yq87ZYC/Mw596V5T9Gq0tD3MRm418zKgEK8czsSxlR9VCRMmVkn51yhf0TwOvC8c+71Vlr3w0Chc+7x1lifhJa6hkTC18NmloVXE/9zjuwEt8hh6YhARCTC6YhARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREItz/B3eq9pPQmIfuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\n",
    "metrics_df.set_index(\"num_samples\").plot(ax=ax)\n",
    "plt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12580\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de_encoded['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zero-shot transfer remains competitive until about 750 training examples\n",
    "- zero-shot transfer is trained on 12580 German data, while fine-tune model is trained on around 750 French data to reach same f1 score on validation set\n",
    "- this zero-shot transfer is still promising: In our experience, getting domain experts to label even hundreds of documents can be costly, especially for NER, where the labeling process is fine-grained and time-consuming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Multiple Languages at Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the concatenate_datasets() function from HuggingFace Datasets to concatenate the German and French corpora together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def concatenate_splits(corpora):\n",
    "    multi_corpus = DatasetDict()\n",
    "    for split in corpora[0].keys():\n",
    "        multi_corpus[split] = concatenate_datasets(\n",
    "            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n",
    "    return multi_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 12580\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 6290\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 6290\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 4580\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 2290\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 2290\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de_encoded, panx_fr_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-a45db90d8976005d.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-027faa8044923ad4.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-3c051681789bf1d4.arrow\n"
     ]
    }
   ],
   "source": [
    "panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8580\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8580\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_de_fr_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\n",
    "training_args.push_to_hub = False\n",
    "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n",
    "    eval_dataset=panx_de_fr_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/torch/nn/modules/module.py:998: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2145' max='2145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2145/2145 02:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.176410</td>\n",
       "      <td>0.834339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.156061</td>\n",
       "      <td>0.850841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.161216</td>\n",
       "      <td>0.861809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2145, training_loss=0.1762014713598576, metrics={'train_runtime': 178.5427, 'train_samples_per_second': 288.334, 'train_steps_per_second': 12.014, 'total_flos': 1140291491923584.0, 'train_loss': 0.1762014713598576, 'epoch': 3.0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-98a1e557d4b73663.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-8b5c8586738b5484.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-c2045239f83cd18a.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='419' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [263/263 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-7077f32b8a439dce.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-d233b9514858a421.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-271f03ea5f605d61.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [de] dataset: 0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-e1449a5a04a2fb57.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-ec7e6d6f11d8f838.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-899a54fb46395a98.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [fr] dataset: 0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-98734a1056b4cd18.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-a6d97d034e66fcb1.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-58411fa278cc6fe8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [de-fr] model on [it] dataset: 0.795\n",
      "F1-score of [de-fr] model on [en] dataset: 0.669\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    f1 = evaluate_lang_performance(lang, trainer)\n",
    "    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs much better on the French split than before, matching the performance on the German test set. Interestingly, its performance on the Italian and English splits also improves by roughly 10 points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing the performance of fine-tuning on each language separately against multilingual learning on all the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=IntervalStrategy.EPOCH,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=40,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=xlm-roberta-base-finetuned-panx-de/runs/Jun29_16-55-15_quanlinux,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=1430,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "output_dir=xlm-roberta-base-finetuned-panx-de-fr,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=24,\n",
       "per_device_train_batch_size=24,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard', 'wandb'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=xlm-roberta-base-finetuned-panx-de,\n",
       "save_on_each_node=False,\n",
       "save_steps=1000000.0,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-7077f32b8a439dce.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-d233b9514858a421.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-271f03ea5f605d61.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-efb4c97d0cfc0aaf.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='573' max='573' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [573/573 00:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321250</td>\n",
       "      <td>0.789412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.273673</td>\n",
       "      <td>0.829170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.279438</td>\n",
       "      <td>0.837592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-e1449a5a04a2fb57.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-ec7e6d6f11d8f838.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-899a54fb46395a98.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-a30240633bf072e8.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317858</td>\n",
       "      <td>0.747385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>0.791616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.252727</td>\n",
       "      <td>0.808597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-98734a1056b4cd18.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-a6d97d034e66fcb1.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-58411fa278cc6fe8.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-f20aa3b0d727f989.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.520126</td>\n",
       "      <td>0.501027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.802200</td>\n",
       "      <td>0.450325</td>\n",
       "      <td>0.619794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.802200</td>\n",
       "      <td>0.402534</td>\n",
       "      <td>0.677778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpora = [panx_de_encoded]\n",
    "\n",
    "# Exclude German from iteration\n",
    "for lang in langs[1:]:\n",
    "    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n",
    "    # Fine-tune on monolingual corpus\n",
    "    ds_encoded = encode_panx_dataset(panx_ch[lang])\n",
    "    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n",
    "    # Collect F1-scores in common dict\n",
    "    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n",
    "    # Add monolingual corpus to list of corpora to concatenate\n",
    "    corpora.append(ds_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 12580\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 6290\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 6290\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 4580\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 2290\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 2290\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 1680\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 840\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 840\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 1180\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 590\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 590\n",
       "     })\n",
       " })]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve fine-tuned on each language’s corpus, the next step is to concatenate all the splits together to create a multilingual corpus of all four languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_encoded = concatenate_splits(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20020\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10010\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10010\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2505' max='2505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2505/2505 03:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.189392</td>\n",
       "      <td>0.810414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.175059</td>\n",
       "      <td>0.842264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.173889</td>\n",
       "      <td>0.852478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2505, training_loss=0.1864429466262787, metrics={'train_runtime': 209.4885, 'train_samples_per_second': 286.698, 'train_steps_per_second': 11.958, 'total_flos': 1312991477886144.0, 'train_loss': 0.1864429466262787, 'epoch': 3.0})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\n",
    "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n",
    "    eval_dataset=corpora_encoded[\"validation\"])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='419' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [263/263 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, lang in enumerate(langs):\n",
    "    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated on</th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tune on</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0.8664</td>\n",
       "      <td>0.7018</td>\n",
       "      <td>0.6779</td>\n",
       "      <td>0.5971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>0.8664</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.8115</td>\n",
       "      <td>0.7094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.8613</td>\n",
       "      <td>0.8651</td>\n",
       "      <td>0.8515</td>\n",
       "      <td>0.7544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated on      de      fr      it      en\n",
       "Fine-tune on                                \n",
       "de            0.8664  0.7018  0.6779  0.5971\n",
       "each          0.8664  0.8350  0.8115  0.7094\n",
       "all           0.8613  0.8651  0.8515  0.7544"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_data = {\"de\": f1_scores[\"de\"],\n",
    "               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n",
    "               \"all\": f1_scores[\"all\"]}\n",
    "f1_scores_df = pd.DataFrame(scores_data).T.round(4)\n",
    "f1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n",
    "                         inplace=True)\n",
    "f1_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multilingual learning can provide significant gains in performance, especially if the low-resource languages for cross-lingual transfer belong to similar language families. \n",
    "    - In our experiments we can see that **German, French, and Italian achieve similar performance in the all category**\n",
    "     => suggesting that these languages are **more similar to each other than to English.**\n",
    "\n",
    "\n",
    "- As a general strategy, it is a good idea to **focus attention on cross-lingual transfer *within language families***, especially when dealing with different scripts like Japanese.\n",
    "\n",
    "- this good performance generally **does not occur if the target language is significantly different from the one the base model was fine-tuned** on or was **not one of the 100 languages used during pretraining**.\n",
    "    - MAD-X are designed precisely for these low-resource scenarios, and since MAD-X is built on top of nlpt_pin01 Transformers you can easily adapt the code in this chapter to work with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "571.7px",
    "left": "10px",
    "top": "150px",
    "width": "262.017px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
