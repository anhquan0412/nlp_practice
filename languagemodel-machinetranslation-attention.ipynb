{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Learn the translation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- From parallel corpus/data (same text with multiple translations)\n",
    "- Break down P(x|y) to P(x,a|y), with a is **alignment** (correspondence b/t French sentence x and English sentence y, at word level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Alignment can be many-to-one (>1 words in E are represented by 1 word in F) or one-to-many (often called fertile word), for example 1 single F word **entarte** means: **hit someone with a pie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Even many-to-many translation (phrase-level)\n",
    "\n",
    "![](images/translation_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### How to compute argmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example of a decoding approach:\n",
    "https://youtu.be/XXtpJxZBa2c?t=830"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cons of statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Do machine translation with a single neural network\n",
    "\n",
    "The **architecture** is called: **sequence-to-sequence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Other uses of sequence-to-sequence (beside NMT):\n",
    "- Summarization (long text -> short text)\n",
    "- Dialogue (previous utterances -> next utterance)\n",
    "- Parsing (input text -> output parse as sentence)\n",
    "- Code generation (natural language -> Python code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **This is test time behavior**. We don't know how to train this yet\n",
    "- The final hidden state of encoder RNN (orange box) become the initial hidden state of decoder RNN\n",
    "- We fed ENGLISH WORD EMBEDDINGS to encoder RNN, and FRENCH WORD EMBEDDINGS to decoder RNN. Aka we have 2 different sets of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### A CONDITIONAL LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How to train these 2 RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The encoder RNN is just a language model (predicting the next English word), but there are no loss yet.\n",
    "\n",
    "- For decoder RNN: use the final hidden state of encoder RNN to initialize the hidden state, then train it as if it's a language model, on French words (with losses and all)\n",
    "\n",
    "![](images/translation_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **End-to-end backprop**: so BOTH the hidden state of decoder RNN and encoder RNN are updated during training. Even further, you can unfreeze the word embeddings of the 2 systems and finetune them as well.\n",
    "\n",
    "- Though of course you can use pretraining language models for encoder RNN and/or decoder RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Beam search for neural translation inference (test time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example: https://youtu.be/XXtpJxZBa2c?t=2132\n",
    "\n",
    "A finished beam search with 2 hypotheses with same length (k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that normally beam search should result in k hypotheses (from beam search), and each hypotheses ideally will have same size. That's why we don't  normalize the score for each of them (see more below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stopping criterion for beam search + pick the best hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since now we have specific stop criterions, thus considered hypotheses won't have same length anymore\n",
    "\n",
    "![](images/translation_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pros and cons of neural MT over statistical MT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Disadvantages:\n",
    "- less interpretable\n",
    "    - hard to interpret the neurons and weights of the RNN\n",
    "    - in contrast NMT has subcomponents, which is understandable since human design them\n",
    "- hard to control\n",
    "    - hard to put or reinforce hard-coded linguistic rule\n",
    "    - hard to put safety guidelines (safety concern: controversial translation, swear words ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with seq2seq model when you only use the final hidden state vector of encoder RNN to feed in decoder RNN, aka the **information bottleneck**\n",
    "\n",
    "![](images/translation_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each step of the decoder, use **direct connection to the encoder** to **focus on particular part** of **source sentence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why 'dot product'? Since hidden state of decoder tries to predict next English word, it will contain all the information to do so, and **with a high dot product (high similarities based on cosine similarity formula)** of that hidden state to the hidden state of encoder (of some French word), **it will say: \"oh this English word I am trying to predict might be highly associated with this French word\"**\n",
    "- note that dot product is **basic** to calculate the attention scores, which assumes 2 vectors have the same size (hidden states of encoder and decoder)\n",
    "- there is also **multiplicative attention** way to calculate the scores\n",
    "    - use a learnable weight matrix to learn the best way to calculate those scores\n",
    "    ![](images/translation_20.png)\n",
    "- also, **additive attention**, see more here: https://youtu.be/XXtpJxZBa2c?t=4517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...continue to attention steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## math equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last step, **'proceed as in the non-attention seq2seq model'** = basically treat the [at;st] as the concat[embedding,old_h] => orange arrow (matmul weight + tanh + dropout) => blue arrow (matmul weight + softmax) to have vocab probability and do cross entropy loss on actual y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros of using attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Solving the bottleneck problem**: instead of counting on only last hidden state of encoder RNN, **with attention, the decoder RNN can look at all the hidden states of encoder and make decision on which to focus on** (based on attention distribution)\n",
    "\n",
    "- **direct connection between encoder and decoder**, similar to shortcut connection (skip connection): as hidden state of decoder will connect to multiple hs of encoders => lots of connection => gradient flowing back easier since there is no bottleneck => **reduce vanishing gradient**\n",
    "\n",
    "- **a sense of interpretability** by looking at the attention distribution for each word. Similar to hard alignment of SMT, but this time the neural net learns the alignment by itself. TODO: reproduce this\n",
    "\n",
    "![](images/translation_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definition of attention\n",
    "\n",
    "https://youtu.be/XXtpJxZBa2c?t=4273"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
