{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=2&t=2433s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this course: to represent the meaning of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet library is an okay to process words using computers:\n",
    "    - extract synonym sets and hypernyms ('is a; relationships)\n",
    "- Wordnet however requires human labor to create and adapt => lot of missing words, some incorrect word synonyms or interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag-of-words (one-hot representation) cannot capture similarity between words (since each word vector are orthogonal, aka dot product is 0, aka different position of '1' => no natural notion of similarity\n",
    "- WordNet can help, but it has its cons (see above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of all the cons above, **vector needs to learn to encode the similarity themselves**\n",
    "\n",
    "- Word meaning is given by its context: other words that frequently appear close-by (position before and after)\n",
    "- With this, you can represent a word using a vector with positive/negative values(not one-hot), aka word vectors, aka word embeddings. Length of these vectors can go from 50 to 300, depends on the computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "- for each word in a text (center word c), look at the context words/neighbor words (o) around c.\n",
    "- Calculate **P(o|c)** using **word vectors' similarity** for c and o\n",
    "    - The idea is to optimize/update this word vector matrix (parameters) using gradient descent on some loss function (below) so that the **CORRECT NEIGHBOR WORD VECTORS (related to a given center word)** are updated in a way that maximize the probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/w2v_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that word vectors matrix is the variable delta** (the only variable to be optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the loss function and P(o|c) (aka P(neighbor|center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/w2v_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/w2v_prob_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: if neighbor word (n) and center word (c) have **high similarity (from dot product OF TWO DIFFERENT VERSION**: **neighbor (NOT CENTER) version of neighbor word** and **center version of center word)** => they are **close together in the vector space aka they have similar meaning**, P(n|c) is big.\n",
    "\n",
    "LOL REALLY??=> (because 2 center version of 2 words should be similar in meaning, like king and prince, instead of how good of neighbors they can be)\n",
    "\n",
    "\n",
    "Or in another word, **Softmax is used on the dot product to calculate P(n|c)**: bigger dot product, higher softmax => higher P(n|c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and update word vector matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **we have two matrices (neighbor version and center version) to update**, each has shape: vocab_size x # of dimension\n",
    "\n",
    "To optimize these matrices (parameters), we use gradient descent. **The only input for this model is the parameters them selves** (which is somewhat similar to collab filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula for derivative of loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/8rXD5-xhemo?t=4316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or: derivative of P(n|c) = nn - sum-w-in-vocab( P(w|c) ) * wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
