{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment explores two key concepts – sub-word modeling and convolutional networks – and applies them to the NMT system we built in the previous assignment. The Assignment 4 NMT model can be thought of as four stages:\n",
    "\n",
    "1. Embedding layer: Converts raw input text (for both the source and target sentences) to a sequence of dense word vectors via lookup.\n",
    "2. Encoder: A RNN that encodes the source sentence as a sequence of encoder hidden states.\n",
    "3. Decoder: A RNN that operates over the target sentence and attends to the encoder hidden states to produce a sequence of decoder hidden states.\n",
    "4. Output prediction layer: A linear layer with softmax that produces a probability distribution for the next target word on each decoder timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Section 1 of this assignment, we will replace (1) with a character-based convolutional encoder\n",
    "- and in Section 2 we will enhance (4) by adding a character-based LSTM decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for VocabEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quantran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "from utils import read_corpus, pad_sents, pad_sents_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0  # Pad Token\n",
    "            self.word2id['<s>'] = 1  # Start Token\n",
    "            self.word2id['</s>'] = 2  # End Token\n",
    "            self.word2id['<unk>'] = 3  # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "        ## Additions to the A4 code:\n",
    "        self.char_list = list(\n",
    "            \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n",
    "\n",
    "        self.char2id = dict()  # Converts characters to integers\n",
    "        self.char2id['∏'] = 0  # <pad> token\n",
    "        self.char2id['{'] = 1  # start of word token\n",
    "        self.char2id['}'] = 2  # end of word token\n",
    "        self.char2id['Û'] = 3  # <unk> token\n",
    "        for i, c in enumerate(self.char_list):\n",
    "            self.char2id[c] = len(self.char2id)\n",
    "        self.char_pad = self.char2id['∏']\n",
    "        self.char_unk = self.char2id['Û']\n",
    "        self.start_of_word = self.char2id[\"{\"]\n",
    "        self.end_of_word = self.char2id[\"}\"]\n",
    "        assert self.start_of_word + 1 == self.end_of_word\n",
    "\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}  # Converts integers to characters\n",
    "        ## End additions to the A4 code\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word\n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained\n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2charindices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[[self.char2id.get(c, self.char_unk) for c in (\"{\" + w + \"}\")] for w in s] for s in sents]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[self[w] for w in s] for s in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for\n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tensor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 1e\n",
    "        ### TODO:\n",
    "        ###     - Use `words2charindices()` from this file, which converts each character to its corresponding index in the\n",
    "        ###       character-vocabulary.\n",
    "        list_of_indices = self.words2charindices(sents) # list of list of list\n",
    "        ###     - Use `pad_sents_char()` from utils.py, which pads all words to max_word_length of all words in the batch,\n",
    "        ###       and pads all sentences to max length of all sentences in the batch. Read __init__ to see how to get\n",
    "        ###       index of character-padding token\n",
    "        sents_var = torch.Tensor(pad_sents_char(list_of_indices,self.char_pad)).permute(1,0,2)\n",
    "        ###     - Connect these two parts to convert the resulting padded sentences to a torch tensor.\n",
    "        return sents_var\n",
    "        ### HINT:\n",
    "        ###     - You may find .contiguous() useful after reshaping. Check the following links for more details:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 12, 2], [1, 41, 44, 51, 34, 2], [1, 54, 44, 50, 2]],\n",
       " [[1, 12, 2], [1, 40, 43, 44, 52, 2]]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = VocabEntry()\n",
    "temp.words2charindices([['I','love','you'],['I','know']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'k', 'n', 'o', 'w', '}']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[temp.id2char[i] for i in [1, 40, 43, 44, 52, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 1., 41., 44., 51., 34., 34., 34.,  2.],\n",
       "         [ 1., 40., 43., 44., 52.,  2.,  0.,  0.]],\n",
       "\n",
       "        [[ 1., 54., 44., 50.,  2.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = torch.Tensor(temp.to_input_tensor_char([['I','loveee','you'],['I','know']],1))\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., 41., 44., 51., 34., 34., 34.,  2.],\n",
       "         [ 1., 54., 44., 50.,  2.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., 40., 43., 44., 52.,  2.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.char2id['∏']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self,e_word):\n",
    "        super().__init__()\n",
    "        self.e_word = e_word\n",
    "        self.w_proj = nn.Linear(e_word,e_word)\n",
    "        self.w_gate = nn.Linear(e_word,e_word)\n",
    "        # init linear weight and bias?\n",
    "    def forward(self,x_conv_out):\n",
    "        \"\"\"\n",
    "         \n",
    "        raw_input: (max_sentence_length,bs,max_word_length aka m)\n",
    "        which should be output of to_input_tensor_char()\n",
    "        --char_emb()-->\n",
    "        x_emb: (max_sentence_length,bs,max_word_length,e_char)\n",
    "        with e_char is size of character embedding. \n",
    "        \n",
    "        --reshape()-->\n",
    "        x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        \n",
    "        --cnn()-->\n",
    "        x_conv: (max_sentence_length,bs,e_word,max_word_length-k+1)\n",
    "        with k is kernel size,e_word is the desired word embedding size\n",
    "        TODO: do a loop for each sentence?\n",
    "        \n",
    "        --relu_and_globalmaxpool()-->\n",
    "        x_conv_out: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        --high_way()-->\n",
    "        x_highway: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        --dropout()-->\n",
    "        x_word_emb: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        input: x_conv_out shape (bs,max_sentence_length,e_word)\n",
    "        output: x_highway shape (bs,max_sentence_length,e_word) (no dropout applied)\n",
    "        \"\"\"\n",
    "        \n",
    "        x_proj = F.relu(self.w_proj(x_conv_out))\n",
    "        x_gate = torch.sigmoid(self.w_gate(x_conv_out))\n",
    "        x_highway = x_gate * x_proj + (1-x_gate) * x_conv_out\n",
    "        return x_highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test highway\n",
    "temp_highway = Highway(2)\n",
    "temp_conv_out = torch.randn(4,3,2)\n",
    "temp_result = temp_highway(temp_conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,e_char,e_word,k):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(e_char,e_word,k)\n",
    "    def forward(self,x_reshaped):\n",
    "        \"\"\"\n",
    "        input: x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        \n",
    "        output: x_conv: (max_sentence_length,bs,e_word,max_word_length-k+1)\n",
    "        with k is kernel size,e_word is the desired word embedding size\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cnn\n",
    "temp_conv = nn.Conv1d(3,4,2) #in_channels,out_channels,kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_w = temp_conv.weight.data\n",
    "temp_b = temp_conv.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = torch.randn(1, 3, 2) # bs,in_channels aka emb size,number_of_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2538],\n",
       "         [-0.0983],\n",
       "         [ 0.3462],\n",
       "         [ 0.8006]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3 = temp_conv(temp_x)\n",
    "temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3.shape # bs,out_channels,new_number_of_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2538)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(temp_w[0] * temp_x[0]).sum() + temp_b[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
