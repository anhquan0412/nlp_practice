{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment explores two key concepts – sub-word modeling and convolutional networks – and applies them to the NMT system we built in the previous assignment. The Assignment 4 NMT model can be thought of as four stages:\n",
    "\n",
    "1. Embedding layer: Converts raw input text (for both the source and target sentences) to a sequence of dense word vectors via lookup.\n",
    "2. Encoder: A RNN that encodes the source sentence as a sequence of encoder hidden states.\n",
    "3. Decoder: A RNN that operates over the target sentence and attends to the encoder hidden states to produce a sequence of decoder hidden states.\n",
    "4. Output prediction layer: A linear layer with softmax that produces a probability distribution for the next target word on each decoder timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Section 1 of this assignment, we will replace (1) with a character-based convolutional encoder\n",
    "- and in Section 2 we will enhance (4) by adding a character-based LSTM decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for VocabEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quantran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "from utils import read_corpus, pad_sents, pad_sents_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0  # Pad Token\n",
    "            self.word2id['<s>'] = 1  # Start Token\n",
    "            self.word2id['</s>'] = 2  # End Token\n",
    "            self.word2id['<unk>'] = 3  # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "        ## Additions to the A4 code:\n",
    "        self.char_list = list(\n",
    "            \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n",
    "\n",
    "        self.char2id = dict()  # Converts characters to integers\n",
    "        self.char2id['∏'] = 0  # <pad> token\n",
    "        self.char2id['{'] = 1  # start of word token\n",
    "        self.char2id['}'] = 2  # end of word token\n",
    "        self.char2id['Û'] = 3  # <unk> token\n",
    "        for i, c in enumerate(self.char_list):\n",
    "            self.char2id[c] = len(self.char2id)\n",
    "        self.char_pad = self.char2id['∏']\n",
    "        self.char_unk = self.char2id['Û']\n",
    "        self.start_of_word = self.char2id[\"{\"]\n",
    "        self.end_of_word = self.char2id[\"}\"]\n",
    "        assert self.start_of_word + 1 == self.end_of_word\n",
    "\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}  # Converts integers to characters\n",
    "        ## End additions to the A4 code\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word\n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained\n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2charindices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[[self.char2id.get(c, self.char_unk) for c in (\"{\" + w + \"}\")] for w in s] for s in sents]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[self[w] for w in s] for s in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for\n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tensor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 1e\n",
    "        ### TODO:\n",
    "        ###     - Use `words2charindices()` from this file, which converts each character to its corresponding index in the\n",
    "        ###       character-vocabulary.\n",
    "        list_of_indices = self.words2charindices(sents) # list of list of list\n",
    "        ###     - Use `pad_sents_char()` from utils.py, which pads all words to max_word_length of all words in the batch,\n",
    "        ###       and pads all sentences to max length of all sentences in the batch. Read __init__ to see how to get\n",
    "        ###       index of character-padding token\n",
    "        sents_var = torch.tensor(pad_sents_char(list_of_indices,self.char_pad),dtype=torch.long, device=device).permute(1,0,2)\n",
    "        sents_var = sents_var.contiguous()\n",
    "        ###     - Connect these two parts to convert the resulting padded sentences to a torch tensor.\n",
    "        return sents_var\n",
    "        ### HINT:\n",
    "        ###     - You may find .contiguous() useful after reshaping. Check the following links for more details:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 12, 2], [1, 41, 44, 51, 34, 2], [1, 54, 44, 50, 2]],\n",
       " [[1, 12, 2], [1, 40, 43, 44, 52, 2]]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = VocabEntry()\n",
    "temp.words2charindices([['I','love','you'],['I','know']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'I', '}']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[temp.id2char[i] for i in [1, 12, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'k', 'n', 'o', 'w', '}']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[temp.id2char[i] for i in [1, 40, 43, 44, 52, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1, 12,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 12,  2,  0,  0,  0,  0,  0]],\n",
       "\n",
       "        [[ 1, 41, 44, 51, 34, 34, 34,  2],\n",
       "         [ 1, 40, 43, 44, 52,  2,  0,  0]],\n",
       "\n",
       "        [[ 1, 54, 44, 50,  2,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0]]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = temp.to_input_tensor_char([['I','loveee','you'],['I','know']],torch.device('cuda:0'))\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1, 41, 44, 51, 34, 34, 34,  2],\n",
       "         [ 1, 40, 43, 44, 52,  2,  0,  0]],\n",
       "\n",
       "        [[ 1, 54, 44, 50,  2,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0]]], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.char2id['∏']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self,e_word):\n",
    "        super().__init__()\n",
    "        self.e_word = e_word\n",
    "        self.w_proj = nn.Linear(e_word,e_word)\n",
    "        self.w_gate = nn.Linear(e_word,e_word)\n",
    "        # init linear weight and bias?\n",
    "    def forward(self,x_conv_out):\n",
    "        \"\"\"\n",
    "         \n",
    "        raw_input x_padded: (max_sentence_length,bs,max_word_length aka m)\n",
    "        which should be output of to_input_tensor_char()\n",
    "        \n",
    "        --char_emb()-->\n",
    "        x_emb: (max_sentence_length,bs,max_word_length,e_char)\n",
    "        with e_char is size of character embedding. \n",
    "        \n",
    "        --reshape()-->\n",
    "        x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        \n",
    "        --cnn()-->\n",
    "        x_conv: (max_sentence_length,bs,e_word,max_word_length-k+1)\n",
    "        with k is kernel size,e_word is the desired word embedding size\n",
    "        \n",
    "        --relu_and_globalmaxpool()-->\n",
    "        x_conv_out: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        --high_way()-->\n",
    "        x_highway: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        --dropout()-->\n",
    "        x_word_emb: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        input: x_conv_out shape (bs,max_sentence_length,e_word)\n",
    "        output: x_highway shape (bs,max_sentence_length,e_word) (no dropout applied)\n",
    "        \"\"\"\n",
    "        \n",
    "        x_proj = F.relu(self.w_proj(x_conv_out))\n",
    "        x_gate = torch.sigmoid(self.w_gate(x_conv_out))\n",
    "        x_highway = x_gate * x_proj + (1-x_gate) * x_conv_out\n",
    "        return x_highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test highway\n",
    "temp_highway = Highway(2)\n",
    "temp_conv_out = torch.randn(4,3,2)\n",
    "temp_result = temp_highway(temp_conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,e_char,e_word,k=5,padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(e_char, e_word, kernel_size = k, padding = padding)\n",
    "        self.mp1d = nn.AdaptiveMaxPool1d(1)\n",
    "        self.e_word = e_word\n",
    "    def forward(self,x_reshaped):\n",
    "        \"\"\"\n",
    "        input: x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        \n",
    "        output:  x_conv_out: (max_sentence_length,bs,e_word)\n",
    "            - e_word is the desired word embedding size\n",
    "        \"\"\"\n",
    "#         x_conv_out2 = []\n",
    "#         for each_sen in torch.split(x_reshaped,1,dim=0):\n",
    "#             each_sen = each_sen.squeeze(dim=0) # bs,e_char,max_word_length\n",
    "            \n",
    "#             x_conv = self.conv1d(each_sen) # (bs,e_word,max_word_length-k+1). \n",
    "#             #relu\n",
    "#             result = F.relu(x_conv) # (bs,e_word,max_word_length-k+1)\n",
    "#             #maxpool\n",
    "#             result = self.mp1d(result).squeeze(2) # (bs,e_word,1) to (bs,e_word) after squeezing\n",
    "            \n",
    "#             x_conv_out2.append(result)\n",
    "            \n",
    "#         x_conv_out2 = torch.stack(x_conv_out2,dim=0)\n",
    "        \n",
    "        # you can combine first and second dimension to avoid loop while conv1d\n",
    "        sent_length,bs = x_reshaped.shape[0],x_reshaped.shape[1]\n",
    "        new_view = (sent_length * bs,x_reshaped.shape[2],x_reshaped.shape[3])        \n",
    "        x_reshaped2 = x_reshaped.view(new_view)\n",
    "#         (max_sentence_length * bs ,e_char,max_word_length)\n",
    "        \n",
    "        x_conv = self.conv1d(x_reshaped2)  # (sent_length*bs,e_word,max_word_length-k+1).\n",
    "        x_conv_out = F.relu(x_conv)\n",
    "        x_conv_out = self.mp1d(x_conv_out).squeeze(-1) # (sent_length*bs,e_word,1) to (sent_length*bs,e_word)\n",
    "        x_conv_out = x_conv_out.view(sent_length,bs,self.e_word)\n",
    "        \n",
    "        return x_conv_out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cnn\n",
    "temp_conv = nn.Conv1d(3,4,2) #in_channels,out_channels,kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_w = temp_conv.weight.data\n",
    "temp_b = temp_conv.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = torch.randn(1, 3, 2) # bs,in_channels aka emb size,number_of_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2154],\n",
       "         [-0.8181],\n",
       "         [-0.2951],\n",
       "         [ 0.4281]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3 = temp_conv(temp_x)\n",
    "temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3.shape # bs,out_channels,new_number_of_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2154)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual calculation\n",
    "(temp_w[0] * temp_x[0]).sum() + temp_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing cnn + maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_conv = nn.Conv1d(3,4,2)\n",
    "temp_x = torch.randn(2, 3, 4)\n",
    "temp3 = temp_conv(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0903,  0.2882,  0.1889],\n",
       "          [ 0.0381,  0.4108,  0.0909],\n",
       "          [ 0.0468, -0.0842,  0.1885],\n",
       "          [-0.0687, -0.6686,  0.0187]],\n",
       " \n",
       "         [[ 0.0435,  0.7521,  0.0692],\n",
       "          [ 0.3959,  0.1063, -0.1477],\n",
       "          [ 1.0713, -0.3758, -0.7133],\n",
       "          [ 0.6434, -0.2213, -0.8657]]], grad_fn=<SqueezeBackward1>),\n",
       " torch.Size([2, 4, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3,temp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mp = nn.AdaptiveMaxPool1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2882],\n",
       "          [0.4108],\n",
       "          [0.1885],\n",
       "          [0.0187]],\n",
       " \n",
       "         [[0.7521],\n",
       "          [0.3959],\n",
       "          [1.0713],\n",
       "          [0.6434]]], grad_fn=<SqueezeBackward1>),\n",
       " torch.Size([2, 4, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp4 = temp_mp(temp3)\n",
    "temp4,temp4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp4.squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cnn = CNN(3,4,2)\n",
    "temp_x = torch.randn(3,1,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final = temp_cnn(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_final[0].shape\n",
    "\n",
    "# temp_final[0] == temp_final[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for ModelEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Do not change these imports; your module names should be\n",
    "#   `CNN` in the file `cnn.py`\n",
    "#   `Highway` in the file `highway.py`\n",
    "# Uncomment the following two imports once you're ready to run part 1(j)\n",
    "\n",
    "from cnn import CNN\n",
    "from highway import Highway\n",
    "\n",
    "\n",
    "# End \"do not change\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that converts input words to their CNN-based embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layer for one language\n",
    "        @param word_embed_size (int): Embedding size (dimensionality) for the output word\n",
    "        aka e_word\n",
    "        \n",
    "        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n",
    "\n",
    "        Hints: - You may find len(self.vocab.char2id) useful when create the embedding\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.word_embed_size = word_embed_size\n",
    "        self.vocab = vocab\n",
    "        self.e_char = 50\n",
    "        self.char_emb = nn.Embedding(len(vocab.char2id),self.e_char,padding_idx=vocab.char_pad)\n",
    "        self.highway = Highway(self.word_embed_size)\n",
    "        self.cnn = CNN(self.e_char,self.word_embed_size)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    def forward(self, x_padded):\n",
    "        \"\"\"\n",
    "        Looks up character-based CNN embeddings for the words in a batch of sentences.\n",
    "        @param x_padded: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n",
    "            each integer is an index into the character vocabulary\n",
    "        @param x_word_emb: Tensor of shape (sentence_length, batch_size, word_embed_size), containing the\n",
    "            CNN-based embeddings for each word of the sentences in the batch\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "#         raw_input x_padded: (max_sentence_length,bs,max_word_length aka m)\n",
    "#             - each integer is an index into the character vocabulary\n",
    "#             - this should be output of to_input_tensor_char()\n",
    "        \n",
    "#         --char_emb()-->\n",
    "#         x_emb: (max_sentence_length,bs,max_word_length,e_char)\n",
    "#             - with e_char is size of character embedding.      \n",
    "        x_emb = self.char_emb(x_padded)\n",
    "        \n",
    "#         --reshape()-->\n",
    "#         x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        x_reshaped = x_emb.permute(0,1,3,2)\n",
    "    \n",
    "#         --cnn()-->\n",
    "#         x_conv: (max_sentence_length,bs,e_word,max_word_length-k+1)\n",
    "#             - with k is kernel size,e_word is the desired word embedding size\n",
    "#             - do a loop for each sentence\n",
    "#         --relu_and_globalmaxpool()-->\n",
    "#         x_conv_out: (max_sentence_length,bs,e_word)\n",
    "        x_conv_out = self.cnn(x_reshaped)\n",
    "\n",
    "#         --high_way()-->\n",
    "#         x_highway: (max_sentence_length,bs,e_word)\n",
    "        x_highway = self.highway(x_conv_out)\n",
    "#         --dropout()-->\n",
    "#         x_word_emb: (max_sentence_length,bs,e_word)\n",
    "        x_word_emb = self.dropout(x_highway)\n",
    "        return x_word_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for new nmt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only small changes to a long codebase, so check nmt_model.py\n",
    "\n",
    "Note that there are more notes about how to use char_decoder and details on shape in this python file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: CharDecoderLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for char_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2019-20: Homework 5\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, char_embedding_size=50, target_vocab=None):\n",
    "        \"\"\" Init Character Decoder.\n",
    "\n",
    "        @param hidden_size (int): Hidden size of the decoder LSTM\n",
    "        @param char_embedding_size (int): dimensionality of character embeddings\n",
    "        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.\n",
    "        \"\"\"\n",
    "        super(CharDecoder, self).__init__()\n",
    "        self.target_vocab = target_vocab\n",
    "        self.charDecoder = nn.LSTM(char_embedding_size, hidden_size)\n",
    "        self.char_output_projection = nn.Linear(hidden_size, len(self.target_vocab.char2id))\n",
    "        self.decoderCharEmb = nn.Embedding(len(self.target_vocab.char2id), char_embedding_size,\n",
    "                                           padding_idx=self.target_vocab.char_pad)\n",
    "\n",
    "    def forward(self, input, dec_hidden=None):\n",
    "        \"\"\" Forward pass of character decoder.\n",
    "\n",
    "        @param input (Tensor): tensor of integers, shape (length, batch_size)\n",
    "        @param dec_hidden (tuple(Tensor, Tensor)): internal state of the LSTM before reading the input characters. \n",
    "        A tuple of two tensors of shape (1, batch, hidden_size)\n",
    "\n",
    "        @returns scores (Tensor): called s_t in the PDF, \n",
    "        shape (length, batch_size, self.vocab_size)\n",
    "        \n",
    "        @returns dec_hidden (tuple(Tensor, Tensor)): internal state of the LSTM after reading the input characters. \n",
    "        A tuple of two tensors of shape (1, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 2a\n",
    "        inp_emb = self.decoderCharEmb(input) #(length,bs,char_emb_size)\n",
    "        outp,new_dec_hidden = self.charDecoder(inp_emb,dec_hidden)\n",
    "        # outp shape: (length,bs,hidden_size)\n",
    "        scores = self.char_output_projection(outp) #(length,bs,vocab_size)\n",
    "        return scores,new_dec_hidden\n",
    "    \n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def train_forward(self, char_sequence, dec_hidden=None):\n",
    "        \"\"\" Forward computation during training.\n",
    "\n",
    "        @param char_sequence (Tensor): tensor of integers, shape (length, batch_size).\n",
    "        \"length\" here is max_word_length, aka number of chars for the longest words ever in the batch\n",
    "        \"batch_size\" is max_sent_length * bs\n",
    "        Note that \"length\" here and in forward() need not be the same.\n",
    "        @param dec_hidden (tuple(Tensor, Tensor)): initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch_size, hidden_size)\n",
    "\n",
    "        @returns The cross-entropy loss (Tensor), computed as the *sum* of cross-entropy losses of all the words in the batch.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 2b\n",
    "        ### TODO - Implement training forward pass.\n",
    "        ###\n",
    "        \n",
    "        X = char_sequence[:-1] # c0 to cn for training\n",
    "        s,dec_hidden = self.forward(X,dec_hidden)\n",
    "        \n",
    "        vocab_size = len(self.target_vocab.char2id)\n",
    "        target = char_sequence[1:].view(-1).contiguous() # c1 to cn+1 for testing, flatten for loss\n",
    "        scores = s.view(-1,vocab_size).contiguous()\n",
    "        \n",
    "        loss   = nn.CrossEntropyLoss(\n",
    "            reduction= \"sum\", # When compute loss_char_dec, we take the sum, not average\n",
    "            ignore_index=self.target_vocab.char_pad # not take into account pad character when compute loss\n",
    "        )\n",
    "        return loss(scores, target)\n",
    "        ### Hint: - Make sure padding characters do not contribute to the cross-entropy loss. \n",
    "        ### Check vocab.py to find the padding token's index.\n",
    "        ###       - char_sequence corresponds to the sequence x_1 ... x_{n+1} \n",
    "        ### (e.g., <START>,m,u,s,i,c,<END>). \n",
    "        ### Read the handout about how to construct input and target sequence of CharDecoderLSTM.\n",
    "        ###       - Carefully read the documentation for nn.CrossEntropyLoss \n",
    "        ### and our handout to see what this criterion have already included:\n",
    "        ###             https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def decode_greedy(self, initialStates, device, max_length=21):\n",
    "        \"\"\" Greedy decoding\n",
    "        This is called only in inference and only when word-model produces an UNK\n",
    "        If the translation contains any <UNK> tokens, \n",
    "        then for each of those positions, we use the word-based decoder’s combined\n",
    "        output vector to initialize the CharDecoderLSTM’s initial h0 and c0, then use CharDecoderLSTM to\n",
    "        generate a sequence of characters.\n",
    "\n",
    "        @param initialStates (tuple(Tensor, Tensor)): initial internal state of the LSTM, \n",
    "        a tuple of two tensors of size (1, batch_size, hidden_size)\n",
    "        NOTE that this is the new hidden state h (the combined output vector, which is AN ACTIVATION) \n",
    "            which is selectively chosen b/c word-model produces UNK tokens at these position\n",
    "        Our job is to create a matrix of size (bs,max_length) initialized with start_of_word token\n",
    "        and run it recursively through CharDecoderLSTM to generate a list of words (with length bs) \n",
    "        to replace these UNKs\n",
    "        \n",
    "        @param device: torch.device (indicates whether the model is on CPU or GPU)\n",
    "        @param max_length (int): maximum length of words to decode\n",
    "\n",
    "        @returns decodedWords (List[str]): a list (of length batch_size) of strings, \n",
    "                            each of which has length <= max_length.\n",
    "                              The decoded strings should NOT contain the start-of-word and end-of-word characters.\n",
    "        \"\"\"\n",
    "        bs = initialStates[0].shape[1]\n",
    "        dec_hidden = initialStates\n",
    "        inp = torch.LongTensor([[self.target_vocab.start_of_word]*bs]).to(device) # shape (1,bs). \n",
    "        \n",
    "        # Bptt is always 1 for inp (even for the loop below) instead of increment from 1\n",
    "        # because we already update dec_hidden for LSTM\n",
    "        # Otherwise, we will feed (1,bs) then (2,bs) ... (max_length-1,bs) into LSTM without\n",
    "        # feeding dec_hidden. This wastes computation however.\n",
    "        \n",
    "        results=torch.LongTensor([[0]*bs]).to(device) # (1,bs)\n",
    "        for s in range(max_length):\n",
    "            scores,dec_hidden = self.forward(inp,dec_hidden) #(1,bs,vocab_size)\n",
    "            inp = F.softmax(scores,dim=2).argmax(dim=2) # (1,bs)\n",
    "            # at s=0, even though inp is the same (1,bs vector filled with start_of_word index)\n",
    "            # new inp (1,bs) will have different values because the  values are determined by dec_hidden, not the LSTM itself \n",
    "            # dec_hidden has shape (1,bs,hidden_size), so basically like running a NN linear layer\n",
    "            results = torch.cat([results,inp.detach()])\n",
    "\n",
    "        decodedWords = []\n",
    "        results = torch.transpose(results[1:],0,1).detach().cpu().numpy() # (bs,max_length)\n",
    "        for row in results:\n",
    "            row_str=[]\n",
    "            for idx in row:\n",
    "                if idx == self.target_vocab.end_of_word: break\n",
    "                row_str.append(self.target_vocab.id2char[idx])\n",
    "            decodedWords.append(''.join(row_str))\n",
    "\n",
    "\n",
    "        return decodedWords\n",
    "\n",
    "        ### YOUR CODE HERE for part 2c\n",
    "        ### TODO - Implement greedy decoding.\n",
    "        ### Hints:\n",
    "        ###      - Use initialStates to get batch_size.\n",
    "        ###      - Use target_vocab.char2id and target_vocab.id2char \n",
    "        ### to convert between integers and characters\n",
    "        ###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.\n",
    "        ###      - You may find torch.argmax or torch.argmax useful\n",
    "        ###      - We use curly brackets as start-of-word and end-of-word characters. \n",
    "        ### That is, use the character '{' for <START> and '}' for <END>.\n",
    "\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final BLEU score after running on full dataset\n",
    "\n",
    "Train:\n",
    "- epoch 29, iter 196000, cum. loss 82.27, cum. ppl 54.79 cum. examples 64000\n",
    "- begin validation ...\n",
    "- validation: iter 196000, dev. ppl 66.432232\n",
    "- hit patience 1\n",
    "-...\n",
    "- epoch 29, iter 196330, avg. loss 86.20, avg. ppl 60.32 cum. examples 10537, speed 5536.83 words/sec, time elapsed 23065.94 sec\n",
    "- reached maximum number of epochs!\n",
    "\n",
    "\n",
    "Test:\n",
    "- Corpus BLEU: 36.56285318055767 which is > 36 ==> full 6 points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3297, -1.0496, -0.6587],\n",
       "         [-0.1881, -1.0775,  0.0590],\n",
       "         [ 1.9649,  0.3657,  0.3198],\n",
       "         [-0.4647, -0.6989,  0.4196]],\n",
       "\n",
       "        [[-0.6974,  1.5548, -0.4080],\n",
       "         [ 0.2737, -0.2731,  1.8505],\n",
       "         [ 0.7109, -0.6390, -0.6627],\n",
       "         [ 0.4823,  1.2234, -1.4263]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minor testing\n",
    "#(length,bs,vocab_size)\n",
    "temp = torch.randn(2,4,3)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 2, 0, 2],\n",
       "         [1, 2, 0, 1]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2=F.softmax(temp,dim=2).argmax(dim=2)\n",
    "temp2,temp2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
