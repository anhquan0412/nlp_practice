{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment explores two key concepts – sub-word modeling and convolutional networks – and applies them to the NMT system we built in the previous assignment. The Assignment 4 NMT model can be thought of as four stages:\n",
    "\n",
    "1. Embedding layer: Converts raw input text (for both the source and target sentences) to a sequence of dense word vectors via lookup.\n",
    "2. Encoder: A RNN that encodes the source sentence as a sequence of encoder hidden states.\n",
    "3. Decoder: A RNN that operates over the target sentence and attends to the encoder hidden states to produce a sequence of decoder hidden states.\n",
    "4. Output prediction layer: A linear layer with softmax that produces a probability distribution for the next target word on each decoder timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Section 1 of this assignment, we will replace (1) with a character-based convolutional encoder\n",
    "- and in Section 2 we will enhance (4) by adding a character-based LSTM decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quantran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "from utils import read_corpus, pad_sents, pad_sents_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0  # Pad Token\n",
    "            self.word2id['<s>'] = 1  # Start Token\n",
    "            self.word2id['</s>'] = 2  # End Token\n",
    "            self.word2id['<unk>'] = 3  # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "        ## Additions to the A4 code:\n",
    "        self.char_list = list(\n",
    "            \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n",
    "\n",
    "        self.char2id = dict()  # Converts characters to integers\n",
    "        self.char2id['∏'] = 0  # <pad> token\n",
    "        self.char2id['{'] = 1  # start of word token\n",
    "        self.char2id['}'] = 2  # end of word token\n",
    "        self.char2id['Û'] = 3  # <unk> token\n",
    "        for i, c in enumerate(self.char_list):\n",
    "            self.char2id[c] = len(self.char2id)\n",
    "        self.char_pad = self.char2id['∏']\n",
    "        self.char_unk = self.char2id['Û']\n",
    "        self.start_of_word = self.char2id[\"{\"]\n",
    "        self.end_of_word = self.char2id[\"}\"]\n",
    "        assert self.start_of_word + 1 == self.end_of_word\n",
    "\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}  # Converts integers to characters\n",
    "        ## End additions to the A4 code\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word\n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained\n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2charindices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[[self.char2id.get(c, self.char_unk) for c in (\"{\" + w + \"}\")] for w in s] for s in sents]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[self[w] for w in s] for s in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for\n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tensor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 1e\n",
    "        ### TODO:\n",
    "        ###     - Use `words2charindices()` from this file, which converts each character to its corresponding index in the\n",
    "        ###       character-vocabulary.\n",
    "        list_of_indices = self.words2charindices(sents) # list of list of list\n",
    "        ###     - Use `pad_sents_char()` from utils.py, which pads all words to max_word_length of all words in the batch,\n",
    "        ###       and pads all sentences to max length of all sentences in the batch. Read __init__ to see how to get\n",
    "        ###       index of character-padding token\n",
    "        sents_var = torchpad_sents_char(list_of_indices,self.char_pad).permute(1,0,2)\n",
    "        ###     - Connect these two parts to convert the resulting padded sentences to a torch tensor.\n",
    "        return sents_var\n",
    "        ### HINT:\n",
    "        ###     - You may find .contiguous() useful after reshaping. Check the following links for more details:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 12, 2], [1, 41, 44, 51, 34, 2], [1, 54, 44, 50, 2]],\n",
       " [[1, 12, 2], [1, 40, 43, 44, 52, 2]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = VocabEntry()\n",
    "temp.words2charindices([['I','love','you'],['I','know']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'k', 'n', 'o', 'w', '}']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[temp.id2char[i] for i in [1, 40, 43, 44, 52, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., 41., 44., 51., 34., 34., 34.,  2.],\n",
       "         [ 1., 54., 44., 50.,  2.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., 40., 43., 44., 52.,  2.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = torch.Tensor(temp.to_input_tensor_char([['I','loveee','you'],['I','know']],1))\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., 12.,  2.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 1., 41., 44., 51., 34., 34., 34.,  2.],\n",
       "         [ 1., 40., 43., 44., 52.,  2.,  0.,  0.]],\n",
       "\n",
       "        [[ 1., 54., 44., 50.,  2.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.char2id['∏']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\" Vocab encapsulating src and target langauges.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
    "        \"\"\" Init Vocab.\n",
    "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
    "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
    "        \"\"\"\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n",
    "        \"\"\" Build Vocabulary.\n",
    "        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n",
    "        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n",
    "        @param vocab_size (int): Size of vocabulary for both source and target languages\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n",
    "        \"\"\"\n",
    "        assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "        print('initialize source vocabulary ..')\n",
    "        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
    "\n",
    "        print('initialize target vocabulary ..')\n",
    "        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
    "\n",
    "        return Vocab(src, tgt)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\" Save Vocab to file as JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        \"\"\"\n",
    "        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\" Load vocabulary from JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        @returns Vocab object loaded from JSON dump\n",
    "        \"\"\"\n",
    "        entry = json.load(open(file_path, 'r'))\n",
    "        src_word2id = entry['src_word2id']\n",
    "        tgt_word2id = entry['tgt_word2id']\n",
    "\n",
    "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = docopt(__doc__)\n",
    "\n",
    "    print('read in source sentences: %s' % args['--train-src'])\n",
    "    print('read in target sentences: %s' % args['--train-tgt'])\n",
    "\n",
    "    src_sents = read_corpus(args['--train-src'], source='src')\n",
    "    tgt_sents = read_corpus(args['--train-tgt'], source='tgt')\n",
    "\n",
    "    vocab = Vocab.build(src_sents, tgt_sents, int(args['--size']), int(args['--freq-cutoff']))\n",
    "    print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n",
    "\n",
    "    vocab.save(args['VOCAB_FILE'])\n",
    "    print('vocabulary saved to %s' % args['VOCAB_FILE'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
