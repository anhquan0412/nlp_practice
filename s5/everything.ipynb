{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment explores two key concepts – sub-word modeling and convolutional networks – and applies them to the NMT system we built in the previous assignment. The Assignment 4 NMT model can be thought of as four stages:\n",
    "\n",
    "1. Embedding layer: Converts raw input text (for both the source and target sentences) to a sequence of dense word vectors via lookup.\n",
    "2. Encoder: A RNN that encodes the source sentence as a sequence of encoder hidden states.\n",
    "3. Decoder: A RNN that operates over the target sentence and attends to the encoder hidden states to produce a sequence of decoder hidden states.\n",
    "4. Output prediction layer: A linear layer with softmax that produces a probability distribution for the next target word on each decoder timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Section 1 of this assignment, we will replace (1) with a character-based convolutional encoder\n",
    "- and in Section 2 we will enhance (4) by adding a character-based LSTM decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for VocabEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quantran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "from utils import read_corpus, pad_sents, pad_sents_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0  # Pad Token\n",
    "            self.word2id['<s>'] = 1  # Start Token\n",
    "            self.word2id['</s>'] = 2  # End Token\n",
    "            self.word2id['<unk>'] = 3  # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "        ## Additions to the A4 code:\n",
    "        self.char_list = list(\n",
    "            \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n",
    "\n",
    "        self.char2id = dict()  # Converts characters to integers\n",
    "        self.char2id['∏'] = 0  # <pad> token\n",
    "        self.char2id['{'] = 1  # start of word token\n",
    "        self.char2id['}'] = 2  # end of word token\n",
    "        self.char2id['Û'] = 3  # <unk> token\n",
    "        for i, c in enumerate(self.char_list):\n",
    "            self.char2id[c] = len(self.char2id)\n",
    "        self.char_pad = self.char2id['∏']\n",
    "        self.char_unk = self.char2id['Û']\n",
    "        self.start_of_word = self.char2id[\"{\"]\n",
    "        self.end_of_word = self.char2id[\"}\"]\n",
    "        assert self.start_of_word + 1 == self.end_of_word\n",
    "\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}  # Converts integers to characters\n",
    "        ## End additions to the A4 code\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word\n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained\n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2charindices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[[self.char2id.get(c, self.char_unk) for c in (\"{\" + w + \"}\")] for w in s] for s in sents]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of sentences of words into list of list of indices.\n",
    "        @param sents (list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        return [[self[w] for w in s] for s in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for\n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tensor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE for part 1e\n",
    "        ### TODO:\n",
    "        ###     - Use `words2charindices()` from this file, which converts each character to its corresponding index in the\n",
    "        ###       character-vocabulary.\n",
    "        list_of_indices = self.words2charindices(sents) # list of list of list\n",
    "        ###     - Use `pad_sents_char()` from utils.py, which pads all words to max_word_length of all words in the batch,\n",
    "        ###       and pads all sentences to max length of all sentences in the batch. Read __init__ to see how to get\n",
    "        ###       index of character-padding token\n",
    "        sents_var = torch.tensor(pad_sents_char(list_of_indices,self.char_pad),dtype=torch.long, device=device).permute(1,0,2)\n",
    "        sents_var = sents_var.contiguous()\n",
    "        ###     - Connect these two parts to convert the resulting padded sentences to a torch tensor.\n",
    "        return sents_var\n",
    "        ### HINT:\n",
    "        ###     - You may find .contiguous() useful after reshaping. Check the following links for more details:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 12, 2], [1, 41, 44, 51, 34, 2], [1, 54, 44, 50, 2]],\n",
       " [[1, 12, 2], [1, 40, 43, 44, 52, 2]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = VocabEntry()\n",
    "temp.words2charindices([['I','love','you'],['I','know']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'k', 'n', 'o', 'w', '}']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[temp.id2char[i] for i in [1, 40, 43, 44, 52, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1, 12,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 12,  2,  0,  0,  0,  0,  0]],\n",
       "\n",
       "        [[ 1, 41, 44, 51, 34, 34, 34,  2],\n",
       "         [ 1, 40, 43, 44, 52,  2,  0,  0]],\n",
       "\n",
       "        [[ 1, 54, 44, 50,  2,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0]]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = temp.to_input_tensor_char([['I','loveee','you'],['I','know']],torch.device('cuda:0'))\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.char2id['∏']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ex5_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self,e_word):\n",
    "        super().__init__()\n",
    "        self.e_word = e_word\n",
    "        self.w_proj = nn.Linear(e_word,e_word)\n",
    "        self.w_gate = nn.Linear(e_word,e_word)\n",
    "        # init linear weight and bias?\n",
    "    def forward(self,x_conv_out):\n",
    "        \"\"\"\n",
    "         \n",
    "        raw_input x_padded: (max_sentence_length,bs,max_word_length aka m)\n",
    "        which should be output of to_input_tensor_char()\n",
    "        \n",
    "        --char_emb()-->\n",
    "        x_emb: (max_sentence_length,bs,max_word_length,e_char)\n",
    "        with e_char is size of character embedding. \n",
    "        \n",
    "        --reshape()-->\n",
    "        x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        \n",
    "        --cnn()-->\n",
    "        x_conv: (max_sentence_length,bs,e_word,max_word_length-k+1)\n",
    "        with k is kernel size,e_word is the desired word embedding size\n",
    "        TODO: do a loop for each sentence?\n",
    "        \n",
    "        --relu_and_globalmaxpool()-->\n",
    "        x_conv_out: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        --high_way()-->\n",
    "        x_highway: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        --dropout()-->\n",
    "        x_word_emb: (max_sentence_length,bs,e_word)\n",
    "        \n",
    "        input: x_conv_out shape (bs,max_sentence_length,e_word)\n",
    "        output: x_highway shape (bs,max_sentence_length,e_word) (no dropout applied)\n",
    "        \"\"\"\n",
    "        \n",
    "        x_proj = F.relu(self.w_proj(x_conv_out))\n",
    "        x_gate = torch.sigmoid(self.w_gate(x_conv_out))\n",
    "        x_highway = x_gate * x_proj + (1-x_gate) * x_conv_out\n",
    "        return x_highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test highway\n",
    "temp_highway = Highway(2)\n",
    "temp_conv_out = torch.randn(4,3,2)\n",
    "temp_result = temp_highway(temp_conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,e_char,e_word,k=5,padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(e_char, e_word, kernel_size = k, padding = padding)\n",
    "        self.mp1d = nn.AdaptiveMaxPool1d(1)\n",
    "        self.e_word = e_word\n",
    "    def forward(self,x_reshaped):\n",
    "        \"\"\"\n",
    "        input: x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        \n",
    "        output:  x_conv_out: (max_sentence_length,bs,e_word)\n",
    "            - e_word is the desired word embedding size\n",
    "        \"\"\"\n",
    "#         x_conv_out2 = []\n",
    "#         for each_sen in torch.split(x_reshaped,1,dim=0):\n",
    "#             each_sen = each_sen.squeeze(dim=0) # bs,e_char,max_word_length\n",
    "            \n",
    "#             x_conv = self.conv1d(each_sen) # (bs,e_word,max_word_length-k+1). \n",
    "#             #relu\n",
    "#             result = F.relu(x_conv) # (bs,e_word,max_word_length-k+1)\n",
    "#             #maxpool\n",
    "#             result = self.mp1d(result).squeeze(2) # (bs,e_word,1) to (bs,e_word) after squeezing\n",
    "            \n",
    "#             x_conv_out2.append(result)\n",
    "            \n",
    "#         x_conv_out2 = torch.stack(x_conv_out2,dim=0)\n",
    "        \n",
    "        # you can combine first and second dimension to avoid loop while conv1d\n",
    "        sent_length,bs = x_reshaped.shape[0],x_reshaped.shape[1]\n",
    "        new_view = (sent_length * bs,x_reshaped.shape[2],x_reshaped.shape[3])        \n",
    "        x_reshaped2 = x_reshaped.view(new_view)\n",
    "#         (max_sentence_length * bs ,e_char,max_word_length)\n",
    "        \n",
    "        x_conv = self.conv1d(x_reshaped2)  # (sent_length*bs,e_word,max_word_length-k+1).\n",
    "        x_conv_out = F.relu(x_conv)\n",
    "        x_conv_out = self.mp1d(x_conv_out).squeeze(-1) # (sent_length*bs,e_word,1) to (sent_length*bs,e_word)\n",
    "        x_conv_out = x_conv_out.view(sent_length,bs,self.e_word)\n",
    "        \n",
    "        return x_conv_out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cnn\n",
    "temp_conv = nn.Conv1d(3,4,2) #in_channels,out_channels,kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_w = temp_conv.weight.data\n",
    "temp_b = temp_conv.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = torch.randn(1, 3, 2) # bs,in_channels aka emb size,number_of_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6414],\n",
       "         [-0.8552],\n",
       "         [-0.3947],\n",
       "         [ 0.2296]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3 = temp_conv(temp_x)\n",
    "temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3.shape # bs,out_channels,new_number_of_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6414)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual calculation\n",
    "(temp_w[0] * temp_x[0]).sum() + temp_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing cnn + maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_conv = nn.Conv1d(3,4,2)\n",
    "temp_x = torch.randn(2, 3, 4)\n",
    "temp3 = temp_conv(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0569, -0.5749,  0.4177],\n",
       "          [ 0.2825,  0.5814,  0.1241],\n",
       "          [-0.0021, -0.3691,  0.4847],\n",
       "          [-0.3763, -0.5331,  0.4839]],\n",
       " \n",
       "         [[-0.0877,  0.4142, -0.3739],\n",
       "          [ 1.0563,  0.0531, -0.3173],\n",
       "          [-0.2251, -0.1101, -1.3916],\n",
       "          [-0.9606,  0.6323, -0.5496]]], grad_fn=<SqueezeBackward1>),\n",
       " torch.Size([2, 4, 3]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp3,temp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mp = nn.AdaptiveMaxPool1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4177],\n",
       "          [ 0.5814],\n",
       "          [ 0.4847],\n",
       "          [ 0.4839]],\n",
       " \n",
       "         [[ 0.4142],\n",
       "          [ 1.0563],\n",
       "          [-0.1101],\n",
       "          [ 0.6323]]], grad_fn=<SqueezeBackward1>),\n",
       " torch.Size([2, 4, 1]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp4 = temp_mp(temp3)\n",
    "temp4,temp4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp4.squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cnn = CNN(3,4,2)\n",
    "temp_x = torch.randn(3,1,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final = temp_cnn(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_final[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True]]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_final[0] == temp_final[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for ModelEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Do not change these imports; your module names should be\n",
    "#   `CNN` in the file `cnn.py`\n",
    "#   `Highway` in the file `highway.py`\n",
    "# Uncomment the following two imports once you're ready to run part 1(j)\n",
    "\n",
    "from cnn import CNN\n",
    "from highway import Highway\n",
    "\n",
    "\n",
    "# End \"do not change\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that converts input words to their CNN-based embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layer for one language\n",
    "        @param word_embed_size (int): Embedding size (dimensionality) for the output word\n",
    "        aka e_word\n",
    "        \n",
    "        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n",
    "\n",
    "        Hints: - You may find len(self.vocab.char2id) useful when create the embedding\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.word_embed_size = word_embed_size\n",
    "        self.vocab = vocab\n",
    "        self.e_char = 50\n",
    "        self.char_emb = nn.Embedding(len(vocab.char2id),self.e_char,padding_idx=vocab.char_pad)\n",
    "        self.highway = Highway(self.word_embed_size)\n",
    "        self.cnn = CNN(self.e_char,self.word_embed_size)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    def forward(self, x_padded):\n",
    "        \"\"\"\n",
    "        Looks up character-based CNN embeddings for the words in a batch of sentences.\n",
    "        @param x_padded: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n",
    "            each integer is an index into the character vocabulary\n",
    "        @param x_word_emb: Tensor of shape (sentence_length, batch_size, word_embed_size), containing the\n",
    "            CNN-based embeddings for each word of the sentences in the batch\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "#         raw_input x_padded: (max_sentence_length,bs,max_word_length aka m)\n",
    "#             - each integer is an index into the character vocabulary\n",
    "#             - this should be output of to_input_tensor_char()\n",
    "        \n",
    "#         --char_emb()-->\n",
    "#         x_emb: (max_sentence_length,bs,max_word_length,e_char)\n",
    "#             - with e_char is size of character embedding.      \n",
    "        x_emb = self.char_emb(x_padded)\n",
    "        \n",
    "#         --reshape()-->\n",
    "#         x_reshaped: (max_sentence_length,bs,e_char,max_word_length)\n",
    "        x_reshaped = x_emb.permute(0,1,3,2)\n",
    "    \n",
    "#         --cnn()-->\n",
    "#         x_conv: (max_sentence_length,bs,e_word,max_word_length-k+1)\n",
    "#             - with k is kernel size,e_word is the desired word embedding size\n",
    "#             - do a loop for each sentence\n",
    "#         --relu_and_globalmaxpool()-->\n",
    "#         x_conv_out: (max_sentence_length,bs,e_word)\n",
    "        x_conv_out = self.cnn(x_reshaped)\n",
    "\n",
    "#         --high_way()-->\n",
    "#         x_highway: (max_sentence_length,bs,e_word)\n",
    "        x_highway = self.highway(x_conv_out)\n",
    "#         --dropout()-->\n",
    "#         x_word_emb: (max_sentence_length,bs,e_word)\n",
    "        x_word_emb = self.dropout(x_highway)\n",
    "        return x_word_emb\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
