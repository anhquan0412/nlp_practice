{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cons of \"one representation\" of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, weâ€™ve basically had one representation of words: The word vectors that we learned about at the beginning\n",
    "    - Word2vec, GloVe, fastText\n",
    "    \n",
    "Problems:\n",
    "    - **Problem 1**: a word can have different meanings (\"sense\"), depending on the context, and now we try to collapse all the meanings of a word into 1 single vector, and hope that your model is complex enough that it can pick out the correct word meaning\n",
    "        - quick solution: define different word-senses for each word and build a vector for each\n",
    "\n",
    "=> what you want is to **build not a perfect word vector that can capture all of its meanings, but a word vector that is correct in your given context**\n",
    "\n",
    "    - **Problem 2**: on a more general view, not just different meanings but words have different aspects, including semantics, syntactic behavior, and register/connotations.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these problems have been solved by the process of building a language model that predicts next word; **by doing so you already generate a context-specific representation of words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag LM (pre ELMO) for Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/context_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"word embedding model\" is the word embedding from w2v/glove/fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/context_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token embedding is from w2v/glove/fasttext\n",
    "\n",
    "Note: \n",
    "- only concat the hidden state, not the embeddings of LM model and w2v model\n",
    "- This is **not end-to-end training for LM model**: by 'pretrained', we only use the output hidden state of LM model as input for the Sequence-tagging feature. In another word, the LM model is trained first ('pretrained') and is frozen after we extract the hidden states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
