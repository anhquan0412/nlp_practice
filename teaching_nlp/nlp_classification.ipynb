{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf68ba4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task:-Shopee-binary-sentiment-classification\" data-toc-modified-id=\"Task:-Shopee-binary-sentiment-classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task: Shopee binary sentiment classification</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-crash-files-(do-once)\" data-toc-modified-id=\"Convert-crash-files-(do-once)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Convert crash files (do once)</a></span></li><li><span><a href=\"#Load-csv\" data-toc-modified-id=\"Load-csv-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load csv</a></span></li></ul></li><li><span><a href=\"#Vietnamese-tokenization\" data-toc-modified-id=\"Vietnamese-tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vietnamese tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Pretokenization-(Vietnamese-word-tokenization)\" data-toc-modified-id=\"Step-1:-Pretokenization-(Vietnamese-word-tokenization)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 1: Pretokenization (Vietnamese word tokenization)</a></span></li><li><span><a href=\"#Step-2:-Model's-tokenization\" data-toc-modified-id=\"Step-2:-Model's-tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 2: Model's tokenization</a></span></li></ul></li><li><span><a href=\"#Use-HuggingFace-Dataset-to-store-and-tokenize-corpus\" data-toc-modified-id=\"Use-HuggingFace-Dataset-to-store-and-tokenize-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Use HuggingFace Dataset to store and tokenize corpus</a></span></li><li><span><a href=\"#Define-dataset-dict-and-perform-train/val-split\" data-toc-modified-id=\"Define-dataset-dict-and-perform-train/val-split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define dataset dict and perform train/val split</a></span></li><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model definition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-using-default-PhoBert\" data-toc-modified-id=\"Classification-using-default-PhoBert-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Classification using default PhoBert</a></span></li><li><span><a href=\"#Define-helper-function-for-training\" data-toc-modified-id=\"Define-helper-function-for-training-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Define helper function for training</a></span></li><li><span><a href=\"#Start-training\" data-toc-modified-id=\"Start-training-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Start training</a></span></li></ul></li><li><span><a href=\"#Prediction-interpretation\" data-toc-modified-id=\"Prediction-interpretation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Prediction interpretation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-report-and-confusion-matrix\" data-toc-modified-id=\"Classification-report-and-confusion-matrix-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Classification report and confusion matrix</a></span></li><li><span><a href=\"#Most-confident-prediction:-right-vs-wrong\" data-toc-modified-id=\"Most-confident-prediction:-right-vs-wrong-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Most confident prediction: right vs wrong</a></span></li></ul></li><li><span><a href=\"#Extract-hidden-states\" data-toc-modified-id=\"Extract-hidden-states-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Extract hidden states</a></span><ul class=\"toc-item\"><li><span><a href=\"#Redefine-dataset-dict\" data-toc-modified-id=\"Redefine-dataset-dict-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Redefine dataset dict</a></span></li><li><span><a href=\"#Retrain-model\" data-toc-modified-id=\"Retrain-model-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Retrain model</a></span></li><li><span><a href=\"#Find-neighbors\" data-toc-modified-id=\"Find-neighbors-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Find neighbors</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10c8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71235da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c8a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4ec21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af5dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel # body only\n",
    "#inherit this to load pretrained weight\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6d3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data_crash')\n",
    "RAW_PATH = Path('raw_crash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01542e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5c682",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Task: Shopee binary sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f326",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83962f99",
   "metadata": {},
   "source": [
    "## Convert crash files (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696ba470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if not os.path.exists(DATA_PATH):\n",
    "#     os.mkdir(DATA_PATH)\n",
    "\n",
    "# ### Cleaning training file\n",
    "\n",
    "# train = open(RAW_PATH/\"train.crash\").readlines()\n",
    "# id_locations = []\n",
    "# label_locations = []\n",
    "# for idx, line in tqdm(enumerate(train)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"train_\"):\n",
    "#         id_locations.append(idx)\n",
    "#     elif line == \"0\" or line == \"1\":\n",
    "#         label_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for id_loc, l_loc in tqdm(zip(id_locations, label_locations)):\n",
    "#     line_id = train[id_loc].strip()\n",
    "#     label = train[l_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(train[id_loc + 1: l_loc])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\\t{label}\")\n",
    "\n",
    "# with open(DATA_PATH/\"train.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\tlabel\\n\")\n",
    "#     f.write(\"\\n\".join(data))\n",
    "\n",
    "# ### Cleaning test file\n",
    "\n",
    "# test = open(RAW_PATH/\"test.crash\").readlines()\n",
    "# id_locations = []\n",
    "# for idx, line in tqdm(enumerate(test)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"test_\"):\n",
    "#         id_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for i, id_loc in tqdm(enumerate(id_locations)):\n",
    "#     if i >= len(id_locations) - 1:\n",
    "#         end = len(test)\n",
    "#     else:\n",
    "#         end = id_locations[i + 1]\n",
    "#     line_id = test[id_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(test[id_loc + 1:end])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\")\n",
    "\n",
    "# with open(DATA_PATH/\"test.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\n\")\n",
    "#     f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe363",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f367e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {str(DATA_PATH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv',sep='\\t').fillna(\"###\")\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv',sep='\\t').fillna(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5cb72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>Ngon. Được nhiều ng thích</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>Shop phục vụ kém Shop phục vụ kém Shop phục vụ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>Chất lượng sản phẩm rất kémShop lừa đảo ng khá...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11294</th>\n",
       "      <td>Nhỏ nhưng có võ! kiểm soát dầu rất tốt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14689</th>\n",
       "      <td>chất liệu vải mỏng quá khuyên anh em nên mua l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>Giao sai hang cho khach mat uy tin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16056</th>\n",
       "      <td>Rất tuyệt bà con ah. Sử dụng 3 lần. Da mặt mịn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>Ko nhận đc áo do shop quên hàng những đã bù ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15159</th>\n",
       "      <td>Sp nói từ 6 đến 12 tháng mà mang ko vừa. Sp ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>Màu rất là đẹp, chất phất cực kỳ pigmented luô...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6292</th>\n",
       "      <td>làm theo hướng dẫn nhưng không đông lúc bỏ máy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7401</th>\n",
       "      <td>Máy may rất ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>Đồ đểu xấu quá không chấp nhận kiểu bán hàng n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>Ko có phiếu bảo hành hả shop. Vs e thấy chạy đ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>Cam on shop nka e tkjk lam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>đồng hồ bị hư giao hàng thiêu 1 máy sấy...ib n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>Đóng gói sản phẩm rất đẹp và chắc chắn. Shop p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>Sách dịch một cách máy móc từ 1 cuốn sách nước...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768</th>\n",
       "      <td>Đợt này bị hụt cân điện tử cầm tay nhưng vẫn r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15633</th>\n",
       "      <td>Đóng gói sản phẩm rất đẹp và chắc chắn. Shop p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1104                           Ngon. Được nhiều ng thích      0\n",
       "13556  Shop phục vụ kém Shop phục vụ kém Shop phục vụ...      1\n",
       "8622   Chất lượng sản phẩm rất kémShop lừa đảo ng khá...      1\n",
       "11294             Nhỏ nhưng có võ! kiểm soát dầu rất tốt      0\n",
       "14689  chất liệu vải mỏng quá khuyên anh em nên mua l...      1\n",
       "11571                 Giao sai hang cho khach mat uy tin      1\n",
       "16056  Rất tuyệt bà con ah. Sử dụng 3 lần. Da mặt mịn...      0\n",
       "2829   Ko nhận đc áo do shop quên hàng những đã bù ti...      0\n",
       "15159  Sp nói từ 6 đến 12 tháng mà mang ko vừa. Sp ch...      1\n",
       "1304   Màu rất là đẹp, chất phất cực kỳ pigmented luô...      0\n",
       "6292   làm theo hướng dẫn nhưng không đông lúc bỏ máy...      1\n",
       "7401                                      Máy may rất ok      0\n",
       "5023   Đồ đểu xấu quá không chấp nhận kiểu bán hàng n...      1\n",
       "11396  Ko có phiếu bảo hành hả shop. Vs e thấy chạy đ...      0\n",
       "4710                          Cam on shop nka e tkjk lam      0\n",
       "5117   đồng hồ bị hư giao hàng thiêu 1 máy sấy...ib n...      1\n",
       "912    Đóng gói sản phẩm rất đẹp và chắc chắn. Shop p...      0\n",
       "4509   Sách dịch một cách máy móc từ 1 cuốn sách nước...      1\n",
       "14768  Đợt này bị hụt cân điện tử cầm tay nhưng vẫn r...      0\n",
       "15633  Đóng gói sản phẩm rất đẹp và chắc chắn. Shop p...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','label']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4a826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['San pham rat tot cam on shop Chất lượng sản phẩm tuyệt vời' 0]\n",
      " ['Shop phục vụ rất kémĐặt size 37, Shop gửi về 1 chiếc size 36, 1 chiếc size 37 K muốn trả hàng nên đặt thêm 1 đôi để Shop gửi bù và Shop chịu phí ship nhưng lại gửi đúng 1 đôi 2 chiếc size 36, và k chịu phi nên phải trả lại hàng Sau đó cũng k có ý kiến gì về trách nhiệm của Shop đối với đôi cũ'\n",
      "  1]\n",
      " ['Tôi mua sản phẩm cách đây vài tháng do mới đầu ko để ý nhưg sau phát hiện ra đệm tai 1 bên cực mềm 1 bên thì cứng giống hệt lót của dòng m20x. Ko thể chấp nhận đc có cửa hàng ntnay trên tiki.'\n",
      "  1]\n",
      " ['Bình cũ, rượu cũ, chỉ có nút chai là mới...ip chẳn thì mới hoàn toàn mới. Còn ip lẻ thì na ná cái cũ...'\n",
      "  1]\n",
      " ['Rất đáng tiền..Shop phục vụ rất tốt' 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['text','label']].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377a4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thích lắm luôn giày ren kem lấp lánh sang mà vẫn trẻ trung']\n",
      " ['Sao truyện của mình lại không có đai obi vậy......']\n",
      " ['Đóng gói sản phẩm rất đẹp và chắc chắn nhớ tới snack trẻ em hồi xưa']\n",
      " ['giày đẹp lắm shop ơi']\n",
      " ['Cam ơn shop nhìu nghen']]\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text']].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ceee",
   "metadata": {},
   "source": [
    "# Vietnamese tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c606",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a346",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it “cleaner”**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be97150",
   "metadata": {},
   "source": [
    "## Step 1: Pretokenization (Vietnamese word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b9e4",
   "metadata": {},
   "source": [
    "https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378087",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from underthesea import word_tokenize\n",
    ">>> sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
    "\n",
    ">>> word_tokenize(sentence)\n",
    "['Chàng trai', '9X', 'Quảng Trị', 'khởi nghiệp', 'từ', 'nấm', 'sò']\n",
    "\n",
    ">>> word_tokenize(sentence, format=\"text\")\n",
    "'Chàng_trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43cefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_tokenize(sen,split_word=False):\n",
    "    # optional step: fix the whitespace between words\n",
    "    sen = \" \".join(sen.split())\n",
    "    sens = sent_tokenize(sen)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen+=word_tokenize(sen,format='text' if not split_word else None)\n",
    "    \n",
    "    if not split_word:\n",
    "        return ''.join(tokenized_sen)\n",
    "    return ['_'.join(words.split(' ')) for words in tokenized_sen]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63754082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '?', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời? Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f171898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '😌_😌', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c089cc",
   "metadata": {},
   "source": [
    "Apply on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32166f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [apply_word_tokenize(s[0],True) for s in train_df[['text']].values]\n",
    "train_label = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90044db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [apply_word_tokenize(s[0],True) for s in test_df[['text']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc34d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'tuyệt_vời',\n",
       "  '.',\n",
       "  'y',\n",
       "  'hình',\n",
       "  'chụp',\n",
       "  '.',\n",
       "  'đáng',\n",
       "  'tiền'],\n",
       " ['Hjhj_shop',\n",
       "  'giao',\n",
       "  'hàng',\n",
       "  'nhanh',\n",
       "  'quá',\n",
       "  '.',\n",
       "  'Đẹp',\n",
       "  'lắm',\n",
       "  'ạ',\n",
       "  'bé',\n",
       "  'nhà',\n",
       "  'm',\n",
       "  'rất',\n",
       "  'thích'],\n",
       " ['nhìn', 'đẹp', 'phết', 'nhỉ', '..'],\n",
       " ['Đóng_gói',\n",
       "  'rất',\n",
       "  'đẹp',\n",
       "  '.',\n",
       "  'Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'rất',\n",
       "  'tốt',\n",
       "  'Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'tuyệt_vời'],\n",
       " ['Săn', 'đc', 'với', 'giá', '11', 'k', '.', 'Toẹt', 'vời']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2f6f",
   "metadata": {},
   "source": [
    "## Step 2: Model's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed8d0",
   "metadata": {},
   "source": [
    "To use pretrained language model such as BERT, GPT, Roberta... We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n",
    "\n",
    "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their hub. In this notebook, we use PhoBERT-base (vinai/phobert-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f842db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e1bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_explain(inp,split_word):\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()\n",
    "\n",
    "\n",
    "def two_step_tokenization_explain(inp,split_word=False):\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Pretokenization ---')\n",
    "    tok = apply_word_tokenize(inp,split_word)\n",
    "    print(tok)\n",
    "    print()\n",
    "    tokenizer_explain(tok,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 4, 7079, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời, phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7978be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "Chất_lượng sản_phẩm tuyệt_vời .Phấn mịn , đóng_gói sản_phẩm rất đẹp và chắc_chắn\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 2586, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.@@', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời.Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời. Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7f2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '😌_😌', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 3, 1751, 3, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '<unk>', '_@@', '<unk>', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời <unk> _<unk>. Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c4f",
   "metadata": {},
   "source": [
    "# Use HuggingFace Dataset to store and tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2481f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f388127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # pad to model's allowed max length, which is max_sequence_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c4b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text[:4],\n",
    "                        'label':train_label[:4],\n",
    "                        }\n",
    "                    )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1572e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8b3fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df12176ac4c4965a635148e2d438e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10db51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013debff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dung', 'dc', 'sp', 'tot', 'cam', 'on_shop', 'Đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời'], ['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Son', 'mịn', 'nhưng', 'khi', 'đánh', 'lên', 'không', 'như', 'màu', 'trên', 'ảnh'], ['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', 'nhưng', 'k', 'có', 'hộp', 'k', 'có', 'dây', 'giày', 'đen', 'k', 'có', 'tất'], [':', '(', '(', 'Mình', 'hơi', 'thất_vọng', '1', 'chút', 'vì', 'mình', 'đã', 'kỳ_vọng', 'cuốn', 'sách', 'khá', 'nhiều', 'hi_vọng', 'nó', 'sẽ', 'nói', 'về', 'việc', 'học_tập', 'của', 'cách', 'sinh_viên', 'trường', 'Harvard', 'ra_sao', 'những', 'nỗ_lực', 'của', 'họ', 'như', 'thế_nào', '4', 'h', 'sáng', '?', 'tại_sao', 'họ', 'lại', 'phải', 'thức', 'dậy', 'vào', 'thời_khắc', 'đấy', '?', 'sau', 'đó', 'là', 'cả', 'một', 'câu_chuyện', 'ra_sao', '.', 'Cái', 'mình', 'thực_sự', 'cần', 'ở', 'đây', 'là', 'câu_chuyện', 'ẩn', 'dấu', 'trong', 'đó', 'để', 'tự', 'bản_thân', 'mỗi', 'người', 'cảm_nhận', 'và', 'đi', 'sâu', 'vào', 'lòng', 'người', 'hơn', '.', 'Còn', 'cuốn', 'sách', 'này', 'chỉ', 'đơn_thuần', 'là', 'cuốn', 'sách', 'dạy', 'kĩ_năng', 'mà', 'hầu_như', 'sách', 'nào', 'cũng', 'đã', 'có', '.', 'BUồn', '...']]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f6c2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3556, 1236, 1894, 36150, 2225, 1204, 2947, 1672, 20811, 54922, 55662, 1685, 265, 59, 258, 6, 994, 6869, 265, 1819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 5, 16332, 5451, 51, 26, 480, 72, 17, 42, 412, 34, 284, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 51, 1947, 10, 2275, 1947, 10, 1747, 2466, 989, 1947, 10, 7328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 27, 20, 20, 3000, 1329, 2804, 99, 2013, 90, 68, 14, 2109, 1088, 713, 281, 36, 4876, 231, 38, 96, 28, 49, 1227, 7, 139, 649, 212, 9913, 57964, 3075, 21, 773, 7, 86, 42, 1279, 163, 1664, 298, 114, 2393, 86, 44, 41, 2908, 1764, 33, 9171, 1582, 114, 53, 37, 8, 94, 16, 876, 57964, 3075, 5, 2510, 68, 742, 115, 25, 97, 8, 876, 4592, 3309, 12, 37, 24, 385, 744, 205, 18, 2601, 6, 57, 808, 33, 605, 18, 48, 5, 631, 1088, 713, 23, 66, 5284, 8, 1088, 713, 940, 10685, 64, 2903, 713, 142, 32, 14, 10, 5, 924, 1878, 5460, 135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1e74a",
   "metadata": {},
   "source": [
    "# Define dataset dict and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1925dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b22df4c38434b7888efe9130ff5f643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63a6238ea644b1daa3c5114b5d062df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text,\n",
    "                        'label':train_label,\n",
    "                        }\n",
    "                    )\n",
    "test_dataset = Dataset.from_dict(\n",
    "                        {'text': test_text,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True)\n",
    "test_dataset_tokenized= test_dataset.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d637faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "train_dataset_tokenized = train_dataset_tokenized.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2184388",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized = DatasetDict()\n",
    "main_dataset_tokenized['train'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8)))\n",
    "main_dataset_tokenized['validation'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8),train_dataset_tokenized.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed94e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3218\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ebcace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.580154\n",
       "1    0.419846\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['train']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d049de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.563704\n",
       "1    0.436296\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['validation']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4fb0",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40541781",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eecb3a",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089eb60",
   "metadata": {},
   "source": [
    "## Classification using default PhoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d414ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "# from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "# from transformers.models.roberta.modeling_roberta import RobertaModel \n",
    "# from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f89e470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "522bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_init(get_hidden=False):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2,output_hidden_states=get_hidden)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780da385",
   "metadata": {},
   "source": [
    "## Define helper function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77905e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gc\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # pred: EvalPrediction object \n",
    "    # (which is a named tuple with predictions and label_ids attributes)\n",
    "    labels = pred.label_ids\n",
    "    if isinstance(pred.predictions,tuple):\n",
    "        preds = pred.predictions[0].argmax(-1)\n",
    "    else:\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1,\"accuracy\": acc}\n",
    "\n",
    "\n",
    "\n",
    "def finetune(lr,bs,wd,epochs,ddict,tokenizer=tokenizer,o_dir = './outputs',logging=False,model_init=base_model_init):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not logging:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    else:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                               logging_dir=os.path.join(o_dir, 'log'),\n",
    "                                logging_steps = len(ddict[\"train\"]) // bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd)\n",
    "\n",
    "    # instantiate trainer\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=ddict['train'],#.shard(200, 0),    # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "        eval_dataset=ddict['validation'],#.shard(100, 0), # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "#         data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a25275",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9cdad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12869\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 09:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288897</td>\n",
       "      <td>0.907470</td>\n",
       "      <td>0.908950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.298636</td>\n",
       "      <td>0.897811</td>\n",
       "      <td>0.899627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.306182</td>\n",
       "      <td>0.909048</td>\n",
       "      <td>0.910193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-403\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-403/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-403/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-403/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-806\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-806/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-806/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-806/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-1209\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-1209/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-1209/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-1209/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr = 1e-4\n",
    "# bs=32\n",
    "# wd=0.01\n",
    "# epochs= 3\n",
    "# o_dir = './default_phobert_finetuned_1'\n",
    "# tmp = finetune(lr,bs,wd,epochs,ddict=main_dataset_tokenized,o_dir = o_dir)\n",
    "\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1\tAccuracy\n",
    "# 1\tNo log\t0.288897\t0.907470\t0.908950\n",
    "# 2\t0.298100\t0.298636\t0.897811\t0.899627\n",
    "# 3\t0.180200\t0.306182\t0.909048\t0.910193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ea961a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12869\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2146' max='6435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2146/6435 04:46 < 09:33, 7.47 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 73/537 00:06 < 00:39, 11.73 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 11.77 GiB total capacity; 10.16 GiB already allocated; 239.88 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      5\u001b[0m o_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./default_phobert_finetuned_1_hiddenstate\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mddict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_dataset_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43mo_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mo_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m(lr, bs, wd, epochs, ddict, tokenizer, o_dir, logging, model_init)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# instantiate trainer\u001b[39;00m\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     57\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[1;32m     58\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:1500\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1497\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1499\u001b[0m )\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:1834\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1834\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1838\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m             metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   2047\u001b[0m                 eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m   2048\u001b[0m                 ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2049\u001b[0m                 metric_key_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2050\u001b[0m             )\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:2774\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2771\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2773\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2774\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2775\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2777\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2784\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2785\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2786\u001b[0m     speed_metrics(\n\u001b[1;32m   2787\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     )\n\u001b[1;32m   2792\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:2979\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2978\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[0;32m-> 2979\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_prediction_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;66;03m# Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:115\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:74\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     71\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     77\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 11.77 GiB total capacity; 10.16 GiB already allocated; 239.88 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=6\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "o_dir = './default_phobert_finetuned_1_hiddenstate'\n",
    "tmp = finetune(lr,bs,wd,epochs,ddict=main_dataset_tokenized,o_dir = o_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11fc7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1209  checkpoint-403  checkpoint-537  checkpoint-806\r\n"
     ]
    }
   ],
   "source": [
    "!ls default_phobert_finetuned_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7655e9",
   "metadata": {},
   "source": [
    "# Prediction interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57401c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model_init()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.from_pretrained('./default_phobert_finetuned_1/checkpoint-806')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d78ff2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2str={0:\"Positive\",1:'Negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f951b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def label_int2str(row):\n",
    "    return idx2str[row]\n",
    "\n",
    "def forward_pass_with_label(batch,model=None):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy(),\n",
    "           'predicted_probability': torch.nn.functional.softmax(output.logits.cpu(),dim=1).numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac5b647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "main_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d9bb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ef1fec038413fb0e6afd0c1d31beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/805 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b3e8a8cb64f6cb896676357428607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute loss values\n",
    "main_dataset_tokenized[\"train\"] = main_dataset_tokenized[\"train\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)\n",
    "main_dataset_tokenized[\"validation\"] = main_dataset_tokenized[\"validation\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98453d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", 'predicted_probability',\"loss\"]\n",
    "df_val = main_dataset_tokenized[\"validation\"][:][cols]\n",
    "df_val[\"label_str\"] = df_val[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_val[\"predicted_label_str\"] = (df_val[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4047fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = main_dataset_tokenized[\"train\"][:][cols]\n",
    "df_trn[\"label_str\"] = df_trn[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_trn[\"predicted_label_str\"] = (df_trn[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31a12ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12869, 7), (3218, 7))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.shape,df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a301ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Thông_tin, chi_tiết, trên, web, là, sản_xuất,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0779537, 0.92204636]</td>\n",
       "      <td>0.081160</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Đồng_hồ, dây_đeo, ọp_ẹp, ., Tất_nilon, rẻ_tiề...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04499709, 0.9550029]</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Shop, de, thương, nhiet, tình, sẽ, ủng, hô, l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99736834, 0.0026315863]</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nhìn, k, chắc_chắn, lắm]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5285818, 0.47141826]</td>\n",
       "      <td>0.637558</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Chất_lượng, sản_phẩm, rất, kém, ., Rách, lỗi,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.022633858, 0.9773662]</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [Thông_tin, chi_tiết, trên, web, là, sản_xuất,...      1                1   \n",
       "1  [Đồng_hồ, dây_đeo, ọp_ẹp, ., Tất_nilon, rẻ_tiề...      1                1   \n",
       "2  [Shop, de, thương, nhiet, tình, sẽ, ủng, hô, l...      0                0   \n",
       "3                          [Nhìn, k, chắc_chắn, lắm]      0                0   \n",
       "4  [Chất_lượng, sản_phẩm, rất, kém, ., Rách, lỗi,...      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0     [0.0779537, 0.92204636]  0.081160  Negative            Negative  \n",
       "1     [0.04499709, 0.9550029]  0.046041  Negative            Negative  \n",
       "2  [0.99736834, 0.0026315863]  0.002635  Positive            Positive  \n",
       "3     [0.5285818, 0.47141826]  0.637558  Positive            Positive  \n",
       "4    [0.022633858, 0.9773662]  0.022894  Negative            Negative  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc7b897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Cầm, khá, chắc_tay, ., Trục, uốn, to, đúng, q...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.997373, 0.0026270347]</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3G, còn, tùy, lúc_nào, và, khi, nào, và, ở, đ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99603903, 0.0039609983]</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Đóng_gói, sản_phẩm, rất, chắc_chắn, ., Thời_g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99715614, 0.0028438682]</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[mình, xài, em, này, bị, đứng, máy, hoài, à]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.13099755, 0.8690025]</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Chết, $]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.09323443, 0.9067656]</td>\n",
       "      <td>0.097871</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [Cầm, khá, chắc_tay, ., Trục, uốn, to, đúng, q...      0                0   \n",
       "1  [3G, còn, tùy, lúc_nào, và, khi, nào, và, ở, đ...      0                0   \n",
       "2  [Đóng_gói, sản_phẩm, rất, chắc_chắn, ., Thời_g...      0                0   \n",
       "3       [mình, xài, em, này, bị, đứng, máy, hoài, à]      1                1   \n",
       "4                                          [Chết, $]      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0    [0.997373, 0.0026270347]  0.002630  Positive            Positive  \n",
       "1  [0.99603903, 0.0039609983]  0.003969  Positive            Positive  \n",
       "2  [0.99715614, 0.0028438682]  0.002848  Positive            Positive  \n",
       "3     [0.13099755, 0.8690025]  0.140409  Negative            Negative  \n",
       "4     [0.09323443, 0.9067656]  0.097871  Negative            Negative  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a29fd",
   "metadata": {},
   "source": [
    "## Classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8f10912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.96      0.96      0.96      7466\n",
      "    Negative       0.94      0.95      0.94      5403\n",
      "\n",
      "    accuracy                           0.95     12869\n",
      "   macro avg       0.95      0.95      0.95     12869\n",
      "weighted avg       0.95      0.95      0.95     12869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_trn.label.values,df_trn.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0670fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.91      0.92      0.91      1814\n",
      "    Negative       0.89      0.88      0.88      1404\n",
      "\n",
      "    accuracy                           0.90      3218\n",
      "   macro avg       0.90      0.90      0.90      3218\n",
      "weighted avg       0.90      0.90      0.90      3218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_val.label.values,df_val.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d826ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb4ac47fdf0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,10))\n",
    "ConfusionMatrixDisplay.from_predictions(df_val.label.values,df_val.predicted_label.values,\n",
    "                                        display_labels=['Positive','Negative'],xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4098",
   "metadata": {},
   "source": [
    "## Most confident prediction: right vs wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c842659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prob_match(row):\n",
    "    l = row['label']\n",
    "    prob = row['predicted_probability']\n",
    "    return prob[l]\n",
    "def _get_prob_mismatch(row):\n",
    "    prob = row['predicted_probability']\n",
    "    return np.max(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d1e4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['correct_confidence'] = df_val.loc[df_val.label==df_val.predicted_label].apply(_get_prob_match,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e765bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['incorrect_confidence'] = df_val.loc[df_val.label!=df_val.predicted_label].apply(_get_prob_mismatch,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f68965e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correct_pred(df,cols=['text','label_str','correct_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('correct_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(tmp[1:])\n",
    "        print('-'*100)\n",
    "def analyze_incorrect_pred(df,cols=['text','label_str','predicted_label_str','incorrect_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('incorrect_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(f'True label: {tmp[1]}, but predict {tmp[2]}, with confidence {tmp[3]}')\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45aa524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mua | ao | mua | bo_shop | giao | ao | mua | doi | LAM | sao | DANH_Giá | the | nao | day\n",
      "['Negative' 0.5087628960609436]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bông | mềm | hơn | nhưng | cũng | mỏng | hơn | đợt | trước | dùng\n",
      "['Positive' 0.5093692541122437]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ghi | date | T2 | / | 2019 | nên | mới | đặt_hàng | đến | 5 | gói | nhận | hàng | thì | T1 | / | 2019 | thực_phẩm | cho | bé | nên | bán | đúng_hạn | sử_dụng | giới_thiệu | để | khách | tin_tưởng | & | quyết_định | mua | hàng\n",
      "['Negative' 0.5200026035308838]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pin | tụt | nhanh | nhưng | không | bị | sập | nguồn\n",
      "['Positive' 0.5242894887924194]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mở_ra | có | mỗi | vỏ | ko | thấy | kính | cường_lực | ở | trong\n",
      "['Negative' 0.5256175994873047]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nhìn | k | chắc_chắn | lắm\n",
      "['Positive' 0.5285817980766296]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vừa | mới | mua | sáng | nay | thì | 333.000 | VND | giờ | vào | nhìn | lại | thấy | 189.000 | VND | ! | ! | ! | !\n",
      "['Negative' 0.5287574529647827]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | . | Tuy_nhiên | giao | thiếu | số_lượng | . | Mình | đặt | 2 | thanh_toán | 2 | nhận | được | chỉ | có | 1 | cái\n",
      "['Positive' 0.5362181067466736]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hàng | dùng | tạm | được | tuy_nhiên | hơi | lỏng_lẻo | . | K | chắc_chắn | lắm | .\n",
      "['Positive' 0.538158655166626]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Giao | hàng | lâu | ơi | là | lâu | chắc | do | trả | tiền | trước | : | (\n",
      "['Positive' 0.5443349480628967]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | giao | hàng | kh | giống | màu\n",
      "['Negative' 0.5501789450645447]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dao | hàng | lâu | đặt | hẳn | 2 | lần | mới | dao | gần | tháng | mới | có | hàng | ....\n",
      "['Negative' 0.5585803389549255]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sản_phẩm | ok | tốc_độ | không | dc | như | mong_đợi\n",
      "['Positive' 0.5587018132209778]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bột | mịn | hửi | thì | có | mùi | thơm | nhưng | khi | dùng | thì | không | đậm | mùi | đậu_nành | bao_bì | in | mờ_nhạt\n",
      "['Negative' 0.5603232979774475]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng | góii | đuọc\n",
      "['Negative' 0.5685207843780518]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vải | khá | mỏng | . | Tiền | nào | của | nấy | . | Nói_chung | hơn | giày_bata | một | tý | .\n",
      "['Negative' 0.5702218413352966]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Trước_mắt | thì | thấy | oke | rồi | đó | cục | này | chọi | trâu | cũng | chết | 😂_😂_😂\n",
      "['Positive' 0.5918132066726685]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vượt | ngoài | mong_đợi\n",
      "['Positive' 0.5925284624099731]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mình | mua | 4 | chai | trong | đó | có | 2 | chai | lưng | KHG | đủ | dung_tích\n",
      "['Negative' 0.595950722694397]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Minh_dung | sp | nay | gan | 10 | nam | roi | do | sp | rat | tot | ..\n",
      "['Positive' 0.5961607694625854]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_correct_pred(df_val,ascending=True,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6c3381f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". | Hàng | rất | đẹp | <3 | !\n",
      "True label: Negative, but predict Positive, with confidence 0.9974151849746704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tam | duoc | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn\n",
      "True label: Negative, but predict Positive, with confidence 0.997407853603363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | sản_phẩm | tuyệt_vời | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | Shop | phục_vụ | rất | tốt | Thời_gian | giao | hàng | chậm | Rất | đáng | tiền | Thời_gian | giao | hàng | chậm\n",
      "True label: Negative, but predict Positive, with confidence 0.997378945350647\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | sản_phẩm | tuyệt_vời | nhưng | toàn | chữ | Trung_Quốc | .\n",
      "True label: Negative, but predict Positive, with confidence 0.997368574142456\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hàng | y | hình | đẹp | sang_trọng\n",
      "True label: Negative, but predict Positive, with confidence 0.9973480701446533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "chat | lượng | rất | tốt | ok\n",
      "True label: Negative, but predict Positive, with confidence 0.9973419308662415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tốt | dong | goi | dep | nhung | the | nho | bi | hong | không | dùng | duoc | ngoài | Ra | OK | 👌\n",
      "True label: Negative, but predict Positive, with confidence 0.9972805976867676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "😡 | 😡\n",
      "True label: Negative, but predict Positive, with confidence 0.9972085356712341\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | phục_vụ | rất | tốt | Thời_gian | giao | hàng | rất | nhanh | tiền | nào | thì | của | đó\n",
      "True label: Negative, but predict Positive, with confidence 0.9970918893814087\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Take | wrong | color\n",
      "True label: Negative, but predict Positive, with confidence 0.9969770908355713\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ốp | hơi | cũ_shop | nha | .\n",
      "True label: Negative, but predict Positive, with confidence 0.9968775510787964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Màu | quá | ghê | huhu\n",
      "True label: Negative, but predict Positive, with confidence 0.9968618154525757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | rất | nhanh | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | . | Trừ | thẳng | 3_* | vì | đội_giá | lên | quá | cao | so | với | thực_tế\n",
      "True label: Negative, but predict Positive, with confidence 0.9968488812446594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | . | Hàng | y_ảnh | 100_% | mẫu_mã | đẹp | cầm | chắc | tay | nhỏ | gọn | mk | thấy | khá | tiện | . | ( | sửa | bài | khi | sd | : | sạc | chậm | nhah | tụt | pin | k | hài | lòg | )\n",
      "True label: Negative, but predict Positive, with confidence 0.9968122839927673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | rất | nhanh\n",
      "True label: Negative, but predict Positive, with confidence 0.9967789053916931\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nhưlon | . | 😌\n",
      "True label: Negative, but predict Positive, with confidence 0.9966498017311096\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cũng | đẹp | nhưng | may | lời\n",
      "True label: Negative, but predict Positive, with confidence 0.9964595437049866\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | hàng | cẩn_thận | Sản_phẩm | ít | bọt | rửa | xong | thấy | da_khô | căng | . | Nên | dùng | thêm | với | xịt | khoáng | để | dưỡng | ẩm\n",
      "True label: Negative, but predict Positive, with confidence 0.9962769150733948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lừa | nhau | à | shop\n",
      "True label: Negative, but predict Positive, with confidence 0.9957062602043152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | sản_phẩm | rất | kem | moi | mua | co | may | ngay | Ma_di_khong | dung | gio | moi | nguoi | dung | co | mua | hang | cua_shop | nua\n",
      "True label: Negative, but predict Positive, with confidence 0.9955772161483765\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=False,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "737caf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khẩu_vị | của | mình | thì | hơi | nhạt | và | thiếu | cay | :)\n",
      "True label: Positive, but predict Negative, with confidence 0.5006212592124939\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | đẹp | . | Nhưng | pin | ko | bền | . | Nạp | sạc | dự_phòng | cả | đêm | mới | đầy | mà | Sạc | chỉ | dc | 1 | lần | là | hết | . | - | _ | -\n",
      "True label: Positive, but predict Negative, with confidence 0.5072213411331177\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | của | sản_phẩm | phù_hợp | với | giá | tiền | . | Có_thể | hãng | cần | nghiên_cứu | sao | cho | cục | sạc | ít | nóng | trong | quá_trình | sạc | vì | hiện_nay | khi | sạc | cục | sạc | rất | nóng | nên | tâm_lý | người | sử_dụng | sợ | nó | cháy | nổ | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5155402421951294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "100 | k | thì | vậy | là | tốt | rồi | ... | mang | đi | chơi | cũng | đẹp | phết | ... | đừng | hoạt_động | mạnh | thì | chắc | không | sao | ... | dùng | được | 2 | tuần | rồi | , | giặc | luôn | rồi | .... | ỔN | ! | ! | Có | cái | là | đến | khi | shiper | đến | cổng | mới | điện_thoại | chứ | chả | báo | trước | gì | cả | ? | ?\n",
      "True label: Positive, but predict Negative, with confidence 0.5188581943511963\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SHOP | ĐĂNG | ẢNH | MỘT_ĐẰNG | SẢN_PHẨM | MỘT | LẺO | ._BÁN | CÁP | THÌ | ÚP | HÌNH | CÁI | CÁP | THÔI | CHỨ | UP | CẢ | CỤC | SẠC | GÂY | NHẦM_LẪN | CHO | KHÁCH_HÀNG | LÀ | COMBO_CÁP | + | DÂY | SẠC | . | TÔI | MUA | LIỀN | 2 | CÁI | LÚC | NHẬN | HÀNG | MỚI | NGỚ | NGƯỜI | RA | BỊ | MUA | ĐẮT | . | CHẤT_LƯỢNG | SẢN_PHẨM | CHƯA | RÕ | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5189838409423828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | chậm | . | doi | nhan | hang | sốt | ca | ruột\n",
      "True label: Negative, but predict Positive, with confidence 0.5209054946899414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Có | ai | dùng | 6 | s | lên | 9.2.1 | lại | nhanh | hết | pin | như | tôi | không | . | Nhanh | hết | pin | quá\n",
      "True label: Negative, but predict Positive, with confidence 0.523202657699585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mình | không | dám | dùng | vì | thấy | mã_vạch | 69 | : | ' | ( | . | Tại | bản_thân | chưa | tìm_hiểu | kĩ | nên | cũng | mua | cho | đủ | giá | free | ship\n",
      "True label: Positive, but predict Negative, with confidence 0.5317555069923401\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Được | co | Thể | mua | lâu | dai\n",
      "True label: Positive, but predict Negative, with confidence 0.5340108871459961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đã | chát | thống_nhất | loại | hàng | nhưng | shop | vẫn | gửi | hàng | ngẫu_nhiên | . | Ko | sử_dụng | đc | chán\n",
      "True label: Negative, but predict Positive, with confidence 0.5393141508102417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | rất | nhanh | Màu_sắc | không | đúng | dù | mình | đã | dặn | trước\n",
      "True label: Positive, but predict Negative, with confidence 0.5393228530883789\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Quạt | mới | mua | nhưng | không | quạt | được\n",
      "True label: Negative, but predict Positive, with confidence 0.5425977110862732\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hơi | rộng | so | vs | size | bình_thường | . | Hàng | i | hình\n",
      "True label: Positive, but predict Negative, with confidence 0.5448225736618042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đặt | có | 8 | mon | à | giao | thiếu | 2 | món | rồi | làm_ăn | thật | quá_đáng | mà\n",
      "True label: Negative, but predict Positive, with confidence 0.5459489822387695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | oi | mua | nhiều | hơn | sao | lại | tiền | ship | đắt | hơn\n",
      "True label: Positive, but predict Negative, with confidence 0.5503779649734497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nhạt_nhẽo | chỉ | có | mỗi | bánh_phở | dai | lạt | và | nước | gia_vị | trong_veo | . | Ăn | hao_hao | hủ_tiếu | luộc | với | nước | Knor | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5527167916297913\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hàng | nhái | . | From | giầy | kg | đẹp | . | Phù_hợp | với | giá | tiền | đã | giảm | . | Chứ | kg | như | hàng | quãng | cáo | . | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn\n",
      "True label: Positive, but predict Negative, with confidence 0.5576053261756897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Như | shit | ngta | vậy | á | .._Hg | làm | được | trò | chống | gì | hit\n",
      "True label: Negative, but predict Positive, with confidence 0.5576322674751282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "khó | uống | mình | cứ | ngỡ | là | đang | uống | và | đắp | bột | nghệ | mà | mình | mua | về | để | nấu_ăn | 😭\n",
      "True label: Negative, but predict Positive, with confidence 0.5652170181274414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Em | đã | mua | giày | mà | em | không | thấy | dây | giày | đâu | hết | shop | ạ\n",
      "True label: Negative, but predict Positive, with confidence 0.5652967691421509\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=True,top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d6354",
   "metadata": {},
   "source": [
    "# Extract hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f60ed705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHO_PATH = Path('/home/quan1080/kwon/PhoBERT_base_transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f64f7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RobertaForHiddenState(RobertaPreTrainedModel):\n",
    "#     # make sure standard XLM-R are used\n",
    "#     config_class = RobertaConfig\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.num_labels = config.num_labels # TODO: change it to ner+chunk output\n",
    "#         # Load model body\n",
    "#         # add_polling_layer to False\n",
    "#         #    to ensure all hidden states are returned \n",
    "#         #    and not only the one associated with the [CLS] token.\n",
    "#         self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "#         # Set up token classification head\n",
    "#         self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "#         # Load and initialize weights from RobertaPretrainedModel\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "#                 labels=None, **kwargs):\n",
    "#         # Use model body to get encoder representations\n",
    "#         # the only ones we need for now are input_ids and attention_mask\n",
    "#         outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "#                                token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "#         hidden_states = outputs['hidden_states'] # tuples with len 13 (number of layer/block)\n",
    "#         # each with shape: (bs,seq_len,hidden_size_len), e.g. for phobert: (bs,256, 768)\n",
    "#         # Note: hidden_size_len = embedding_size\n",
    "#         last_hidden_state = hidden_states[-1][:,0] # (bs,768)\n",
    "\n",
    "#         logits = self.dropout(last_hidden_state)\n",
    "#         logits = self.classifier(logits) # (bs,num_labels)\n",
    "        \n",
    "    \n",
    "# #         # Calculate losses\n",
    "#         loss = None\n",
    "#         if labels is not None: # labels size: (bs,seq_len) # TODO: change loss for dual prediction\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#         # Return model output object\n",
    "#         return SequenceClassifierOutput(loss=loss, logits=logits,\n",
    "#                                      hidden_states=outputs.hidden_states,\n",
    "#                                      attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e700532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_model_init():\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     config = AutoConfig.from_pretrained(\n",
    "#         PHO_PATH/'config.json',\n",
    "#         output_hidden_states=True,\n",
    "#         num_labels=2,\n",
    "#     )\n",
    "\n",
    "#     model = RobertaForHiddenState.from_pretrained(PHO_PATH/'model.bin',config=config).to(device)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc878c5",
   "metadata": {},
   "source": [
    "## Redefine dataset dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ff6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b10e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset.from_dict(\n",
    "#                         {'text': train_text,\n",
    "#                         'label':train_label,\n",
    "#                         }\n",
    "#                     )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )\n",
    "\n",
    "# train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True)\n",
    "# test_dataset_tokenized= test_dataset.map(tokenize_function,batched=True)\n",
    "\n",
    "# train_dataset_tokenized = train_dataset_tokenized.shuffle()\n",
    "train_sample = train_dataset_tokenized.shuffle().select(range(int(train_dataset_tokenized.num_rows*0.1)))\n",
    "sample_dataset_tokenized = DatasetDict()\n",
    "sample_dataset_tokenized['train'] = train_sample.select(range(int(train_sample.num_rows*0.8)))\n",
    "sample_dataset_tokenized['validation'] = train_sample.select(range(int(train_sample.num_rows*0.8),train_sample.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bb5c72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1286, 322)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset_tokenized['train'].num_rows,sample_dataset_tokenized['validation'].num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d3b30",
   "metadata": {},
   "source": [
    "## Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f99d4427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1286\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [645/645 01:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.886299</td>\n",
       "      <td>0.894410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.395047</td>\n",
       "      <td>0.892988</td>\n",
       "      <td>0.900621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.377929</td>\n",
       "      <td>0.908195</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 322\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1_hiddenstate/checkpoint-215\n",
      "Configuration saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 322\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1_hiddenstate/checkpoint-430\n",
      "Configuration saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 322\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1_hiddenstate/checkpoint-645\n",
      "Configuration saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=6\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "o_dir = './default_phobert_finetuned_1_hiddenstate'\n",
    "tmp = finetune(lr,bs,wd,epochs,ddict=sample_dataset_tokenized,o_dir = o_dir,model_init=partial(base_model_init,get_hidden=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f550445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tmp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fad914e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'lm_head.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'classifier.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ./default_phobert_finetuned_1/checkpoint-806 were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'classifier.out_proj.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'classifier.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'classifier.out_proj.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'classifier.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./default_phobert_finetuned_1/checkpoint-806 and are newly initialized: ['encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'classifier.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\",config=config)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.from_pretrained('./default_phobert_finetuned_1_hiddenstate/checkpoint-645')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "933e411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).hidden_states[-1]\n",
    "    # Return vector for [CLS] token\n",
    "    \n",
    "    # \"hidden_state\": the new transformer datasetdict will have\n",
    "    # an extra column called 'hidden_state'\n",
    "    # also, must return a numpy array/python object for datasetdict\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b5bbbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a8f83bebf47a2b316dcd59758bb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "sample_dataset_tokenized['validation'] = sample_dataset_tokenized['validation'].map(extract_hidden_states, batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "127007a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([322, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset_tokenized['validation']['hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcda0e2",
   "metadata": {},
   "source": [
    "## Find neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1affa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_dataset_tokenized['validation']['hidden_state'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b4a8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fd91b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "neigh.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a96bbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset_tokenized['validation']['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79f630c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str(inp):\n",
    "    return ' '.join(inp)\n",
    "def show_neighbors(neigh,idx,data=data,metadata=sample_dataset_tokenized['validation'],n_neighbors=5):\n",
    "    print(f\"Sentence: {concat_str(metadata['text'][idx])}\\nLabel: {metadata['label'][idx]}\")\n",
    "    distances,nbors = neigh.kneighbors([data[idx]])\n",
    "\n",
    "    print('\\nNeighbors: ')\n",
    "    for d,n_idx in zip(distances[0],nbors[0]):\n",
    "        print(f\"\\t{concat_str(metadata['text'][n_idx])}, d={d:.3f}, label={metadata['label'][n_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "166f9147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Rất đáng tiền\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tRất đáng tiền, d=0.000, label=0\n",
      "\tGiày rất đẹp êm ., d=0.724, label=0\n",
      "\tShop làm_việc OK lắm, d=0.870, label=0\n",
      "\tĐồng_hồ đẹp, d=0.885, label=0\n",
      "\tMẫu_mã đẹp hy_vọng chất_lượng tốt ., d=0.902, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c79f8",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- concat last 4 hidden states for better sentence presentation\n",
    "- run on 3090 for better results?\n",
    "- raw output from phobert and check representation => comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "10px",
    "top": "150px",
    "width": "315.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
