{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf68ba4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task:-Shopee-binary-sentiment-classification\" data-toc-modified-id=\"Task:-Shopee-binary-sentiment-classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task: Shopee binary sentiment classification</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-crash-files-(do-once)\" data-toc-modified-id=\"Convert-crash-files-(do-once)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Convert crash files (do once)</a></span></li><li><span><a href=\"#Load-csv\" data-toc-modified-id=\"Load-csv-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load csv</a></span></li></ul></li><li><span><a href=\"#Vietnamese-tokenization\" data-toc-modified-id=\"Vietnamese-tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vietnamese tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Pretokenization-(Vietnamese-word-tokenization)\" data-toc-modified-id=\"Step-1:-Pretokenization-(Vietnamese-word-tokenization)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 1: Pretokenization (Vietnamese word tokenization)</a></span></li><li><span><a href=\"#Step-2:-Model's-tokenization\" data-toc-modified-id=\"Step-2:-Model's-tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 2: Model's tokenization</a></span></li></ul></li><li><span><a href=\"#Use-HuggingFace-Dataset-to-store-and-tokenize-corpus\" data-toc-modified-id=\"Use-HuggingFace-Dataset-to-store-and-tokenize-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Use HuggingFace Dataset to store and tokenize corpus</a></span></li><li><span><a href=\"#Load-pretrained-model\" data-toc-modified-id=\"Load-pretrained-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Load pretrained model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10c8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6d3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data_crash')\n",
    "RAW_PATH = Path('raw_crash')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5c682",
   "metadata": {},
   "source": [
    "# Task: Shopee binary sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f326",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83962f99",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convert crash files (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696ba470",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66966it [00:00, 1915214.36it/s]\n",
      "16087it [00:00, 134272.15it/s]\n",
      "34750it [00:00, 2274852.34it/s]\n",
      "10981it [00:00, 132888.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "### Cleaning training file\n",
    "\n",
    "train = open(RAW_PATH/\"train.crash\").readlines()\n",
    "id_locations = []\n",
    "label_locations = []\n",
    "for idx, line in tqdm(enumerate(train)):\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"train_\"):\n",
    "        id_locations.append(idx)\n",
    "    elif line == \"0\" or line == \"1\":\n",
    "        label_locations.append(idx)\n",
    "data = []\n",
    "\n",
    "for id_loc, l_loc in tqdm(zip(id_locations, label_locations)):\n",
    "    line_id = train[id_loc].strip()\n",
    "    label = train[l_loc].strip()\n",
    "    text = re.sub('\\s+', ' ', ' '.join(train[id_loc + 1: l_loc])).strip()[1:-1].strip()\n",
    "    data.append(f\"{line_id}\\t{text}\\t{label}\")\n",
    "\n",
    "with open(DATA_PATH/\"train.csv\", \"wt\") as f:\n",
    "    f.write(\"id\\ttext\\tlabel\\n\")\n",
    "    f.write(\"\\n\".join(data))\n",
    "\n",
    "### Cleaning test file\n",
    "\n",
    "test = open(RAW_PATH/\"test.crash\").readlines()\n",
    "id_locations = []\n",
    "for idx, line in tqdm(enumerate(test)):\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"test_\"):\n",
    "        id_locations.append(idx)\n",
    "data = []\n",
    "\n",
    "for i, id_loc in tqdm(enumerate(id_locations)):\n",
    "    if i >= len(id_locations) - 1:\n",
    "        end = len(test)\n",
    "    else:\n",
    "        end = id_locations[i + 1]\n",
    "    line_id = test[id_loc].strip()\n",
    "    text = re.sub('\\s+', ' ', ' '.join(test[id_loc + 1:end])).strip()[1:-1].strip()\n",
    "    data.append(f\"{line_id}\\t{text}\")\n",
    "\n",
    "with open(DATA_PATH/\"test.csv\", \"wt\") as f:\n",
    "    f.write(\"id\\ttext\\n\")\n",
    "    f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe363",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f367e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {str(DATA_PATH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv',sep='\\t').fillna(\"###\")\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv',sep='\\t').fillna(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cb72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13322</th>\n",
       "      <td>Mẫu mã thì ok nhưng chất lượng ko ổn lắm chỉ g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9448</th>\n",
       "      <td>Giao hàng rất lâu không giống trog ảnh lắm Thờ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>Giấy bên trong có kích thước không vừa với kíc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời. Áo mặc rất mát ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>Mua của shop lần 2 rồi và lần nào cũng rất hài...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>Hàng bị bẩn đen làm ăn thế mất uy tín</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14492</th>\n",
       "      <td>Chất lượng sản phẩm rất kém. Rất không đáng ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>Chất lượng tốt đóng gói đẹp chắc chắn tuy nhiê...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14939</th>\n",
       "      <td>Giày bên phải dây dính màu còn giày trái góc t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>. cũnq thơm mùi hơi nồnq...so vs giá thì oke</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Hàng nguyên seal ngon bổ rẻ chỉ có điều vận ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>Đẹp hơn mong đợi. Có 6☆ thì cũng đánh giá luôn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>Đa phần những sản phẩm mình mua trước đây từ T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>K thấy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>Tạm được so với giá tiền. Mua đc các bạn ạ.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13296</th>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời Đóng gói sản phẩ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11143</th>\n",
       "      <td>Hàng ok đẹp lắm bạn nhak 😍 Chất lượng sản phẩm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8932</th>\n",
       "      <td>ôi thật bất ngờ... có con nào mobi sờ ta giá r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8265</th>\n",
       "      <td>Giày đẹp lắm nha shop còn được bỏ vào h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>lưu hương kg được lâu lắm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "13322  Mẫu mã thì ok nhưng chất lượng ko ổn lắm chỉ g...      1\n",
       "9448   Giao hàng rất lâu không giống trog ảnh lắm Thờ...      1\n",
       "9685   Giấy bên trong có kích thước không vừa với kíc...      1\n",
       "6521   Chất lượng sản phẩm tuyệt vời. Áo mặc rất mát ...      0\n",
       "4303   Mua của shop lần 2 rồi và lần nào cũng rất hài...      0\n",
       "7700               Hàng bị bẩn đen làm ăn thế mất uy tín      1\n",
       "14492  Chất lượng sản phẩm rất kém. Rất không đáng ti...      1\n",
       "5702   Chất lượng tốt đóng gói đẹp chắc chắn tuy nhiê...      0\n",
       "14939  Giày bên phải dây dính màu còn giày trái góc t...      1\n",
       "778    . cũnq thơm mùi hơi nồnq...so vs giá thì oke      0\n",
       "242    Hàng nguyên seal ngon bổ rẻ chỉ có điều vận ch...      0\n",
       "5882   Đẹp hơn mong đợi. Có 6☆ thì cũng đánh giá luôn...      0\n",
       "14973  Đa phần những sản phẩm mình mua trước đây từ T...      1\n",
       "8977                                              K thấy      1\n",
       "2059         Tạm được so với giá tiền. Mua đc các bạn ạ.      0\n",
       "13296  Chất lượng sản phẩm tuyệt vời Đóng gói sản phẩ...      0\n",
       "11143  Hàng ok đẹp lắm bạn nhak 😍 Chất lượng sản phẩm...      0\n",
       "8932   ôi thật bất ngờ... có con nào mobi sờ ta giá r...      0\n",
       "8265   Giày đẹp lắm nha shop còn được bỏ vào h...      0\n",
       "7541                           lưu hương kg được lâu lắm      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','label']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb4a826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chất lượng sản phẩm tuyệt vời và đang mình đặt 1 chiếc nữa' 0]\n",
      " ['Cảm thấy bị lừa đảo' 1]\n",
      " ['Sach rat dep' 0]\n",
      " ['Chất lượng sản phẩm tuyệt vời. Rất đáng tiền' 0]\n",
      " ['Rất không đáng tiền. Đặt số 42 hết số kiếm đâu đôi 40 dán đại cai số 42 rồi giao cho khách hàng. K biết làm an kiểu j. Hàng thì k cho xem k cho kiểm tra. Bắt phải nhận mới đc mở. Ép người ta lấy hàng bán k đc. Đủ trò.'\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['text','label']].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377a4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chất lượng sản phẩm tuyệt vời. Dán cho con s8+ xong nhìn sướng cả mắt. Trc dùng dán dẻo toàn bị mất 1 tý rìa màn hình']\n",
      " ['Chất lượng sản phẩm tuyệt vời.Tuyet vời']\n",
      " ['Đóng gói sản phẩm rất rất chắc chắn. Giao hàng nhanh. Đúng màu. Còn có cả phiếu bảo hành ^^ Tuy chưa có điều kiện dùng thử nhưng thấy ấn tượng đầu rất tốt. Chúc shop bán đắt nha. 10 điểm cho chất lượng và tác phong bán hàng nek. Yêu shop <3 Rất đáng tiền']\n",
      " ['Đẹp lung linh']\n",
      " ['4sim 2 thẻ nhớ 8 sóng 12 mạng :):) đã test thử ukie màu kute phô mai qe']]\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text']].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ceee",
   "metadata": {},
   "source": [
    "# Vietnamese tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c606",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a346",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it “cleaner”**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be97150",
   "metadata": {},
   "source": [
    "## Step 1: Pretokenization (Vietnamese word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b9e4",
   "metadata": {},
   "source": [
    "https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378087",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from underthesea import word_tokenize\n",
    ">>> sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
    "\n",
    ">>> word_tokenize(sentence)\n",
    "['Chàng trai', '9X', 'Quảng Trị', 'khởi nghiệp', 'từ', 'nấm', 'sò']\n",
    "\n",
    ">>> word_tokenize(sentence, format=\"text\")\n",
    "'Chàng_trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43cefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_tokenize(sen,split_word=False):\n",
    "    # optional step: fix the whitespace between words\n",
    "    sen = \" \".join(sen.split())\n",
    "    sens = sent_tokenize(sen)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen+=word_tokenize(sen,format='text' if not split_word else None)\n",
    "    \n",
    "    if not split_word:\n",
    "        return ''.join(tokenized_sen)\n",
    "    return ['_'.join(words.split(' ')) for words in tokenized_sen]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f83210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63754082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '?', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời? Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f171898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '😌_😌', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c089cc",
   "metadata": {},
   "source": [
    "Apply on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32166f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [apply_word_tokenize(s[0],True) for s in train_df[['text']].values]\n",
    "train_label = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90044db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [apply_word_tokenize(s[0],True) for s in test_df[['text']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc34d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'tuyệt_vời',\n",
       "  '.',\n",
       "  'y',\n",
       "  'hình',\n",
       "  'chụp',\n",
       "  '.',\n",
       "  'đáng',\n",
       "  'tiền'],\n",
       " ['Hjhj_shop',\n",
       "  'giao',\n",
       "  'hàng',\n",
       "  'nhanh',\n",
       "  'quá',\n",
       "  '.',\n",
       "  'Đẹp',\n",
       "  'lắm',\n",
       "  'ạ',\n",
       "  'bé',\n",
       "  'nhà',\n",
       "  'm',\n",
       "  'rất',\n",
       "  'thích'],\n",
       " ['nhìn', 'đẹp', 'phết', 'nhỉ', '..'],\n",
       " ['Đóng_gói',\n",
       "  'rất',\n",
       "  'đẹp',\n",
       "  '.',\n",
       "  'Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'rất',\n",
       "  'tốt',\n",
       "  'Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'tuyệt_vời'],\n",
       " ['Săn', 'đc', 'với', 'giá', '11', 'k', '.', 'Toẹt', 'vời']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2f6f",
   "metadata": {},
   "source": [
    "## Step 2: Model's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed8d0",
   "metadata": {},
   "source": [
    "To use pretrained language model such as BERT, GPT, Roberta... We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n",
    "\n",
    "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their hub. In this notebook, we use PhoBERT-base (vinai/phobert-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f842db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37e1bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_explain(inp,split_word):\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()\n",
    "\n",
    "\n",
    "def two_step_tokenization_explain(inp,split_word=False):\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Pretokenization ---')\n",
    "    tok = apply_word_tokenize(inp,split_word)\n",
    "    print(tok)\n",
    "    print()\n",
    "    tokenizer_explain(tok,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 4, 7079, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời, phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7978be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "Chất_lượng sản_phẩm tuyệt_vời .Phấn mịn , đóng_gói sản_phẩm rất đẹp và chắc_chắn\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 2586, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.@@', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời.Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời. Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7f2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '😌_😌', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 3, 1751, 3, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '<unk>', '_@@', '<unk>', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời <unk> _<unk>. Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c4f",
   "metadata": {},
   "source": [
    "# Use HuggingFace Dataset to store and tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2481f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "87c4b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text[:4],\n",
    "                        'label':train_label[:4],\n",
    "                        }\n",
    "                    )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e1572e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # pad to model's allowed max length, which is max_sequence_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8b3fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3019a96ef5049b79a9a69fedf04a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10db51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "013debff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dung', 'dc', 'sp', 'tot', 'cam', 'on_shop', 'Đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời'], ['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Son', 'mịn', 'nhưng', 'khi', 'đánh', 'lên', 'không', 'như', 'màu', 'trên', 'ảnh'], ['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', 'nhưng', 'k', 'có', 'hộp', 'k', 'có', 'dây', 'giày', 'đen', 'k', 'có', 'tất'], [':', '(', '(', 'Mình', 'hơi', 'thất_vọng', '1', 'chút', 'vì', 'mình', 'đã', 'kỳ_vọng', 'cuốn', 'sách', 'khá', 'nhiều', 'hi_vọng', 'nó', 'sẽ', 'nói', 'về', 'việc', 'học_tập', 'của', 'cách', 'sinh_viên', 'trường', 'Harvard', 'ra_sao', 'những', 'nỗ_lực', 'của', 'họ', 'như', 'thế_nào', '4', 'h', 'sáng', '?', 'tại_sao', 'họ', 'lại', 'phải', 'thức', 'dậy', 'vào', 'thời_khắc', 'đấy', '?', 'sau', 'đó', 'là', 'cả', 'một', 'câu_chuyện', 'ra_sao', '.', 'Cái', 'mình', 'thực_sự', 'cần', 'ở', 'đây', 'là', 'câu_chuyện', 'ẩn', 'dấu', 'trong', 'đó', 'để', 'tự', 'bản_thân', 'mỗi', 'người', 'cảm_nhận', 'và', 'đi', 'sâu', 'vào', 'lòng', 'người', 'hơn', '.', 'Còn', 'cuốn', 'sách', 'này', 'chỉ', 'đơn_thuần', 'là', 'cuốn', 'sách', 'dạy', 'kĩ_năng', 'mà', 'hầu_như', 'sách', 'nào', 'cũng', 'đã', 'có', '.', 'BUồn', '...']]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f6c2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3556, 1236, 1894, 36150, 2225, 1204, 2947, 1672, 20811, 54922, 55662, 1685, 265, 59, 258, 6, 994, 6869, 265, 1819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 5, 16332, 5451, 51, 26, 480, 72, 17, 42, 412, 34, 284, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 51, 1947, 10, 2275, 1947, 10, 1747, 2466, 989, 1947, 10, 7328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 27, 20, 20, 3000, 1329, 2804, 99, 2013, 90, 68, 14, 2109, 1088, 713, 281, 36, 4876, 231, 38, 96, 28, 49, 1227, 7, 139, 649, 212, 9913, 57964, 3075, 21, 773, 7, 86, 42, 1279, 163, 1664, 298, 114, 2393, 86, 44, 41, 2908, 1764, 33, 9171, 1582, 114, 53, 37, 8, 94, 16, 876, 57964, 3075, 5, 2510, 68, 742, 115, 25, 97, 8, 876, 4592, 3309, 12, 37, 24, 385, 744, 205, 18, 2601, 6, 57, 808, 33, 605, 18, 48, 5, 631, 1088, 713, 23, 66, 5284, 8, 1088, 713, 940, 10685, 64, 2903, 713, 142, 32, 14, 10, 5, 924, 1878, 5460, 135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4fb0",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efa1f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea1571b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed89f689888c490c82b40e14068cfe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/518M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "10px",
    "top": "150px",
    "width": "315.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
