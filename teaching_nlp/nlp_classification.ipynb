{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf68ba4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task:-Shopee-binary-sentiment-classification\" data-toc-modified-id=\"Task:-Shopee-binary-sentiment-classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task: Shopee binary sentiment classification</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-crash-files-(do-once)\" data-toc-modified-id=\"Convert-crash-files-(do-once)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Convert crash files (do once)</a></span></li><li><span><a href=\"#Load-csv\" data-toc-modified-id=\"Load-csv-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load csv</a></span></li></ul></li><li><span><a href=\"#Vietnamese-tokenization\" data-toc-modified-id=\"Vietnamese-tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vietnamese tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Pretokenization-(Vietnamese-word-tokenization)\" data-toc-modified-id=\"Step-1:-Pretokenization-(Vietnamese-word-tokenization)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 1: Pretokenization (Vietnamese word tokenization)</a></span></li><li><span><a href=\"#Step-2:-Model's-tokenization\" data-toc-modified-id=\"Step-2:-Model's-tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 2: Model's tokenization</a></span></li></ul></li><li><span><a href=\"#Use-HuggingFace-Dataset-to-store-and-tokenize-corpus\" data-toc-modified-id=\"Use-HuggingFace-Dataset-to-store-and-tokenize-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Use HuggingFace Dataset to store and tokenize corpus</a></span></li><li><span><a href=\"#Define-dataset-dict-and-perform-train/val-split\" data-toc-modified-id=\"Define-dataset-dict-and-perform-train/val-split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define dataset dict and perform train/val split</a></span></li><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model definition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-using-default-PhoBert\" data-toc-modified-id=\"Classification-using-default-PhoBert-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Classification using default PhoBert</a></span></li><li><span><a href=\"#Define-helper-function-for-training\" data-toc-modified-id=\"Define-helper-function-for-training-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Define helper function for training</a></span></li><li><span><a href=\"#Start-training\" data-toc-modified-id=\"Start-training-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Start training</a></span></li></ul></li><li><span><a href=\"#Prediction-interpretation\" data-toc-modified-id=\"Prediction-interpretation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Prediction interpretation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-report-and-confusion-matrix\" data-toc-modified-id=\"Classification-report-and-confusion-matrix-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Classification report and confusion matrix</a></span></li><li><span><a href=\"#Most-confident-prediction:-right-vs-wrong\" data-toc-modified-id=\"Most-confident-prediction:-right-vs-wrong-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Most confident prediction: right vs wrong</a></span></li></ul></li><li><span><a href=\"#Extract-hidden-states\" data-toc-modified-id=\"Extract-hidden-states-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Extract hidden states</a></span><ul class=\"toc-item\"><li><span><a href=\"#Redefine-dataset-dict\" data-toc-modified-id=\"Redefine-dataset-dict-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Redefine dataset dict</a></span></li><li><span><a href=\"#Retrain-model\" data-toc-modified-id=\"Retrain-model-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Retrain model</a></span></li><li><span><a href=\"#Find-neighbors\" data-toc-modified-id=\"Find-neighbors-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Find neighbors</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10c8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71235da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c8a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4ec21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af5dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel # body only\n",
    "#inherit this to load pretrained weight\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6d3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data_crash')\n",
    "RAW_PATH = Path('raw_crash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01542e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5c682",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Task: Shopee binary sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f326",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83962f99",
   "metadata": {},
   "source": [
    "## Convert crash files (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696ba470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if not os.path.exists(DATA_PATH):\n",
    "#     os.mkdir(DATA_PATH)\n",
    "\n",
    "# ### Cleaning training file\n",
    "\n",
    "# train = open(RAW_PATH/\"train.crash\").readlines()\n",
    "# id_locations = []\n",
    "# label_locations = []\n",
    "# for idx, line in tqdm(enumerate(train)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"train_\"):\n",
    "#         id_locations.append(idx)\n",
    "#     elif line == \"0\" or line == \"1\":\n",
    "#         label_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for id_loc, l_loc in tqdm(zip(id_locations, label_locations)):\n",
    "#     line_id = train[id_loc].strip()\n",
    "#     label = train[l_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(train[id_loc + 1: l_loc])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\\t{label}\")\n",
    "\n",
    "# with open(DATA_PATH/\"train.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\tlabel\\n\")\n",
    "#     f.write(\"\\n\".join(data))\n",
    "\n",
    "# ### Cleaning test file\n",
    "\n",
    "# test = open(RAW_PATH/\"test.crash\").readlines()\n",
    "# id_locations = []\n",
    "# for idx, line in tqdm(enumerate(test)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"test_\"):\n",
    "#         id_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for i, id_loc in tqdm(enumerate(id_locations)):\n",
    "#     if i >= len(id_locations) - 1:\n",
    "#         end = len(test)\n",
    "#     else:\n",
    "#         end = id_locations[i + 1]\n",
    "#     line_id = test[id_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(test[id_loc + 1:end])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\")\n",
    "\n",
    "# with open(DATA_PATH/\"test.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\n\")\n",
    "#     f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe363",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f367e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {str(DATA_PATH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv',sep='\\t').fillna(\"###\")\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv',sep='\\t').fillna(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5cb72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>Ngon. ƒê∆∞·ª£c nhi·ªÅu ng th√≠ch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>Shop ph·ª•c v·ª• k√©m Shop ph·ª•c v·ª• k√©m Shop ph·ª•c v·ª•...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©mShop l·ª´a ƒë·∫£o ng kh√°...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11294</th>\n",
       "      <td>Nh·ªè nh∆∞ng c√≥ v√µ! ki·ªÉm so√°t d·∫ßu r·∫•t t·ªët</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14689</th>\n",
       "      <td>ch·∫•t li·ªáu v·∫£i m·ªèng qu√° khuy√™n anh em n√™n mua l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>Giao sai hang cho khach mat uy tin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16056</th>\n",
       "      <td>R·∫•t tuy·ªát b√† con ah. S·ª≠ d·ª•ng 3 l·∫ßn. Da m·∫∑t m·ªãn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>Ko nh·∫≠n ƒëc √°o do shop qu√™n h√†ng nh·ªØng ƒë√£ b√π ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15159</th>\n",
       "      <td>Sp n√≥i t·ª´ 6 ƒë·∫øn 12 th√°ng m√† mang ko v·ª´a. Sp ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>M√†u r·∫•t l√† ƒë·∫πp, ch·∫•t ph·∫•t c·ª±c k·ª≥ pigmented lu√¥...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6292</th>\n",
       "      <td>l√†m theo h∆∞·ªõng d·∫´n nh∆∞ng kh√¥ng ƒë√¥ng l√∫c b·ªè m√°y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7401</th>\n",
       "      <td>M√°y may r·∫•t ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>ƒê·ªì ƒë·ªÉu x·∫•u qu√° kh√¥ng ch·∫•p nh·∫≠n ki·ªÉu b√°n h√†ng n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>Ko c√≥ phi·∫øu b·∫£o h√†nh h·∫£ shop. Vs e th·∫•y ch·∫°y ƒë...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>Cam on shop nka e tkjk lam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>ƒë·ªìng h·ªì b·ªã h∆∞ giao h√†ng thi√™u 1 m√°y s·∫•y...ib n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. Shop p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>S√°ch d·ªãch m·ªôt c√°ch m√°y m√≥c t·ª´ 1 cu·ªën s√°ch n∆∞·ªõc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768</th>\n",
       "      <td>ƒê·ª£t n√†y b·ªã h·ª•t c√¢n ƒëi·ªán t·ª≠ c·∫ßm tay nh∆∞ng v·∫´n r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15633</th>\n",
       "      <td>ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. Shop p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1104                           Ngon. ƒê∆∞·ª£c nhi·ªÅu ng th√≠ch      0\n",
       "13556  Shop ph·ª•c v·ª• k√©m Shop ph·ª•c v·ª• k√©m Shop ph·ª•c v·ª•...      1\n",
       "8622   Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©mShop l·ª´a ƒë·∫£o ng kh√°...      1\n",
       "11294             Nh·ªè nh∆∞ng c√≥ v√µ! ki·ªÉm so√°t d·∫ßu r·∫•t t·ªët      0\n",
       "14689  ch·∫•t li·ªáu v·∫£i m·ªèng qu√° khuy√™n anh em n√™n mua l...      1\n",
       "11571                 Giao sai hang cho khach mat uy tin      1\n",
       "16056  R·∫•t tuy·ªát b√† con ah. S·ª≠ d·ª•ng 3 l·∫ßn. Da m·∫∑t m·ªãn...      0\n",
       "2829   Ko nh·∫≠n ƒëc √°o do shop qu√™n h√†ng nh·ªØng ƒë√£ b√π ti...      0\n",
       "15159  Sp n√≥i t·ª´ 6 ƒë·∫øn 12 th√°ng m√† mang ko v·ª´a. Sp ch...      1\n",
       "1304   M√†u r·∫•t l√† ƒë·∫πp, ch·∫•t ph·∫•t c·ª±c k·ª≥ pigmented lu√¥...      0\n",
       "6292   l√†m theo h∆∞·ªõng d·∫´n nh∆∞ng kh√¥ng ƒë√¥ng l√∫c b·ªè m√°y...      1\n",
       "7401                                      M√°y may r·∫•t ok      0\n",
       "5023   ƒê·ªì ƒë·ªÉu x·∫•u qu√° kh√¥ng ch·∫•p nh·∫≠n ki·ªÉu b√°n h√†ng n...      1\n",
       "11396  Ko c√≥ phi·∫øu b·∫£o h√†nh h·∫£ shop. Vs e th·∫•y ch·∫°y ƒë...      0\n",
       "4710                          Cam on shop nka e tkjk lam      0\n",
       "5117   ƒë·ªìng h·ªì b·ªã h∆∞ giao h√†ng thi√™u 1 m√°y s·∫•y...ib n...      1\n",
       "912    ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. Shop p...      0\n",
       "4509   S√°ch d·ªãch m·ªôt c√°ch m√°y m√≥c t·ª´ 1 cu·ªën s√°ch n∆∞·ªõc...      1\n",
       "14768  ƒê·ª£t n√†y b·ªã h·ª•t c√¢n ƒëi·ªán t·ª≠ c·∫ßm tay nh∆∞ng v·∫´n r...      0\n",
       "15633  ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. Shop p...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','label']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4a826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['San pham rat tot cam on shop Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi' 0]\n",
      " ['Shop ph·ª•c v·ª• r·∫•t k√©mƒê·∫∑t size 37, Shop g·ª≠i v·ªÅ 1 chi·∫øc size 36, 1 chi·∫øc size 37 K mu·ªën tr·∫£ h√†ng n√™n ƒë·∫∑t th√™m 1 ƒë√¥i ƒë·ªÉ Shop g·ª≠i b√π v√† Shop ch·ªãu ph√≠ ship nh∆∞ng l·∫°i g·ª≠i ƒë√∫ng 1 ƒë√¥i 2 chi·∫øc size 36, v√† k ch·ªãu phi n√™n ph·∫£i tr·∫£ l·∫°i h√†ng Sau ƒë√≥ c≈©ng k c√≥ √Ω ki·∫øn g√¨ v·ªÅ tr√°ch nhi·ªám c·ªßa Shop ƒë·ªëi v·ªõi ƒë√¥i c≈©'\n",
      "  1]\n",
      " ['T√¥i mua s·∫£n ph·∫©m c√°ch ƒë√¢y v√†i th√°ng do m·ªõi ƒë·∫ßu ko ƒë·ªÉ √Ω nh∆∞g sau ph√°t hi·ªán ra ƒë·ªám tai 1 b√™n c·ª±c m·ªÅm 1 b√™n th√¨ c·ª©ng gi·ªëng h·ªát l√≥t c·ªßa d√≤ng m20x. Ko th·ªÉ ch·∫•p nh·∫≠n ƒëc c√≥ c·ª≠a h√†ng ntnay tr√™n tiki.'\n",
      "  1]\n",
      " ['B√¨nh c≈©, r∆∞·ª£u c≈©, ch·ªâ c√≥ n√∫t chai l√† m·ªõi...ip ch·∫≥n th√¨ m·ªõi ho√†n to√†n m·ªõi. C√≤n ip l·∫ª th√¨ na n√° c√°i c≈©...'\n",
      "  1]\n",
      " ['R·∫•t ƒë√°ng ti·ªÅn..Shop ph·ª•c v·ª• r·∫•t t·ªët' 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['text','label']].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377a4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Th√≠ch l·∫Øm lu√¥n gi√†y ren kem l·∫•p l√°nh sang m√† v·∫´n tr·∫ª trung']\n",
      " ['Sao truy·ªán c·ªßa m√¨nh l·∫°i kh√¥ng c√≥ ƒëai obi v·∫≠y......']\n",
      " ['ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn nh·ªõ t·ªõi snack tr·∫ª em h·ªìi x∆∞a']\n",
      " ['gi√†y ƒë·∫πp l·∫Øm shop ∆°i']\n",
      " ['Cam ∆°n shop nh√¨u nghen']]\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text']].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ceee",
   "metadata": {},
   "source": [
    "# Vietnamese tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c606",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a346",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it ‚Äúcleaner‚Äù**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be97150",
   "metadata": {},
   "source": [
    "## Step 1: Pretokenization (Vietnamese word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b9e4",
   "metadata": {},
   "source": [
    "https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378087",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from underthesea import word_tokenize\n",
    ">>> sentence = 'Ch√†ng trai 9X Qu·∫£ng Tr·ªã kh·ªüi nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "\n",
    ">>> word_tokenize(sentence)\n",
    "['Ch√†ng trai', '9X', 'Qu·∫£ng Tr·ªã', 'kh·ªüi nghi·ªáp', 't·ª´', 'n·∫•m', 's√≤']\n",
    "\n",
    ">>> word_tokenize(sentence, format=\"text\")\n",
    "'Ch√†ng_trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43cefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_tokenize(sen,split_word=False):\n",
    "    # optional step: fix the whitespace between words\n",
    "    sen = \" \".join(sen.split())\n",
    "    sens = sent_tokenize(sen)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen+=word_tokenize(sen,format='text' if not split_word else None)\n",
    "    \n",
    "    if not split_word:\n",
    "        return ''.join(tokenized_sen)\n",
    "    return ['_'.join(words.split(' ')) for words in tokenized_sen]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63754082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '?', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi? Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f171898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'üòå_üòå', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c089cc",
   "metadata": {},
   "source": [
    "Apply on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32166f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [apply_word_tokenize(s[0],True) for s in train_df[['text']].values]\n",
    "train_label = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90044db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [apply_word_tokenize(s[0],True) for s in test_df[['text']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc34d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'tuy·ªát_v·ªùi',\n",
       "  '.',\n",
       "  'y',\n",
       "  'h√¨nh',\n",
       "  'ch·ª•p',\n",
       "  '.',\n",
       "  'ƒë√°ng',\n",
       "  'ti·ªÅn'],\n",
       " ['Hjhj_shop',\n",
       "  'giao',\n",
       "  'h√†ng',\n",
       "  'nhanh',\n",
       "  'qu√°',\n",
       "  '.',\n",
       "  'ƒê·∫πp',\n",
       "  'l·∫Øm',\n",
       "  '·∫°',\n",
       "  'b√©',\n",
       "  'nh√†',\n",
       "  'm',\n",
       "  'r·∫•t',\n",
       "  'th√≠ch'],\n",
       " ['nh√¨n', 'ƒë·∫πp', 'ph·∫øt', 'nh·ªâ', '..'],\n",
       " ['ƒê√≥ng_g√≥i',\n",
       "  'r·∫•t',\n",
       "  'ƒë·∫πp',\n",
       "  '.',\n",
       "  'Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'r·∫•t',\n",
       "  't·ªët',\n",
       "  'Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'tuy·ªát_v·ªùi'],\n",
       " ['SƒÉn', 'ƒëc', 'v·ªõi', 'gi√°', '11', 'k', '.', 'To·∫πt', 'v·ªùi']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2f6f",
   "metadata": {},
   "source": [
    "## Step 2: Model's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed8d0",
   "metadata": {},
   "source": [
    "To use pretrained language model such as BERT, GPT, Roberta... We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n",
    "\n",
    "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their hub. In this notebook, we use PhoBERT-base (vinai/phobert-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f842db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e1bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_explain(inp,split_word):\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()\n",
    "\n",
    "\n",
    "def two_step_tokenization_explain(inp,split_word=False):\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Pretokenization ---')\n",
    "    tok = apply_word_tokenize(inp,split_word)\n",
    "    print(tok)\n",
    "    print()\n",
    "    tokenizer_explain(tok,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 4, 7079, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7978be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi .Ph·∫•n m·ªãn , ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 2586, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.@@', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi.Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7f2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'üòå_üòå', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 3, 1751, 3, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '<unk>', '_@@', '<unk>', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi <unk> _<unk>. Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c4f",
   "metadata": {},
   "source": [
    "# Use HuggingFace Dataset to store and tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2481f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f388127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # pad to model's allowed max length, which is max_sequence_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c4b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text[:4],\n",
    "                        'label':train_label[:4],\n",
    "                        }\n",
    "                    )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1572e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8b3fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df12176ac4c4965a635148e2d438e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10db51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013debff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dung', 'dc', 'sp', 'tot', 'cam', 'on_shop', 'ƒê√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi'], ['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Son', 'm·ªãn', 'nh∆∞ng', 'khi', 'ƒë√°nh', 'l√™n', 'kh√¥ng', 'nh∆∞', 'm√†u', 'tr√™n', '·∫£nh'], ['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'nh∆∞ng', 'k', 'c√≥', 'h·ªôp', 'k', 'c√≥', 'd√¢y', 'gi√†y', 'ƒëen', 'k', 'c√≥', 't·∫•t'], [':', '(', '(', 'M√¨nh', 'h∆°i', 'th·∫•t_v·ªçng', '1', 'ch√∫t', 'v√¨', 'm√¨nh', 'ƒë√£', 'k·ª≥_v·ªçng', 'cu·ªën', 's√°ch', 'kh√°', 'nhi·ªÅu', 'hi_v·ªçng', 'n√≥', 's·∫Ω', 'n√≥i', 'v·ªÅ', 'vi·ªác', 'h·ªçc_t·∫≠p', 'c·ªßa', 'c√°ch', 'sinh_vi√™n', 'tr∆∞·ªùng', 'Harvard', 'ra_sao', 'nh·ªØng', 'n·ªó_l·ª±c', 'c·ªßa', 'h·ªç', 'nh∆∞', 'th·∫ø_n√†o', '4', 'h', 's√°ng', '?', 't·∫°i_sao', 'h·ªç', 'l·∫°i', 'ph·∫£i', 'th·ª©c', 'd·∫≠y', 'v√†o', 'th·ªùi_kh·∫Øc', 'ƒë·∫•y', '?', 'sau', 'ƒë√≥', 'l√†', 'c·∫£', 'm·ªôt', 'c√¢u_chuy·ªán', 'ra_sao', '.', 'C√°i', 'm√¨nh', 'th·ª±c_s·ª±', 'c·∫ßn', '·ªü', 'ƒë√¢y', 'l√†', 'c√¢u_chuy·ªán', '·∫©n', 'd·∫•u', 'trong', 'ƒë√≥', 'ƒë·ªÉ', 't·ª±', 'b·∫£n_th√¢n', 'm·ªói', 'ng∆∞·ªùi', 'c·∫£m_nh·∫≠n', 'v√†', 'ƒëi', 's√¢u', 'v√†o', 'l√≤ng', 'ng∆∞·ªùi', 'h∆°n', '.', 'C√≤n', 'cu·ªën', 's√°ch', 'n√†y', 'ch·ªâ', 'ƒë∆°n_thu·∫ßn', 'l√†', 'cu·ªën', 's√°ch', 'd·∫°y', 'kƒ©_nƒÉng', 'm√†', 'h·∫ßu_nh∆∞', 's√°ch', 'n√†o', 'c≈©ng', 'ƒë√£', 'c√≥', '.', 'BU·ªìn', '...']]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f6c2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3556, 1236, 1894, 36150, 2225, 1204, 2947, 1672, 20811, 54922, 55662, 1685, 265, 59, 258, 6, 994, 6869, 265, 1819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 5, 16332, 5451, 51, 26, 480, 72, 17, 42, 412, 34, 284, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 51, 1947, 10, 2275, 1947, 10, 1747, 2466, 989, 1947, 10, 7328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 27, 20, 20, 3000, 1329, 2804, 99, 2013, 90, 68, 14, 2109, 1088, 713, 281, 36, 4876, 231, 38, 96, 28, 49, 1227, 7, 139, 649, 212, 9913, 57964, 3075, 21, 773, 7, 86, 42, 1279, 163, 1664, 298, 114, 2393, 86, 44, 41, 2908, 1764, 33, 9171, 1582, 114, 53, 37, 8, 94, 16, 876, 57964, 3075, 5, 2510, 68, 742, 115, 25, 97, 8, 876, 4592, 3309, 12, 37, 24, 385, 744, 205, 18, 2601, 6, 57, 808, 33, 605, 18, 48, 5, 631, 1088, 713, 23, 66, 5284, 8, 1088, 713, 940, 10685, 64, 2903, 713, 142, 32, 14, 10, 5, 924, 1878, 5460, 135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1e74a",
   "metadata": {},
   "source": [
    "# Define dataset dict and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1925dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b22df4c38434b7888efe9130ff5f643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63a6238ea644b1daa3c5114b5d062df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text,\n",
    "                        'label':train_label,\n",
    "                        }\n",
    "                    )\n",
    "test_dataset = Dataset.from_dict(\n",
    "                        {'text': test_text,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True)\n",
    "test_dataset_tokenized= test_dataset.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d637faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "train_dataset_tokenized = train_dataset_tokenized.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2184388",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized = DatasetDict()\n",
    "main_dataset_tokenized['train'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8)))\n",
    "main_dataset_tokenized['validation'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8),train_dataset_tokenized.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed94e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3218\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ebcace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.580154\n",
       "1    0.419846\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['train']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d049de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.563704\n",
       "1    0.436296\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['validation']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4fb0",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40541781",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eecb3a",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089eb60",
   "metadata": {},
   "source": [
    "## Classification using default PhoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d414ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "# from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "# from transformers.models.roberta.modeling_roberta import RobertaModel \n",
    "# from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f89e470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "522bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_init(get_hidden=False):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2,output_hidden_states=get_hidden)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780da385",
   "metadata": {},
   "source": [
    "## Define helper function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77905e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gc\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # pred: EvalPrediction object \n",
    "    # (which is a named tuple with predictions and label_ids attributes)\n",
    "    labels = pred.label_ids\n",
    "    if isinstance(pred.predictions,tuple):\n",
    "        preds = pred.predictions[0].argmax(-1)\n",
    "    else:\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1,\"accuracy\": acc}\n",
    "\n",
    "\n",
    "\n",
    "def finetune(lr,bs,wd,epochs,ddict,tokenizer=tokenizer,o_dir = './outputs',logging=False,model_init=base_model_init):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not logging:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    else:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                               logging_dir=os.path.join(o_dir, 'log'),\n",
    "                                logging_steps = len(ddict[\"train\"]) // bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd)\n",
    "\n",
    "    # instantiate trainer\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=ddict['train'],#.shard(200, 0),    # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "        eval_dataset=ddict['validation'],#.shard(100, 0), # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "#         data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a25275",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9cdad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12869\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 09:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288897</td>\n",
       "      <td>0.907470</td>\n",
       "      <td>0.908950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.298636</td>\n",
       "      <td>0.897811</td>\n",
       "      <td>0.899627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.306182</td>\n",
       "      <td>0.909048</td>\n",
       "      <td>0.910193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-403\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-403/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-403/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-403/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-806\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-806/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-806/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-806/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-1209\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-1209/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-1209/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-1209/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr = 1e-4\n",
    "# bs=32\n",
    "# wd=0.01\n",
    "# epochs= 3\n",
    "# o_dir = './default_phobert_finetuned_1'\n",
    "# tmp = finetune(lr,bs,wd,epochs,ddict=main_dataset_tokenized,o_dir = o_dir)\n",
    "\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1\tAccuracy\n",
    "# 1\tNo log\t0.288897\t0.907470\t0.908950\n",
    "# 2\t0.298100\t0.298636\t0.897811\t0.899627\n",
    "# 3\t0.180200\t0.306182\t0.909048\t0.910193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ea961a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12869\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2146' max='6435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2146/6435 04:46 < 09:33, 7.47 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 73/537 00:06 < 00:39, 11.73 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 11.77 GiB total capacity; 10.16 GiB already allocated; 239.88 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      5\u001b[0m o_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./default_phobert_finetuned_1_hiddenstate\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mddict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_dataset_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43mo_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mo_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m(lr, bs, wd, epochs, ddict, tokenizer, o_dir, logging, model_init)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# instantiate trainer\u001b[39;00m\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     57\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[1;32m     58\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:1500\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1497\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1499\u001b[0m )\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:1834\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1834\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1838\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m             metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   2047\u001b[0m                 eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m   2048\u001b[0m                 ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2049\u001b[0m                 metric_key_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2050\u001b[0m             )\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:2774\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2771\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2773\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2774\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2775\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2777\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2784\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2785\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2786\u001b[0m     speed_metrics(\n\u001b[1;32m   2787\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     )\n\u001b[1;32m   2792\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer.py:2979\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2978\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[0;32m-> 2979\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_prediction_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;66;03m# Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:113\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    110\u001b[0m     new_tensors\n\u001b[1;32m    111\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:115\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py:74\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     71\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     77\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 334.00 MiB (GPU 0; 11.77 GiB total capacity; 10.16 GiB already allocated; 239.88 MiB free; 10.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=6\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "o_dir = './default_phobert_finetuned_1_hiddenstate'\n",
    "tmp = finetune(lr,bs,wd,epochs,ddict=main_dataset_tokenized,o_dir = o_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11fc7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1209  checkpoint-403  checkpoint-537  checkpoint-806\r\n"
     ]
    }
   ],
   "source": [
    "!ls default_phobert_finetuned_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7655e9",
   "metadata": {},
   "source": [
    "# Prediction interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57401c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model_init()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.from_pretrained('./default_phobert_finetuned_1/checkpoint-806')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d78ff2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2str={0:\"Positive\",1:'Negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f951b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def label_int2str(row):\n",
    "    return idx2str[row]\n",
    "\n",
    "def forward_pass_with_label(batch,model=None):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy(),\n",
    "           'predicted_probability': torch.nn.functional.softmax(output.logits.cpu(),dim=1).numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac5b647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "main_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d9bb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ef1fec038413fb0e6afd0c1d31beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/805 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b3e8a8cb64f6cb896676357428607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute loss values\n",
    "main_dataset_tokenized[\"train\"] = main_dataset_tokenized[\"train\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)\n",
    "main_dataset_tokenized[\"validation\"] = main_dataset_tokenized[\"validation\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98453d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", 'predicted_probability',\"loss\"]\n",
    "df_val = main_dataset_tokenized[\"validation\"][:][cols]\n",
    "df_val[\"label_str\"] = df_val[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_val[\"predicted_label_str\"] = (df_val[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4047fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = main_dataset_tokenized[\"train\"][:][cols]\n",
    "df_trn[\"label_str\"] = df_trn[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_trn[\"predicted_label_str\"] = (df_trn[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31a12ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12869, 7), (3218, 7))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.shape,df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a301ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Th√¥ng_tin, chi_ti·∫øt, tr√™n, web, l√†, s·∫£n_xu·∫•t,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0779537, 0.92204636]</td>\n",
       "      <td>0.081160</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ƒê·ªìng_h·ªì, d√¢y_ƒëeo, ·ªçp_·∫πp, ., T·∫•t_nilon, r·∫ª_ti·ªÅ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04499709, 0.9550029]</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Shop, de, th∆∞∆°ng, nhiet, t√¨nh, s·∫Ω, ·ªßng, h√¥, l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99736834, 0.0026315863]</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nh√¨n, k, ch·∫Øc_ch·∫Øn, l·∫Øm]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5285818, 0.47141826]</td>\n",
       "      <td>0.637558</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ch·∫•t_l∆∞·ª£ng, s·∫£n_ph·∫©m, r·∫•t, k√©m, ., R√°ch, l·ªói,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.022633858, 0.9773662]</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [Th√¥ng_tin, chi_ti·∫øt, tr√™n, web, l√†, s·∫£n_xu·∫•t,...      1                1   \n",
       "1  [ƒê·ªìng_h·ªì, d√¢y_ƒëeo, ·ªçp_·∫πp, ., T·∫•t_nilon, r·∫ª_ti·ªÅ...      1                1   \n",
       "2  [Shop, de, th∆∞∆°ng, nhiet, t√¨nh, s·∫Ω, ·ªßng, h√¥, l...      0                0   \n",
       "3                          [Nh√¨n, k, ch·∫Øc_ch·∫Øn, l·∫Øm]      0                0   \n",
       "4  [Ch·∫•t_l∆∞·ª£ng, s·∫£n_ph·∫©m, r·∫•t, k√©m, ., R√°ch, l·ªói,...      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0     [0.0779537, 0.92204636]  0.081160  Negative            Negative  \n",
       "1     [0.04499709, 0.9550029]  0.046041  Negative            Negative  \n",
       "2  [0.99736834, 0.0026315863]  0.002635  Positive            Positive  \n",
       "3     [0.5285818, 0.47141826]  0.637558  Positive            Positive  \n",
       "4    [0.022633858, 0.9773662]  0.022894  Negative            Negative  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc7b897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C·∫ßm, kh√°, ch·∫Øc_tay, ., Tr·ª•c, u·ªën, to, ƒë√∫ng, q...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.997373, 0.0026270347]</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3G, c√≤n, t√πy, l√∫c_n√†o, v√†, khi, n√†o, v√†, ·ªü, ƒë...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99603903, 0.0039609983]</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ƒê√≥ng_g√≥i, s·∫£n_ph·∫©m, r·∫•t, ch·∫Øc_ch·∫Øn, ., Th·ªùi_g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99715614, 0.0028438682]</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[m√¨nh, x√†i, em, n√†y, b·ªã, ƒë·ª©ng, m√°y, ho√†i, √†]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.13099755, 0.8690025]</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ch·∫øt, $]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.09323443, 0.9067656]</td>\n",
       "      <td>0.097871</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [C·∫ßm, kh√°, ch·∫Øc_tay, ., Tr·ª•c, u·ªën, to, ƒë√∫ng, q...      0                0   \n",
       "1  [3G, c√≤n, t√πy, l√∫c_n√†o, v√†, khi, n√†o, v√†, ·ªü, ƒë...      0                0   \n",
       "2  [ƒê√≥ng_g√≥i, s·∫£n_ph·∫©m, r·∫•t, ch·∫Øc_ch·∫Øn, ., Th·ªùi_g...      0                0   \n",
       "3       [m√¨nh, x√†i, em, n√†y, b·ªã, ƒë·ª©ng, m√°y, ho√†i, √†]      1                1   \n",
       "4                                          [Ch·∫øt, $]      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0    [0.997373, 0.0026270347]  0.002630  Positive            Positive  \n",
       "1  [0.99603903, 0.0039609983]  0.003969  Positive            Positive  \n",
       "2  [0.99715614, 0.0028438682]  0.002848  Positive            Positive  \n",
       "3     [0.13099755, 0.8690025]  0.140409  Negative            Negative  \n",
       "4     [0.09323443, 0.9067656]  0.097871  Negative            Negative  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a29fd",
   "metadata": {},
   "source": [
    "## Classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8f10912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.96      0.96      0.96      7466\n",
      "    Negative       0.94      0.95      0.94      5403\n",
      "\n",
      "    accuracy                           0.95     12869\n",
      "   macro avg       0.95      0.95      0.95     12869\n",
      "weighted avg       0.95      0.95      0.95     12869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_trn.label.values,df_trn.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0670fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.91      0.92      0.91      1814\n",
      "    Negative       0.89      0.88      0.88      1404\n",
      "\n",
      "    accuracy                           0.90      3218\n",
      "   macro avg       0.90      0.90      0.90      3218\n",
      "weighted avg       0.90      0.90      0.90      3218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_val.label.values,df_val.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d826ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb4ac47fdf0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,10))\n",
    "ConfusionMatrixDisplay.from_predictions(df_val.label.values,df_val.predicted_label.values,\n",
    "                                        display_labels=['Positive','Negative'],xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4098",
   "metadata": {},
   "source": [
    "## Most confident prediction: right vs wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c842659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prob_match(row):\n",
    "    l = row['label']\n",
    "    prob = row['predicted_probability']\n",
    "    return prob[l]\n",
    "def _get_prob_mismatch(row):\n",
    "    prob = row['predicted_probability']\n",
    "    return np.max(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d1e4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['correct_confidence'] = df_val.loc[df_val.label==df_val.predicted_label].apply(_get_prob_match,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e765bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['incorrect_confidence'] = df_val.loc[df_val.label!=df_val.predicted_label].apply(_get_prob_mismatch,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f68965e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correct_pred(df,cols=['text','label_str','correct_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('correct_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(tmp[1:])\n",
    "        print('-'*100)\n",
    "def analyze_incorrect_pred(df,cols=['text','label_str','predicted_label_str','incorrect_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('incorrect_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(f'True label: {tmp[1]}, but predict {tmp[2]}, with confidence {tmp[3]}')\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45aa524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mua | ao | mua | bo_shop | giao | ao | mua | doi | LAM | sao | DANH_Gi√° | the | nao | day\n",
      "['Negative' 0.5087628960609436]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "B√¥ng | m·ªÅm | h∆°n | nh∆∞ng | c≈©ng | m·ªèng | h∆°n | ƒë·ª£t | tr∆∞·ªõc | d√πng\n",
      "['Positive' 0.5093692541122437]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ghi | date | T2 | / | 2019 | n√™n | m·ªõi | ƒë·∫∑t_h√†ng | ƒë·∫øn | 5 | g√≥i | nh·∫≠n | h√†ng | th√¨ | T1 | / | 2019 | th·ª±c_ph·∫©m | cho | b√© | n√™n | b√°n | ƒë√∫ng_h·∫°n | s·ª≠_d·ª•ng | gi·ªõi_thi·ªáu | ƒë·ªÉ | kh√°ch | tin_t∆∞·ªüng | & | quy·∫øt_ƒë·ªãnh | mua | h√†ng\n",
      "['Negative' 0.5200026035308838]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pin | t·ª•t | nhanh | nh∆∞ng | kh√¥ng | b·ªã | s·∫≠p | ngu·ªìn\n",
      "['Positive' 0.5242894887924194]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "M·ªü_ra | c√≥ | m·ªói | v·ªè | ko | th·∫•y | k√≠nh | c∆∞·ªùng_l·ª±c | ·ªü | trong\n",
      "['Negative' 0.5256175994873047]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh√¨n | k | ch·∫Øc_ch·∫Øn | l·∫Øm\n",
      "['Positive' 0.5285817980766296]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "V·ª´a | m·ªõi | mua | s√°ng | nay | th√¨ | 333.000 | VND | gi·ªù | v√†o | nh√¨n | l·∫°i | th·∫•y | 189.000 | VND | ! | ! | ! | !\n",
      "['Negative' 0.5287574529647827]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | . | Tuy_nhi√™n | giao | thi·∫øu | s·ªë_l∆∞·ª£ng | . | M√¨nh | ƒë·∫∑t | 2 | thanh_to√°n | 2 | nh·∫≠n | ƒë∆∞·ª£c | ch·ªâ | c√≥ | 1 | c√°i\n",
      "['Positive' 0.5362181067466736]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H√†ng | d√πng | t·∫°m | ƒë∆∞·ª£c | tuy_nhi√™n | h∆°i | l·ªèng_l·∫ªo | . | K | ch·∫Øc_ch·∫Øn | l·∫Øm | .\n",
      "['Positive' 0.538158655166626]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Giao | h√†ng | l√¢u | ∆°i | l√† | l√¢u | ch·∫Øc | do | tr·∫£ | ti·ªÅn | tr∆∞·ªõc | : | (\n",
      "['Positive' 0.5443349480628967]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | giao | h√†ng | kh | gi·ªëng | m√†u\n",
      "['Negative' 0.5501789450645447]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dao | h√†ng | l√¢u | ƒë·∫∑t | h·∫≥n | 2 | l·∫ßn | m·ªõi | dao | g·∫ßn | th√°ng | m·ªõi | c√≥ | h√†ng | ....\n",
      "['Negative' 0.5585803389549255]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "S·∫£n_ph·∫©m | ok | t·ªëc_ƒë·ªô | kh√¥ng | dc | nh∆∞ | mong_ƒë·ª£i\n",
      "['Positive' 0.5587018132209778]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "B·ªôt | m·ªãn | h·ª≠i | th√¨ | c√≥ | m√πi | th∆°m | nh∆∞ng | khi | d√πng | th√¨ | kh√¥ng | ƒë·∫≠m | m√πi | ƒë·∫≠u_n√†nh | bao_b√¨ | in | m·ªù_nh·∫°t\n",
      "['Negative' 0.5603232979774475]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng | g√≥ii | ƒëu·ªçc\n",
      "['Negative' 0.5685207843780518]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "V·∫£i | kh√° | m·ªèng | . | Ti·ªÅn | n√†o | c·ªßa | n·∫•y | . | N√≥i_chung | h∆°n | gi√†y_bata | m·ªôt | t√Ω | .\n",
      "['Negative' 0.5702218413352966]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tr∆∞·ªõc_m·∫Øt | th√¨ | th·∫•y | oke | r·ªìi | ƒë√≥ | c·ª•c | n√†y | ch·ªçi | tr√¢u | c≈©ng | ch·∫øt | üòÇ_üòÇ_üòÇ\n",
      "['Positive' 0.5918132066726685]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "V∆∞·ª£t | ngo√†i | mong_ƒë·ª£i\n",
      "['Positive' 0.5925284624099731]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "m√¨nh | mua | 4 | chai | trong | ƒë√≥ | c√≥ | 2 | chai | l∆∞ng | KHG | ƒë·ªß | dung_t√≠ch\n",
      "['Negative' 0.595950722694397]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Minh_dung | sp | nay | gan | 10 | nam | roi | do | sp | rat | tot | ..\n",
      "['Positive' 0.5961607694625854]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_correct_pred(df_val,ascending=True,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6c3381f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". | H√†ng | r·∫•t | ƒë·∫πp | <3 | !\n",
      "True label: Negative, but predict Positive, with confidence 0.9974151849746704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tam | duoc | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn\n",
      "True label: Negative, but predict Positive, with confidence 0.997407853603363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | s·∫£n_ph·∫©m | tuy·ªát_v·ªùi | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | Shop | ph·ª•c_v·ª• | r·∫•t | t·ªët | Th·ªùi_gian | giao | h√†ng | ch·∫≠m | R·∫•t | ƒë√°ng | ti·ªÅn | Th·ªùi_gian | giao | h√†ng | ch·∫≠m\n",
      "True label: Negative, but predict Positive, with confidence 0.997378945350647\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | s·∫£n_ph·∫©m | tuy·ªát_v·ªùi | nh∆∞ng | to√†n | ch·ªØ | Trung_Qu·ªëc | .\n",
      "True label: Negative, but predict Positive, with confidence 0.997368574142456\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H√†ng | y | h√¨nh | ƒë·∫πp | sang_tr·ªçng\n",
      "True label: Negative, but predict Positive, with confidence 0.9973480701446533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "chat | l∆∞·ª£ng | r·∫•t | t·ªët | ok\n",
      "True label: Negative, but predict Positive, with confidence 0.9973419308662415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "t·ªët | dong | goi | dep | nhung | the | nho | bi | hong | kh√¥ng | d√πng | duoc | ngo√†i | Ra | OK | üëå\n",
      "True label: Negative, but predict Positive, with confidence 0.9972805976867676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üò° | üò°\n",
      "True label: Negative, but predict Positive, with confidence 0.9972085356712341\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | ph·ª•c_v·ª• | r·∫•t | t·ªët | Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh | ti·ªÅn | n√†o | th√¨ | c·ªßa | ƒë√≥\n",
      "True label: Negative, but predict Positive, with confidence 0.9970918893814087\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Take | wrong | color\n",
      "True label: Negative, but predict Positive, with confidence 0.9969770908355713\n",
      "----------------------------------------------------------------------------------------------------\n",
      "·ªêp | h∆°i | c≈©_shop | nha | .\n",
      "True label: Negative, but predict Positive, with confidence 0.9968775510787964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "M√†u | qu√° | gh√™ | huhu\n",
      "True label: Negative, but predict Positive, with confidence 0.9968618154525757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | . | Tr·ª´ | th·∫≥ng | 3_* | v√¨ | ƒë·ªôi_gi√° | l√™n | qu√° | cao | so | v·ªõi | th·ª±c_t·∫ø\n",
      "True label: Negative, but predict Positive, with confidence 0.9968488812446594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | . | H√†ng | y_·∫£nh | 100_% | m·∫´u_m√£ | ƒë·∫πp | c·∫ßm | ch·∫Øc | tay | nh·ªè | g·ªçn | mk | th·∫•y | kh√° | ti·ªán | . | ( | s·ª≠a | b√†i | khi | sd | : | s·∫°c | ch·∫≠m | nhah | t·ª•t | pin | k | h√†i | l√≤g | )\n",
      "True label: Negative, but predict Positive, with confidence 0.9968122839927673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh\n",
      "True label: Negative, but predict Positive, with confidence 0.9967789053916931\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh∆∞lon | . | üòå\n",
      "True label: Negative, but predict Positive, with confidence 0.9966498017311096\n",
      "----------------------------------------------------------------------------------------------------\n",
      "c≈©ng | ƒë·∫πp | nh∆∞ng | may | l·ªùi\n",
      "True label: Negative, but predict Positive, with confidence 0.9964595437049866\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | h√†ng | c·∫©n_th·∫≠n | S·∫£n_ph·∫©m | √≠t | b·ªçt | r·ª≠a | xong | th·∫•y | da_kh√¥ | cƒÉng | . | N√™n | d√πng | th√™m | v·ªõi | x·ªãt | kho√°ng | ƒë·ªÉ | d∆∞·ª°ng | ·∫©m\n",
      "True label: Negative, but predict Positive, with confidence 0.9962769150733948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "L·ª´a | nhau | √† | shop\n",
      "True label: Negative, but predict Positive, with confidence 0.9957062602043152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | s·∫£n_ph·∫©m | r·∫•t | kem | moi | mua | co | may | ngay | Ma_di_khong | dung | gio | moi | nguoi | dung | co | mua | hang | cua_shop | nua\n",
      "True label: Negative, but predict Positive, with confidence 0.9955772161483765\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=False,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "737caf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kh·∫©u_v·ªã | c·ªßa | m√¨nh | th√¨ | h∆°i | nh·∫°t | v√† | thi·∫øu | cay | :)\n",
      "True label: Positive, but predict Negative, with confidence 0.5006212592124939\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | ƒë·∫πp | . | Nh∆∞ng | pin | ko | b·ªÅn | . | N·∫°p | s·∫°c | d·ª±_ph√≤ng | c·∫£ | ƒë√™m | m·ªõi | ƒë·∫ßy | m√† | S·∫°c | ch·ªâ | dc | 1 | l·∫ßn | l√† | h·∫øt | . | - | _ | -\n",
      "True label: Positive, but predict Negative, with confidence 0.5072213411331177\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | c·ªßa | s·∫£n_ph·∫©m | ph√π_h·ª£p | v·ªõi | gi√° | ti·ªÅn | . | C√≥_th·ªÉ | h√£ng | c·∫ßn | nghi√™n_c·ª©u | sao | cho | c·ª•c | s·∫°c | √≠t | n√≥ng | trong | qu√°_tr√¨nh | s·∫°c | v√¨ | hi·ªán_nay | khi | s·∫°c | c·ª•c | s·∫°c | r·∫•t | n√≥ng | n√™n | t√¢m_l√Ω | ng∆∞·ªùi | s·ª≠_d·ª•ng | s·ª£ | n√≥ | ch√°y | n·ªï | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5155402421951294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "100 | k | th√¨ | v·∫≠y | l√† | t·ªët | r·ªìi | ... | mang | ƒëi | ch∆°i | c≈©ng | ƒë·∫πp | ph·∫øt | ... | ƒë·ª´ng | ho·∫°t_ƒë·ªông | m·∫°nh | th√¨ | ch·∫Øc | kh√¥ng | sao | ... | d√πng | ƒë∆∞·ª£c | 2 | tu·∫ßn | r·ªìi | , | gi·∫∑c | lu√¥n | r·ªìi | .... | ·ªîN | ! | ! | C√≥ | c√°i | l√† | ƒë·∫øn | khi | shiper | ƒë·∫øn | c·ªïng | m·ªõi | ƒëi·ªán_tho·∫°i | ch·ª© | ch·∫£ | b√°o | tr∆∞·ªõc | g√¨ | c·∫£ | ? | ?\n",
      "True label: Positive, but predict Negative, with confidence 0.5188581943511963\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SHOP | ƒêƒÇNG | ·∫¢NH | M·ªòT_ƒê·∫∞NG | S·∫¢N_PH·∫®M | M·ªòT | L·∫∫O | ._B√ÅN | C√ÅP | TH√å | √öP | H√åNH | C√ÅI | C√ÅP | TH√îI | CH·ª® | UP | C·∫¢ | C·ª§C | S·∫†C | G√ÇY | NH·∫¶M_L·∫™N | CHO | KH√ÅCH_H√ÄNG | L√Ä | COMBO_C√ÅP | + | D√ÇY | S·∫†C | . | T√îI | MUA | LI·ªÄN | 2 | C√ÅI | L√öC | NH·∫¨N | H√ÄNG | M·ªöI | NG·ªö | NG∆Ø·ªúI | RA | B·ªä | MUA | ƒê·∫ÆT | . | CH·∫§T_L∆Ø·ª¢NG | S·∫¢N_PH·∫®M | CH∆ØA | R√ï | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5189838409423828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | ch·∫≠m | . | doi | nhan | hang | s·ªët | ca | ru·ªôt\n",
      "True label: Negative, but predict Positive, with confidence 0.5209054946899414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "C√≥ | ai | d√πng | 6 | s | l√™n | 9.2.1 | l·∫°i | nhanh | h·∫øt | pin | nh∆∞ | t√¥i | kh√¥ng | . | Nhanh | h·∫øt | pin | qu√°\n",
      "True label: Negative, but predict Positive, with confidence 0.523202657699585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "M√¨nh | kh√¥ng | d√°m | d√πng | v√¨ | th·∫•y | m√£_v·∫°ch | 69 | : | ' | ( | . | T·∫°i | b·∫£n_th√¢n | ch∆∞a | t√¨m_hi·ªÉu | kƒ© | n√™n | c≈©ng | mua | cho | ƒë·ªß | gi√° | free | ship\n",
      "True label: Positive, but predict Negative, with confidence 0.5317555069923401\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê∆∞·ª£c | co | Th·ªÉ | mua | l√¢u | dai\n",
      "True label: Positive, but predict Negative, with confidence 0.5340108871459961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√£ | ch√°t | th·ªëng_nh·∫•t | lo·∫°i | h√†ng | nh∆∞ng | shop | v·∫´n | g·ª≠i | h√†ng | ng·∫´u_nhi√™n | . | Ko | s·ª≠_d·ª•ng | ƒëc | ch√°n\n",
      "True label: Negative, but predict Positive, with confidence 0.5393141508102417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh | M√†u_s·∫Øc | kh√¥ng | ƒë√∫ng | d√π | m√¨nh | ƒë√£ | d·∫∑n | tr∆∞·ªõc\n",
      "True label: Positive, but predict Negative, with confidence 0.5393228530883789\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qu·∫°t | m·ªõi | mua | nh∆∞ng | kh√¥ng | qu·∫°t | ƒë∆∞·ª£c\n",
      "True label: Negative, but predict Positive, with confidence 0.5425977110862732\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H∆°i | r·ªông | so | vs | size | b√¨nh_th∆∞·ªùng | . | H√†ng | i | h√¨nh\n",
      "True label: Positive, but predict Negative, with confidence 0.5448225736618042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê·∫∑t | c√≥ | 8 | mon | √† | giao | thi·∫øu | 2 | m√≥n | r·ªìi | l√†m_ƒÉn | th·∫≠t | qu√°_ƒë√°ng | m√†\n",
      "True label: Negative, but predict Positive, with confidence 0.5459489822387695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | oi | mua | nhi·ªÅu | h∆°n | sao | l·∫°i | ti·ªÅn | ship | ƒë·∫Øt | h∆°n\n",
      "True label: Positive, but predict Negative, with confidence 0.5503779649734497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh·∫°t_nh·∫Ωo | ch·ªâ | c√≥ | m·ªói | b√°nh_ph·ªü | dai | l·∫°t | v√† | n∆∞·ªõc | gia_v·ªã | trong_veo | . | ƒÇn | hao_hao | h·ªß_ti·∫øu | lu·ªôc | v·ªõi | n∆∞·ªõc | Knor | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5527167916297913\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H√†ng | nh√°i | . | From | gi·∫ßy | kg | ƒë·∫πp | . | Ph√π_h·ª£p | v·ªõi | gi√° | ti·ªÅn | ƒë√£ | gi·∫£m | . | Ch·ª© | kg | nh∆∞ | h√†ng | qu√£ng | c√°o | . | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn\n",
      "True label: Positive, but predict Negative, with confidence 0.5576053261756897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh∆∞ | shit | ngta | v·∫≠y | √° | .._Hg | l√†m | ƒë∆∞·ª£c | tr√≤ | ch·ªëng | g√¨ | hit\n",
      "True label: Negative, but predict Positive, with confidence 0.5576322674751282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "kh√≥ | u·ªëng | m√¨nh | c·ª© | ng·ª° | l√† | ƒëang | u·ªëng | v√† | ƒë·∫Øp | b·ªôt | ngh·ªá | m√† | m√¨nh | mua | v·ªÅ | ƒë·ªÉ | n·∫•u_ƒÉn | üò≠\n",
      "True label: Negative, but predict Positive, with confidence 0.5652170181274414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Em | ƒë√£ | mua | gi√†y | m√† | em | kh√¥ng | th·∫•y | d√¢y | gi√†y | ƒë√¢u | h·∫øt | shop | ·∫°\n",
      "True label: Negative, but predict Positive, with confidence 0.5652967691421509\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=True,top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d6354",
   "metadata": {},
   "source": [
    "# Extract hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f60ed705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHO_PATH = Path('/home/quan1080/kwon/PhoBERT_base_transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f64f7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RobertaForHiddenState(RobertaPreTrainedModel):\n",
    "#     # make sure standard XLM-R are used\n",
    "#     config_class = RobertaConfig\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.num_labels = config.num_labels # TODO: change it to ner+chunk output\n",
    "#         # Load model body\n",
    "#         # add_polling_layer to False\n",
    "#         #    to ensure all hidden states are returned \n",
    "#         #    and not only the one associated with the [CLS] token.\n",
    "#         self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "#         # Set up token classification head\n",
    "#         self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "#         # Load and initialize weights from RobertaPretrainedModel\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "#                 labels=None, **kwargs):\n",
    "#         # Use model body to get encoder representations\n",
    "#         # the only ones we need for now are input_ids and attention_mask\n",
    "#         outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "#                                token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "#         hidden_states = outputs['hidden_states'] # tuples with len 13 (number of layer/block)\n",
    "#         # each with shape: (bs,seq_len,hidden_size_len), e.g. for phobert: (bs,256, 768)\n",
    "#         # Note: hidden_size_len = embedding_size\n",
    "#         last_hidden_state = hidden_states[-1][:,0] # (bs,768)\n",
    "\n",
    "#         logits = self.dropout(last_hidden_state)\n",
    "#         logits = self.classifier(logits) # (bs,num_labels)\n",
    "        \n",
    "    \n",
    "# #         # Calculate losses\n",
    "#         loss = None\n",
    "#         if labels is not None: # labels size: (bs,seq_len) # TODO: change loss for dual prediction\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#         # Return model output object\n",
    "#         return SequenceClassifierOutput(loss=loss, logits=logits,\n",
    "#                                      hidden_states=outputs.hidden_states,\n",
    "#                                      attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e700532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_model_init():\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     config = AutoConfig.from_pretrained(\n",
    "#         PHO_PATH/'config.json',\n",
    "#         output_hidden_states=True,\n",
    "#         num_labels=2,\n",
    "#     )\n",
    "\n",
    "#     model = RobertaForHiddenState.from_pretrained(PHO_PATH/'model.bin',config=config).to(device)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc878c5",
   "metadata": {},
   "source": [
    "## Redefine dataset dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ff6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b10e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset.from_dict(\n",
    "#                         {'text': train_text,\n",
    "#                         'label':train_label,\n",
    "#                         }\n",
    "#                     )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )\n",
    "\n",
    "# train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True)\n",
    "# test_dataset_tokenized= test_dataset.map(tokenize_function,batched=True)\n",
    "\n",
    "# train_dataset_tokenized = train_dataset_tokenized.shuffle()\n",
    "train_sample = train_dataset_tokenized.shuffle().select(range(int(train_dataset_tokenized.num_rows*0.1)))\n",
    "sample_dataset_tokenized = DatasetDict()\n",
    "sample_dataset_tokenized['train'] = train_sample.select(range(int(train_sample.num_rows*0.8)))\n",
    "sample_dataset_tokenized['validation'] = train_sample.select(range(int(train_sample.num_rows*0.8),train_sample.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bb5c72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1286, 322)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset_tokenized['train'].num_rows,sample_dataset_tokenized['validation'].num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d3b30",
   "metadata": {},
   "source": [
    "## Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f99d4427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1286\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [645/645 01:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.886299</td>\n",
       "      <td>0.894410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.395047</td>\n",
       "      <td>0.892988</td>\n",
       "      <td>0.900621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.377929</td>\n",
       "      <td>0.908195</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 322\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1_hiddenstate/checkpoint-215\n",
      "Configuration saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-215/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 322\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1_hiddenstate/checkpoint-430\n",
      "Configuration saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-430/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 322\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1_hiddenstate/checkpoint-645\n",
      "Configuration saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1_hiddenstate/checkpoint-645/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=6\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "o_dir = './default_phobert_finetuned_1_hiddenstate'\n",
    "tmp = finetune(lr,bs,wd,epochs,ddict=sample_dataset_tokenized,o_dir = o_dir,model_init=partial(base_model_init,get_hidden=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f550445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tmp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fad914e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'lm_head.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'classifier.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ./default_phobert_finetuned_1/checkpoint-806 were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'classifier.out_proj.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'classifier.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'classifier.out_proj.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'classifier.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./default_phobert_finetuned_1/checkpoint-806 and are newly initialized: ['encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'classifier.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\",config=config)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.from_pretrained('./default_phobert_finetuned_1_hiddenstate/checkpoint-645')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "933e411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).hidden_states[-1]\n",
    "    # Return vector for [CLS] token\n",
    "    \n",
    "    # \"hidden_state\": the new transformer datasetdict will have\n",
    "    # an extra column called 'hidden_state'\n",
    "    # also, must return a numpy array/python object for datasetdict\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b5bbbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a8f83bebf47a2b316dcd59758bb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "sample_dataset_tokenized['validation'] = sample_dataset_tokenized['validation'].map(extract_hidden_states, batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "127007a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([322, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset_tokenized['validation']['hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcda0e2",
   "metadata": {},
   "source": [
    "## Find neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1affa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_dataset_tokenized['validation']['hidden_state'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b4a8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fd91b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "neigh.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a96bbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset_tokenized['validation']['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79f630c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str(inp):\n",
    "    return ' '.join(inp)\n",
    "def show_neighbors(neigh,idx,data=data,metadata=sample_dataset_tokenized['validation'],n_neighbors=5):\n",
    "    print(f\"Sentence: {concat_str(metadata['text'][idx])}\\nLabel: {metadata['label'][idx]}\")\n",
    "    distances,nbors = neigh.kneighbors([data[idx]])\n",
    "\n",
    "    print('\\nNeighbors: ')\n",
    "    for d,n_idx in zip(distances[0],nbors[0]):\n",
    "        print(f\"\\t{concat_str(metadata['text'][n_idx])}, d={d:.3f}, label={metadata['label'][n_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "166f9147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: R·∫•t ƒë√°ng ti·ªÅn\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tR·∫•t ƒë√°ng ti·ªÅn, d=0.000, label=0\n",
      "\tGi√†y r·∫•t ƒë·∫πp √™m ., d=0.724, label=0\n",
      "\tShop l√†m_vi·ªác OK l·∫Øm, d=0.870, label=0\n",
      "\tƒê·ªìng_h·ªì ƒë·∫πp, d=0.885, label=0\n",
      "\tM·∫´u_m√£ ƒë·∫πp hy_v·ªçng ch·∫•t_l∆∞·ª£ng t·ªët ., d=0.902, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c79f8",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- concat last 4 hidden states for better sentence presentation\n",
    "- run on 3090 for better results?\n",
    "- raw output from phobert and check representation => comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "10px",
    "top": "150px",
    "width": "315.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
