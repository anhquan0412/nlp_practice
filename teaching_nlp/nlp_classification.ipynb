{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf68ba4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Task:-Shopee-binary-sentiment-classification\" data-toc-modified-id=\"Task:-Shopee-binary-sentiment-classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Task: Shopee binary sentiment classification</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-crash-files-(do-once)\" data-toc-modified-id=\"Convert-crash-files-(do-once)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Convert crash files (do once)</a></span></li><li><span><a href=\"#Load-csv\" data-toc-modified-id=\"Load-csv-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load csv</a></span></li></ul></li><li><span><a href=\"#Vietnamese-tokenization\" data-toc-modified-id=\"Vietnamese-tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vietnamese tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Pretokenization-(Vietnamese-word-tokenization)\" data-toc-modified-id=\"Step-1:-Pretokenization-(Vietnamese-word-tokenization)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 1: Pretokenization (Vietnamese word tokenization)</a></span></li><li><span><a href=\"#Step-2:-Model's-tokenization\" data-toc-modified-id=\"Step-2:-Model's-tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 2: Model's tokenization</a></span></li></ul></li><li><span><a href=\"#Use-HuggingFace-Dataset-to-store-and-tokenize-corpus\" data-toc-modified-id=\"Use-HuggingFace-Dataset-to-store-and-tokenize-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Use HuggingFace Dataset to store and tokenize corpus</a></span></li><li><span><a href=\"#Load-pretrained-model\" data-toc-modified-id=\"Load-pretrained-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Load pretrained model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10c8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6d3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data_crash')\n",
    "RAW_PATH = Path('raw_crash')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5c682",
   "metadata": {},
   "source": [
    "# Task: Shopee binary sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f326",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83962f99",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convert crash files (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696ba470",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66966it [00:00, 1915214.36it/s]\n",
      "16087it [00:00, 134272.15it/s]\n",
      "34750it [00:00, 2274852.34it/s]\n",
      "10981it [00:00, 132888.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "### Cleaning training file\n",
    "\n",
    "train = open(RAW_PATH/\"train.crash\").readlines()\n",
    "id_locations = []\n",
    "label_locations = []\n",
    "for idx, line in tqdm(enumerate(train)):\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"train_\"):\n",
    "        id_locations.append(idx)\n",
    "    elif line == \"0\" or line == \"1\":\n",
    "        label_locations.append(idx)\n",
    "data = []\n",
    "\n",
    "for id_loc, l_loc in tqdm(zip(id_locations, label_locations)):\n",
    "    line_id = train[id_loc].strip()\n",
    "    label = train[l_loc].strip()\n",
    "    text = re.sub('\\s+', ' ', ' '.join(train[id_loc + 1: l_loc])).strip()[1:-1].strip()\n",
    "    data.append(f\"{line_id}\\t{text}\\t{label}\")\n",
    "\n",
    "with open(DATA_PATH/\"train.csv\", \"wt\") as f:\n",
    "    f.write(\"id\\ttext\\tlabel\\n\")\n",
    "    f.write(\"\\n\".join(data))\n",
    "\n",
    "### Cleaning test file\n",
    "\n",
    "test = open(RAW_PATH/\"test.crash\").readlines()\n",
    "id_locations = []\n",
    "for idx, line in tqdm(enumerate(test)):\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"test_\"):\n",
    "        id_locations.append(idx)\n",
    "data = []\n",
    "\n",
    "for i, id_loc in tqdm(enumerate(id_locations)):\n",
    "    if i >= len(id_locations) - 1:\n",
    "        end = len(test)\n",
    "    else:\n",
    "        end = id_locations[i + 1]\n",
    "    line_id = test[id_loc].strip()\n",
    "    text = re.sub('\\s+', ' ', ' '.join(test[id_loc + 1:end])).strip()[1:-1].strip()\n",
    "    data.append(f\"{line_id}\\t{text}\")\n",
    "\n",
    "with open(DATA_PATH/\"test.csv\", \"wt\") as f:\n",
    "    f.write(\"id\\ttext\\n\")\n",
    "    f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe363",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f367e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {str(DATA_PATH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv',sep='\\t').fillna(\"###\")\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv',sep='\\t').fillna(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cb72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13322</th>\n",
       "      <td>M·∫´u m√£ th√¨ ok nh∆∞ng ch·∫•t l∆∞·ª£ng ko ·ªïn l·∫Øm ch·ªâ g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9448</th>\n",
       "      <td>Giao h√†ng r·∫•t l√¢u kh√¥ng gi·ªëng trog ·∫£nh l·∫Øm Th·ªù...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>Gi·∫•y b√™n trong c√≥ k√≠ch th∆∞·ªõc kh√¥ng v·ª´a v·ªõi k√≠c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. √Åo m·∫∑c r·∫•t m√°t ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>Mua c·ªßa shop l·∫ßn 2 r·ªìi v√† l·∫ßn n√†o c≈©ng r·∫•t h√†i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>H√†ng b·ªã b·∫©n ƒëen l√†m ƒÉn th·∫ø m·∫•t uy t√≠n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14492</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m. R·∫•t kh√¥ng ƒë√°ng ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng t·ªët ƒë√≥ng g√≥i ƒë·∫πp ch·∫Øc ch·∫Øn tuy nhi√™...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14939</th>\n",
       "      <td>Gi√†y b√™n ph·∫£i d√¢y d√≠nh m√†u c√≤n gi√†y tr√°i g√≥c t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>. cuÃÉnq th∆°m muÃÄi h∆°i n√¥ÃÄnq...so vs giaÃÅ thiÃÄ oke</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>H√†ng nguy√™n seal ngon b·ªï r·∫ª ch·ªâ c√≥ ƒëi·ªÅu v·∫≠n ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>ƒê·∫πp h∆°n mong ƒë·ª£i. C√≥ 6‚òÜ th√¨ c≈©ng ƒë√°nh gi√° lu√¥n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>ƒêa ph·∫ßn nh·ªØng s·∫£n ph·∫©m m√¨nh mua tr∆∞·ªõc ƒë√¢y t·ª´ T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>K th·∫•y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>T·∫°m ƒë∆∞·ª£c so v·ªõi gi√° ti·ªÅn. Mua ƒëc c√°c b·∫°n ·∫°.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13296</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11143</th>\n",
       "      <td>H√†ng ok ƒë·∫πp l·∫Øm b·∫°n nhak üòç Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8932</th>\n",
       "      <td>√¥i th·∫≠t b·∫•t ng·ªù... c√≥ con n√†o mobi s·ªù ta gi√° r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8265</th>\n",
       "      <td>GiaÃÄy ƒëeÃ£p lƒÉÃÅm nha shop coÃÄn ƒë∆∞∆°Ã£c boÃâ vaÃÄo h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>l∆∞u h∆∞∆°ng kg ƒë∆∞·ª£c l√¢u l·∫Øm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "13322  M·∫´u m√£ th√¨ ok nh∆∞ng ch·∫•t l∆∞·ª£ng ko ·ªïn l·∫Øm ch·ªâ g...      1\n",
       "9448   Giao h√†ng r·∫•t l√¢u kh√¥ng gi·ªëng trog ·∫£nh l·∫Øm Th·ªù...      1\n",
       "9685   Gi·∫•y b√™n trong c√≥ k√≠ch th∆∞·ªõc kh√¥ng v·ª´a v·ªõi k√≠c...      1\n",
       "6521   Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. √Åo m·∫∑c r·∫•t m√°t ...      0\n",
       "4303   Mua c·ªßa shop l·∫ßn 2 r·ªìi v√† l·∫ßn n√†o c≈©ng r·∫•t h√†i...      0\n",
       "7700               H√†ng b·ªã b·∫©n ƒëen l√†m ƒÉn th·∫ø m·∫•t uy t√≠n      1\n",
       "14492  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m. R·∫•t kh√¥ng ƒë√°ng ti...      1\n",
       "5702   Ch·∫•t l∆∞·ª£ng t·ªët ƒë√≥ng g√≥i ƒë·∫πp ch·∫Øc ch·∫Øn tuy nhi√™...      0\n",
       "14939  Gi√†y b√™n ph·∫£i d√¢y d√≠nh m√†u c√≤n gi√†y tr√°i g√≥c t...      1\n",
       "778    . cuÃÉnq th∆°m muÃÄi h∆°i n√¥ÃÄnq...so vs giaÃÅ thiÃÄ oke      0\n",
       "242    H√†ng nguy√™n seal ngon b·ªï r·∫ª ch·ªâ c√≥ ƒëi·ªÅu v·∫≠n ch...      0\n",
       "5882   ƒê·∫πp h∆°n mong ƒë·ª£i. C√≥ 6‚òÜ th√¨ c≈©ng ƒë√°nh gi√° lu√¥n...      0\n",
       "14973  ƒêa ph·∫ßn nh·ªØng s·∫£n ph·∫©m m√¨nh mua tr∆∞·ªõc ƒë√¢y t·ª´ T...      1\n",
       "8977                                              K th·∫•y      1\n",
       "2059         T·∫°m ƒë∆∞·ª£c so v·ªõi gi√° ti·ªÅn. Mua ƒëc c√°c b·∫°n ·∫°.      0\n",
       "13296  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©...      0\n",
       "11143  H√†ng ok ƒë·∫πp l·∫Øm b·∫°n nhak üòç Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m...      0\n",
       "8932   √¥i th·∫≠t b·∫•t ng·ªù... c√≥ con n√†o mobi s·ªù ta gi√° r...      0\n",
       "8265   GiaÃÄy ƒëeÃ£p lƒÉÃÅm nha shop coÃÄn ƒë∆∞∆°Ã£c boÃâ vaÃÄo h...      0\n",
       "7541                           l∆∞u h∆∞∆°ng kg ƒë∆∞·ª£c l√¢u l·∫Øm      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','label']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb4a826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi v√† ƒëang m√¨nh ƒë·∫∑t 1 chi·∫øc n·ªØa' 0]\n",
      " ['C·∫£m th·∫•y b·ªã l·ª´a ƒë·∫£o' 1]\n",
      " ['Sach rat dep' 0]\n",
      " ['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. R·∫•t ƒë√°ng ti·ªÅn' 0]\n",
      " ['R·∫•t kh√¥ng ƒë√°ng ti·ªÅn. ƒê·∫∑t s·ªë 42 h·∫øt s·ªë ki·∫øm ƒë√¢u ƒë√¥i 40 d√°n ƒë·∫°i cai s·ªë 42 r·ªìi giao cho kh√°ch h√†ng. K bi·∫øt l√†m an ki·ªÉu j. H√†ng th√¨ k cho xem k cho ki·ªÉm tra. B·∫Øt ph·∫£i nh·∫≠n m·ªõi ƒëc m·ªü. √âp ng∆∞·ªùi ta l·∫•y h√†ng b√°n k ƒëc. ƒê·ªß tr√≤.'\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['text','label']].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377a4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. D√°n cho con s8+ xong nh√¨n s∆∞·ªõng c·∫£ m·∫Øt. Trc d√πng d√°n d·∫ªo to√†n b·ªã m·∫•t 1 t√Ω r√¨a m√†n h√¨nh']\n",
      " ['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.Tuyet v·ªùi']\n",
      " ['ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t r·∫•t ch·∫Øc ch·∫Øn. Giao h√†ng nhanh. ƒê√∫ng m√†u. C√≤n c√≥ c·∫£ phi·∫øu b·∫£o h√†nh ^^ Tuy ch∆∞a c√≥ ƒëi·ªÅu ki·ªán d√πng th·ª≠ nh∆∞ng th·∫•y ·∫•n t∆∞·ª£ng ƒë·∫ßu r·∫•t t·ªët. Ch√∫c shop b√°n ƒë·∫Øt nha. 10 ƒëi·ªÉm cho ch·∫•t l∆∞·ª£ng v√† t√°c phong b√°n h√†ng nek. Y√™u shop <3 R·∫•t ƒë√°ng ti·ªÅn']\n",
      " ['ƒê·∫πp lung linh']\n",
      " ['4sim 2 th·∫ª nh·ªõ 8 s√≥ng 12 m·∫°ng :):) ƒë√£ test th·ª≠ ukie m√†u kute ph√¥ mai qe']]\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text']].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ceee",
   "metadata": {},
   "source": [
    "# Vietnamese tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c606",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a346",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it ‚Äúcleaner‚Äù**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be97150",
   "metadata": {},
   "source": [
    "## Step 1: Pretokenization (Vietnamese word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b9e4",
   "metadata": {},
   "source": [
    "https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378087",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from underthesea import word_tokenize\n",
    ">>> sentence = 'Ch√†ng trai 9X Qu·∫£ng Tr·ªã kh·ªüi nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "\n",
    ">>> word_tokenize(sentence)\n",
    "['Ch√†ng trai', '9X', 'Qu·∫£ng Tr·ªã', 'kh·ªüi nghi·ªáp', 't·ª´', 'n·∫•m', 's√≤']\n",
    "\n",
    ">>> word_tokenize(sentence, format=\"text\")\n",
    "'Ch√†ng_trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43cefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_tokenize(sen,split_word=False):\n",
    "    # optional step: fix the whitespace between words\n",
    "    sen = \" \".join(sen.split())\n",
    "    sens = sent_tokenize(sen)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen+=word_tokenize(sen,format='text' if not split_word else None)\n",
    "    \n",
    "    if not split_word:\n",
    "        return ''.join(tokenized_sen)\n",
    "    return ['_'.join(words.split(' ')) for words in tokenized_sen]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f83210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63754082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '?', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi? Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f171898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'üòå_üòå', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c089cc",
   "metadata": {},
   "source": [
    "Apply on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32166f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [apply_word_tokenize(s[0],True) for s in train_df[['text']].values]\n",
    "train_label = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90044db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [apply_word_tokenize(s[0],True) for s in test_df[['text']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc34d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'tuy·ªát_v·ªùi',\n",
       "  '.',\n",
       "  'y',\n",
       "  'h√¨nh',\n",
       "  'ch·ª•p',\n",
       "  '.',\n",
       "  'ƒë√°ng',\n",
       "  'ti·ªÅn'],\n",
       " ['Hjhj_shop',\n",
       "  'giao',\n",
       "  'h√†ng',\n",
       "  'nhanh',\n",
       "  'qu√°',\n",
       "  '.',\n",
       "  'ƒê·∫πp',\n",
       "  'l·∫Øm',\n",
       "  '·∫°',\n",
       "  'b√©',\n",
       "  'nh√†',\n",
       "  'm',\n",
       "  'r·∫•t',\n",
       "  'th√≠ch'],\n",
       " ['nh√¨n', 'ƒë·∫πp', 'ph·∫øt', 'nh·ªâ', '..'],\n",
       " ['ƒê√≥ng_g√≥i',\n",
       "  'r·∫•t',\n",
       "  'ƒë·∫πp',\n",
       "  '.',\n",
       "  'Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'r·∫•t',\n",
       "  't·ªët',\n",
       "  'Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'tuy·ªát_v·ªùi'],\n",
       " ['SƒÉn', 'ƒëc', 'v·ªõi', 'gi√°', '11', 'k', '.', 'To·∫πt', 'v·ªùi']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2f6f",
   "metadata": {},
   "source": [
    "## Step 2: Model's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed8d0",
   "metadata": {},
   "source": [
    "To use pretrained language model such as BERT, GPT, Roberta... We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n",
    "\n",
    "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their hub. In this notebook, we use PhoBERT-base (vinai/phobert-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f842db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37e1bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_explain(inp,split_word):\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()\n",
    "\n",
    "\n",
    "def two_step_tokenization_explain(inp,split_word=False):\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Pretokenization ---')\n",
    "    tok = apply_word_tokenize(inp,split_word)\n",
    "    print(tok)\n",
    "    print()\n",
    "    tokenizer_explain(tok,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 4, 7079, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7978be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi .Ph·∫•n m·ªãn , ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 2586, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.@@', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi.Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7f2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'üòå_üòå', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 3, 1751, 3, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '<unk>', '_@@', '<unk>', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi <unk> _<unk>. Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c4f",
   "metadata": {},
   "source": [
    "# Use HuggingFace Dataset to store and tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2481f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "87c4b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text[:4],\n",
    "                        'label':train_label[:4],\n",
    "                        }\n",
    "                    )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e1572e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # pad to model's allowed max length, which is max_sequence_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8b3fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3019a96ef5049b79a9a69fedf04a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10db51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "013debff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dung', 'dc', 'sp', 'tot', 'cam', 'on_shop', 'ƒê√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi'], ['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Son', 'm·ªãn', 'nh∆∞ng', 'khi', 'ƒë√°nh', 'l√™n', 'kh√¥ng', 'nh∆∞', 'm√†u', 'tr√™n', '·∫£nh'], ['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'nh∆∞ng', 'k', 'c√≥', 'h·ªôp', 'k', 'c√≥', 'd√¢y', 'gi√†y', 'ƒëen', 'k', 'c√≥', 't·∫•t'], [':', '(', '(', 'M√¨nh', 'h∆°i', 'th·∫•t_v·ªçng', '1', 'ch√∫t', 'v√¨', 'm√¨nh', 'ƒë√£', 'k·ª≥_v·ªçng', 'cu·ªën', 's√°ch', 'kh√°', 'nhi·ªÅu', 'hi_v·ªçng', 'n√≥', 's·∫Ω', 'n√≥i', 'v·ªÅ', 'vi·ªác', 'h·ªçc_t·∫≠p', 'c·ªßa', 'c√°ch', 'sinh_vi√™n', 'tr∆∞·ªùng', 'Harvard', 'ra_sao', 'nh·ªØng', 'n·ªó_l·ª±c', 'c·ªßa', 'h·ªç', 'nh∆∞', 'th·∫ø_n√†o', '4', 'h', 's√°ng', '?', 't·∫°i_sao', 'h·ªç', 'l·∫°i', 'ph·∫£i', 'th·ª©c', 'd·∫≠y', 'v√†o', 'th·ªùi_kh·∫Øc', 'ƒë·∫•y', '?', 'sau', 'ƒë√≥', 'l√†', 'c·∫£', 'm·ªôt', 'c√¢u_chuy·ªán', 'ra_sao', '.', 'C√°i', 'm√¨nh', 'th·ª±c_s·ª±', 'c·∫ßn', '·ªü', 'ƒë√¢y', 'l√†', 'c√¢u_chuy·ªán', '·∫©n', 'd·∫•u', 'trong', 'ƒë√≥', 'ƒë·ªÉ', 't·ª±', 'b·∫£n_th√¢n', 'm·ªói', 'ng∆∞·ªùi', 'c·∫£m_nh·∫≠n', 'v√†', 'ƒëi', 's√¢u', 'v√†o', 'l√≤ng', 'ng∆∞·ªùi', 'h∆°n', '.', 'C√≤n', 'cu·ªën', 's√°ch', 'n√†y', 'ch·ªâ', 'ƒë∆°n_thu·∫ßn', 'l√†', 'cu·ªën', 's√°ch', 'd·∫°y', 'kƒ©_nƒÉng', 'm√†', 'h·∫ßu_nh∆∞', 's√°ch', 'n√†o', 'c≈©ng', 'ƒë√£', 'c√≥', '.', 'BU·ªìn', '...']]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f6c2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3556, 1236, 1894, 36150, 2225, 1204, 2947, 1672, 20811, 54922, 55662, 1685, 265, 59, 258, 6, 994, 6869, 265, 1819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 5, 16332, 5451, 51, 26, 480, 72, 17, 42, 412, 34, 284, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 51, 1947, 10, 2275, 1947, 10, 1747, 2466, 989, 1947, 10, 7328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 27, 20, 20, 3000, 1329, 2804, 99, 2013, 90, 68, 14, 2109, 1088, 713, 281, 36, 4876, 231, 38, 96, 28, 49, 1227, 7, 139, 649, 212, 9913, 57964, 3075, 21, 773, 7, 86, 42, 1279, 163, 1664, 298, 114, 2393, 86, 44, 41, 2908, 1764, 33, 9171, 1582, 114, 53, 37, 8, 94, 16, 876, 57964, 3075, 5, 2510, 68, 742, 115, 25, 97, 8, 876, 4592, 3309, 12, 37, 24, 385, 744, 205, 18, 2601, 6, 57, 808, 33, 605, 18, 48, 5, 631, 1088, 713, 23, 66, 5284, 8, 1088, 713, 940, 10685, 64, 2903, 713, 142, 32, 14, 10, 5, 924, 1878, 5460, 135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4fb0",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efa1f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea1571b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed89f689888c490c82b40e14068cfe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/518M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "10px",
    "top": "150px",
    "width": "315.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
