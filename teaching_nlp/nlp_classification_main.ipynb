{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf68ba4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-lib\" data-toc-modified-id=\"Load-lib-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load lib</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-crash-files-(do-once)\" data-toc-modified-id=\"Convert-crash-files-(do-once)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Convert crash files (do once)</a></span></li><li><span><a href=\"#Load-csv\" data-toc-modified-id=\"Load-csv-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load csv</a></span></li></ul></li><li><span><a href=\"#Vietnamese-tokenization\" data-toc-modified-id=\"Vietnamese-tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vietnamese tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Pretokenization-(Vietnamese-word-tokenization)\" data-toc-modified-id=\"Step-1:-Pretokenization-(Vietnamese-word-tokenization)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 1: Pretokenization (Vietnamese word tokenization)</a></span></li><li><span><a href=\"#Step-2:-Model's-tokenization\" data-toc-modified-id=\"Step-2:-Model's-tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 2: Model's tokenization</a></span></li></ul></li><li><span><a href=\"#Use-HuggingFace-Dataset-to-store-and-tokenize-corpus\" data-toc-modified-id=\"Use-HuggingFace-Dataset-to-store-and-tokenize-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Use HuggingFace Dataset to store and tokenize corpus</a></span></li><li><span><a href=\"#Define-dataset-dict-and-perform-train/val-split\" data-toc-modified-id=\"Define-dataset-dict-and-perform-train/val-split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define dataset dict and perform train/val split</a></span></li><li><span><a href=\"#Model-Understanding\" data-toc-modified-id=\"Model-Understanding-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Understanding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-hidden-states-for-sentence-similarity\" data-toc-modified-id=\"Extract-hidden-states-for-sentence-similarity-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Extract hidden states for sentence similarity</a></span></li></ul></li><li><span><a href=\"#Sentiment-classification-training\" data-toc-modified-id=\"Sentiment-classification-training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Sentiment classification training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-using-default-PhoBert\" data-toc-modified-id=\"Classification-using-default-PhoBert-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Classification using default PhoBert</a></span></li><li><span><a href=\"#Define-helper-function-for-training\" data-toc-modified-id=\"Define-helper-function-for-training-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Define helper function for training</a></span></li><li><span><a href=\"#Start-training\" data-toc-modified-id=\"Start-training-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Start training</a></span></li></ul></li><li><span><a href=\"#Prediction-interpretation\" data-toc-modified-id=\"Prediction-interpretation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Prediction interpretation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-report-and-confusion-matrix\" data-toc-modified-id=\"Classification-report-and-confusion-matrix-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Classification report and confusion matrix</a></span></li><li><span><a href=\"#Most-confident-prediction:-right-vs-wrong\" data-toc-modified-id=\"Most-confident-prediction:-right-vs-wrong-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Most confident prediction: right vs wrong</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dc9b6",
   "metadata": {},
   "source": [
    "# Load lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10c8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71235da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c8a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4ec21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af5dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel # body only\n",
    "#inherit this to load pretrained weight\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6d3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data_crash')\n",
    "RAW_PATH = Path('raw_crash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01542e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f326",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83962f99",
   "metadata": {},
   "source": [
    "## Convert crash files (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696ba470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if not os.path.exists(DATA_PATH):\n",
    "#     os.mkdir(DATA_PATH)\n",
    "\n",
    "# ### Cleaning training file\n",
    "\n",
    "# train = open(RAW_PATH/\"train.crash\").readlines()\n",
    "# id_locations = []\n",
    "# label_locations = []\n",
    "# for idx, line in tqdm(enumerate(train)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"train_\"):\n",
    "#         id_locations.append(idx)\n",
    "#     elif line == \"0\" or line == \"1\":\n",
    "#         label_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for id_loc, l_loc in tqdm(zip(id_locations, label_locations)):\n",
    "#     line_id = train[id_loc].strip()\n",
    "#     label = train[l_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(train[id_loc + 1: l_loc])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\\t{label}\")\n",
    "\n",
    "# with open(DATA_PATH/\"train.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\tlabel\\n\")\n",
    "#     f.write(\"\\n\".join(data))\n",
    "\n",
    "# ### Cleaning test file\n",
    "\n",
    "# test = open(RAW_PATH/\"test.crash\").readlines()\n",
    "# id_locations = []\n",
    "# for idx, line in tqdm(enumerate(test)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"test_\"):\n",
    "#         id_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for i, id_loc in tqdm(enumerate(id_locations)):\n",
    "#     if i >= len(id_locations) - 1:\n",
    "#         end = len(test)\n",
    "#     else:\n",
    "#         end = id_locations[i + 1]\n",
    "#     line_id = test[id_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(test[id_loc + 1:end])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\")\n",
    "\n",
    "# with open(DATA_PATH/\"test.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\n\")\n",
    "#     f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe363",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f367e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {str(DATA_PATH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv',sep='\\t').fillna(\"###\")\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv',sep='\\t').fillna(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5cb72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9602</th>\n",
       "      <td>San pham dung nhu tren hinh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13611</th>\n",
       "      <td>ga gi∆∞·ªùng m·ªèng nh∆∞ l√° l√∫a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13163</th>\n",
       "      <td>ƒê·∫ø gi√†y c√≤n nhi·ªÅu l·ªói üôÇ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13740</th>\n",
       "      <td>Do vchuyen len b·ªã r·ªõt b√™n ngo√†i ch√∫t ·∫° nh∆∞ng k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùiüòò ƒê√≥ng g√≥i s·∫£n ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13362</th>\n",
       "      <td>- gi√†y r·∫•t ƒë·∫πp sau n√†y s·∫Ω ·ªßng h·ªô shop ti·∫øp n·ªØa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>Mua 2 c√°i s·∫°c iphone th√¨ giao 2 c√°i s·∫°c samsun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>N√≥ t·ªá h∆°n so v·ªõi nh·ªØng g√¨ m√¨nh t∆∞·ªüng. mua n√≥ m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7944</th>\n",
       "      <td>Giao hang r√¢ÃÅt nhanh. Dong goi r√¢ÃÅt can than.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>L·∫ßn sau ·ªßng h·ªô shop n·ªØa. Ngon.ƒê√≥ng g√≥i s·∫£n ph·∫©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Th·ªùi gian giao h√†ng r·∫•t nhanh Shop ph·ª•c v·ª• r·∫•t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>ƒê·∫∑t 10 m√≥n m√† giao thi·∫øu kh√¥ng b√°o thi·∫øu kh√¥ng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>S·∫£n ph·∫©m m√πi r·∫•t th∆°m m√πi n∆∞·ªõc hoa h·ªìng r·∫•t th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>Sp k gi·ªëng h√¨nh. H√†ng giao c√≥ 1 c√°i b·ªã r√°ch ƒë·ªï...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Shop giao h√†ng kh gi·ªëng m√†u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13132</th>\n",
       "      <td>c·∫£m ∆°n shop ƒë√£ giao ƒë√∫ng m·∫´u m√¨nh r·∫•t ∆∞ng s·∫Ω ·ªß...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9029</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>san pham khac so voi hinh mau.quan din dau nho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8456</th>\n",
       "      <td>Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Gi·∫•y qu√° m·ªèng ƒë·ªçc kh√° l√† ƒëau ƒë·∫ßu. T√™n ri√™ng d·ªã...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "9602                         San pham dung nhu tren hinh      0\n",
       "13611                          ga gi∆∞·ªùng m·ªèng nh∆∞ l√° l√∫a      1\n",
       "13163                            ƒê·∫ø gi√†y c√≤n nhi·ªÅu l·ªói üôÇ      1\n",
       "13740  Do vchuyen len b·ªã r·ªõt b√™n ngo√†i ch√∫t ·∫° nh∆∞ng k...      0\n",
       "1799   Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùiüòò ƒê√≥ng g√≥i s·∫£n ph...      0\n",
       "13362     - gi√†y r·∫•t ƒë·∫πp sau n√†y s·∫Ω ·ªßng h·ªô shop ti·∫øp n·ªØa      0\n",
       "9671   Mua 2 c√°i s·∫°c iphone th√¨ giao 2 c√°i s·∫°c samsun...      1\n",
       "10343  N√≥ t·ªá h∆°n so v·ªõi nh·ªØng g√¨ m√¨nh t∆∞·ªüng. mua n√≥ m...      1\n",
       "7944       Giao hang r√¢ÃÅt nhanh. Dong goi r√¢ÃÅt can than.      0\n",
       "2296   L·∫ßn sau ·ªßng h·ªô shop n·ªØa. Ngon.ƒê√≥ng g√≥i s·∫£n ph·∫©...      0\n",
       "4997   Th·ªùi gian giao h√†ng r·∫•t nhanh Shop ph·ª•c v·ª• r·∫•t...      0\n",
       "2026   ƒê·∫∑t 10 m√≥n m√† giao thi·∫øu kh√¥ng b√°o thi·∫øu kh√¥ng...      1\n",
       "10569  S·∫£n ph·∫©m m√πi r·∫•t th∆°m m√πi n∆∞·ªõc hoa h·ªìng r·∫•t th...      0\n",
       "6667   Sp k gi·ªëng h√¨nh. H√†ng giao c√≥ 1 c√°i b·ªã r√°ch ƒë·ªï...      1\n",
       "33                           Shop giao h√†ng kh gi·ªëng m√†u      1\n",
       "13132  c·∫£m ∆°n shop ƒë√£ giao ƒë√∫ng m·∫´u m√¨nh r·∫•t ∆∞ng s·∫Ω ·ªß...      0\n",
       "9029   Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©...      0\n",
       "154    san pham khac so voi hinh mau.quan din dau nho...      1\n",
       "8456   Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n ph·∫©...      0\n",
       "220    Gi·∫•y qu√° m·ªèng ƒë·ªçc kh√° l√† ƒëau ƒë·∫ßu. T√™n ri√™ng d·ªã...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','label']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4a826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m. M√¨nh mua 2sp 1sp d√πng ƒë∆∞·ª£c 1 sp b·ªã h·ªèng'\n",
      "  1]\n",
      " ['Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi thanks shop ƒë√£ l√™n deal!' 0]\n",
      " ['Gi√†y chi·∫øc to chi·∫øc nh·ªè' 1]\n",
      " ['Qu√° x·∫•u ƒë∆∞·ªùng may qu√° th∆∞a ch·∫•t v·∫£i qu√° x·∫•u ko ph·∫£i ch·∫•t qu·∫ßn jieans'\n",
      "  1]\n",
      " ['·ªêi z·ªùi ∆°i, ngon qu√° x√° lu√¥n Ngon ngo√†i s·ª©c t∆∞·ªüng t∆∞·ª£ng Thik nh·∫•t s·∫•u v√† m∆° chua cay'\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['text','label']].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377a4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Kh√¥ng gi·ªëng ·∫£nh ·ªü nhi·ªÅu chi ti·∫øt']\n",
      " ['H√†ng y h√¨nh r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn.']\n",
      " ['d·ªÖ th∆∞∆°ng ko c√≥ j ƒë·ªÉ b√†n c√£i ch·ªâ m·ªói t·ªôi b·ªã nh·ªè em gi√†nh m·∫•t ko c√≥ x√†i TT.TT']\n",
      " ['kem kh√¥ng th·∫•m kh√≥ t√°n ƒë·ªÅu sau kho·∫£ng 1-2h l·∫°i b·∫øt v√†o n·∫øp da tr·∫Øng b·ªát nh√¨n kh√¥ng t·ª± nhi√™n ch√∫t n√†o c·∫£ kh√≥ tr√¥i k·ªÉ c·∫£ d√πng x√† ph√≤ng. ph·∫£i d√πng mu·ªëi bi·ªÉn t·∫©y da m·ªõi h·∫øt. t√°c d·ª•ng ch·ªëng n·∫Øng c·∫£m gi√°c kh√¥ng ƒë·∫°t spf 50 ƒë√¢u. n√≥i chung l√† x√†i m·ªõi c√≥ v√†i l·∫ßn l√† b·ªè lu√¥n c·∫£ type. u·ªïng ti·ªÅn!!!']\n",
      " ['R·∫•t ph√π h·ª£p gi√° ti·ªÅn. Ck m√¨nh m·∫∑c c√°i n√†o c≈©ng ƒë·∫πp. C√≥ 6sao th√¨ cho 6 sao lu√¥n.']]\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text']].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ceee",
   "metadata": {},
   "source": [
    "# Vietnamese tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c606",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a346",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it ‚Äúcleaner‚Äù**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be97150",
   "metadata": {},
   "source": [
    "## Step 1: Pretokenization (Vietnamese word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b9e4",
   "metadata": {},
   "source": [
    "https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378087",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from underthesea import word_tokenize\n",
    ">>> sentence = 'Ch√†ng trai 9X Qu·∫£ng Tr·ªã kh·ªüi nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "\n",
    ">>> word_tokenize(sentence)\n",
    "['Ch√†ng trai', '9X', 'Qu·∫£ng Tr·ªã', 'kh·ªüi nghi·ªáp', 't·ª´', 'n·∫•m', 's√≤']\n",
    "\n",
    ">>> word_tokenize(sentence, format=\"text\")\n",
    "'Ch√†ng_trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43cefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_tokenize(sen,split_word=False):\n",
    "    # optional step: fix the whitespace between words\n",
    "    sen = \" \".join(sen.split())\n",
    "    sens = sent_tokenize(sen)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen+=word_tokenize(sen,format='text' if not split_word else None)\n",
    "    \n",
    "    if not split_word:\n",
    "        return ''.join(tokenized_sen)\n",
    "    return ['_'.join(words.split(' ')) for words in tokenized_sen]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63754082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '?', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi? Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f171898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'üòå_üòå', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c089cc",
   "metadata": {},
   "source": [
    "Apply on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32166f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [apply_word_tokenize(s[0],True) for s in train_df[['text']].values]\n",
    "train_label = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90044db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [apply_word_tokenize(s[0],True) for s in test_df[['text']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc34d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'tuy·ªát_v·ªùi',\n",
       "  '.',\n",
       "  'y',\n",
       "  'h√¨nh',\n",
       "  'ch·ª•p',\n",
       "  '.',\n",
       "  'ƒë√°ng',\n",
       "  'ti·ªÅn'],\n",
       " ['Hjhj_shop',\n",
       "  'giao',\n",
       "  'h√†ng',\n",
       "  'nhanh',\n",
       "  'qu√°',\n",
       "  '.',\n",
       "  'ƒê·∫πp',\n",
       "  'l·∫Øm',\n",
       "  '·∫°',\n",
       "  'b√©',\n",
       "  'nh√†',\n",
       "  'm',\n",
       "  'r·∫•t',\n",
       "  'th√≠ch'],\n",
       " ['nh√¨n', 'ƒë·∫πp', 'ph·∫øt', 'nh·ªâ', '..'],\n",
       " ['ƒê√≥ng_g√≥i',\n",
       "  'r·∫•t',\n",
       "  'ƒë·∫πp',\n",
       "  '.',\n",
       "  'Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'r·∫•t',\n",
       "  't·ªët',\n",
       "  'Ch·∫•t_l∆∞·ª£ng',\n",
       "  's·∫£n_ph·∫©m',\n",
       "  'tuy·ªát_v·ªùi'],\n",
       " ['SƒÉn', 'ƒëc', 'v·ªõi', 'gi√°', '11', 'k', '.', 'To·∫πt', 'v·ªùi']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2f6f",
   "metadata": {},
   "source": [
    "## Step 2: Model's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed8d0",
   "metadata": {},
   "source": [
    "To use pretrained language model such as BERT, GPT, Roberta... We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n",
    "\n",
    "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their hub. In this notebook, we use PhoBERT-base (vinai/phobert-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f842db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e1bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_explain(inp,split_word):\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()\n",
    "\n",
    "\n",
    "def two_step_tokenization_explain(inp,split_word=False):\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Pretokenization ---')\n",
    "    tok = apply_word_tokenize(inp,split_word)\n",
    "    print(tok)\n",
    "    print()\n",
    "    tokenizer_explain(tok,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 4, 7079, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', ',', 'ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi, ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7978be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi .Ph·∫•n m·ªãn , ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 2586, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.@@', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi.Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7f2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'üòå_üòå', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 3, 1751, 3, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '<unk>', '_@@', '<unk>', '.', 'Ph·∫•n', 'm·ªãn', ',', 'ƒë√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi <unk> _<unk>. Ph·∫•n m·ªãn, ƒë√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üòåüòå. Ph·∫•n m·ªãn, ƒë√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn'\n",
    "two_step_tokenization_explain(_tmp,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c4f",
   "metadata": {},
   "source": [
    "# Use HuggingFace Dataset to store and tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2481f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f388127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # pad to model's allowed max length, which is max_sequence_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c4b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text[:4],\n",
    "                        'label':train_label[:4],\n",
    "                        }\n",
    "                    )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1572e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8b3fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e055d34efa4d8ca9456242a6bef98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10db51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013debff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dung', 'dc', 'sp', 'tot', 'cam', 'on_shop', 'ƒê√≥ng_g√≥i', 's·∫£n_ph·∫©m', 'r·∫•t', 'ƒë·∫πp', 'v√†', 'ch·∫Øc_ch·∫Øn', 'Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi'], ['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', '.', 'Son', 'm·ªãn', 'nh∆∞ng', 'khi', 'ƒë√°nh', 'l√™n', 'kh√¥ng', 'nh∆∞', 'm√†u', 'tr√™n', '·∫£nh'], ['Ch·∫•t_l∆∞·ª£ng', 's·∫£n_ph·∫©m', 'tuy·ªát_v·ªùi', 'nh∆∞ng', 'k', 'c√≥', 'h·ªôp', 'k', 'c√≥', 'd√¢y', 'gi√†y', 'ƒëen', 'k', 'c√≥', 't·∫•t'], [':', '(', '(', 'M√¨nh', 'h∆°i', 'th·∫•t_v·ªçng', '1', 'ch√∫t', 'v√¨', 'm√¨nh', 'ƒë√£', 'k·ª≥_v·ªçng', 'cu·ªën', 's√°ch', 'kh√°', 'nhi·ªÅu', 'hi_v·ªçng', 'n√≥', 's·∫Ω', 'n√≥i', 'v·ªÅ', 'vi·ªác', 'h·ªçc_t·∫≠p', 'c·ªßa', 'c√°ch', 'sinh_vi√™n', 'tr∆∞·ªùng', 'Harvard', 'ra_sao', 'nh·ªØng', 'n·ªó_l·ª±c', 'c·ªßa', 'h·ªç', 'nh∆∞', 'th·∫ø_n√†o', '4', 'h', 's√°ng', '?', 't·∫°i_sao', 'h·ªç', 'l·∫°i', 'ph·∫£i', 'th·ª©c', 'd·∫≠y', 'v√†o', 'th·ªùi_kh·∫Øc', 'ƒë·∫•y', '?', 'sau', 'ƒë√≥', 'l√†', 'c·∫£', 'm·ªôt', 'c√¢u_chuy·ªán', 'ra_sao', '.', 'C√°i', 'm√¨nh', 'th·ª±c_s·ª±', 'c·∫ßn', '·ªü', 'ƒë√¢y', 'l√†', 'c√¢u_chuy·ªán', '·∫©n', 'd·∫•u', 'trong', 'ƒë√≥', 'ƒë·ªÉ', 't·ª±', 'b·∫£n_th√¢n', 'm·ªói', 'ng∆∞·ªùi', 'c·∫£m_nh·∫≠n', 'v√†', 'ƒëi', 's√¢u', 'v√†o', 'l√≤ng', 'ng∆∞·ªùi', 'h∆°n', '.', 'C√≤n', 'cu·ªën', 's√°ch', 'n√†y', 'ch·ªâ', 'ƒë∆°n_thu·∫ßn', 'l√†', 'cu·ªën', 's√°ch', 'd·∫°y', 'kƒ©_nƒÉng', 'm√†', 'h·∫ßu_nh∆∞', 's√°ch', 'n√†o', 'c≈©ng', 'ƒë√£', 'c√≥', '.', 'BU·ªìn', '...']]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f6c2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3556, 1236, 1894, 36150, 2225, 1204, 2947, 1672, 20811, 54922, 55662, 1685, 265, 59, 258, 6, 994, 6869, 265, 1819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 5, 16332, 5451, 51, 26, 480, 72, 17, 42, 412, 34, 284, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 51, 1947, 10, 2275, 1947, 10, 1747, 2466, 989, 1947, 10, 7328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 27, 20, 20, 3000, 1329, 2804, 99, 2013, 90, 68, 14, 2109, 1088, 713, 281, 36, 4876, 231, 38, 96, 28, 49, 1227, 7, 139, 649, 212, 9913, 57964, 3075, 21, 773, 7, 86, 42, 1279, 163, 1664, 298, 114, 2393, 86, 44, 41, 2908, 1764, 33, 9171, 1582, 114, 53, 37, 8, 94, 16, 876, 57964, 3075, 5, 2510, 68, 742, 115, 25, 97, 8, 876, 4592, 3309, 12, 37, 24, 385, 744, 205, 18, 2601, 6, 57, 808, 33, 605, 18, 48, 5, 631, 1088, 713, 23, 66, 5284, 8, 1088, 713, 940, 10685, 64, 2903, 713, 142, 32, 14, 10, 5, 924, 1878, 5460, 135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1e74a",
   "metadata": {},
   "source": [
    "# Define dataset dict and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1925dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1215ad4a192441790921b1574c3d5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cef9a2ae974bc2a8414156499e9625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text,\n",
    "                        'label':train_label,\n",
    "                        }\n",
    "                    )\n",
    "test_dataset = Dataset.from_dict(\n",
    "                        {'text': test_text,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True)\n",
    "test_dataset_tokenized= test_dataset.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d637faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "train_dataset_tokenized = train_dataset_tokenized.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2184388",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized = DatasetDict()\n",
    "main_dataset_tokenized['train'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8)))\n",
    "main_dataset_tokenized['validation'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8),train_dataset_tokenized.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed94e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3218\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ebcace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.580154\n",
       "1    0.419846\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['train']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d049de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.563704\n",
       "1    0.436296\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['validation']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e7953",
   "metadata": {},
   "source": [
    "# Model Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224bed4",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "646a3340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ad9f6",
   "metadata": {},
   "source": [
    "## Extract hidden states for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18f42a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch,model=None):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).hidden_states[-1]\n",
    "\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "def extract_hidden_states_concat(batch,model=None):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last 4 hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(**inputs).hidden_states[-4:]\n",
    "        last_hidden_states = [hs[:,0] for hs in last_hidden_states]\n",
    "        concat_hidden_states = torch.cat(last_hidden_states,1)\n",
    "\n",
    "    return {\"hidden_state\": concat_hidden_states.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "333a3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def raw_model_init():\n",
    "    model = AutoModel.from_pretrained(\"vinai/phobert-base\",output_hidden_states=True,output_attentions=True)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "raw_model = raw_model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfa93af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d20ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "train_sample = train_dataset_tokenized.shuffle().select(range(int(train_dataset_tokenized.num_rows*1)))\n",
    "sample_dataset_tokenized = DatasetDict()\n",
    "trn_sz = 1\n",
    "sample_dataset_tokenized['train'] = train_sample.select(range(int(train_sample.num_rows*trn_sz)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48cb6c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1178ceff9dd64753913809e2fe2bafdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8044 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "sample_dataset_tokenized['train'] = sample_dataset_tokenized['train'].map(partial(extract_hidden_states_concat,model=raw_model), batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ec66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c0b0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def concat_str(inp):\n",
    "    return ' '.join(inp)\n",
    "def show_neighbors(neigh,idx,data=data,metadata=sample_dataset_tokenized['train'],n_neighbors=5):\n",
    "    print(f\"Sentence: {concat_str(metadata['text'][idx])}\\nLabel: {metadata['label'][idx]}\")\n",
    "    distances,nbors = neigh.kneighbors([data[idx]])\n",
    "\n",
    "    print('\\nNeighbors: ')\n",
    "    for d,n_idx in zip(distances[0],nbors[0]):\n",
    "        print(f\"\\t{concat_str(metadata['text'][n_idx])}, d={d:.3f}, label={metadata['label'][n_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06760021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sample_dataset_tokenized['train']['hidden_state'].cpu().numpy()\n",
    "neigh = NearestNeighbors(n_neighbors=4)\n",
    "neigh.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53482b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ƒê·∫∑t 1 h·ªôp grow plus + 4 chai pediasure th√¨ c√≥ 1_Chai ƒë√£ b·ªã khui ra v√† d√°n n·∫Øp l·∫°i b·∫±ng keo d√°n s·∫Øt . T√¥i l·∫•y ƒë·ªì c·ªë_g·∫Øng m·ªü n·∫Øp ra th√¨ th·∫•y m√†u_n∆∞·ªõc ƒë·ª•c v√† m√πi h√¥i_th·ªëi b·ªëc l√™n n·ªìng_n·∫∑c . B√°n h√†ng sao ko c√≥ l∆∞∆°ng_t√¢m v·∫≠y b·∫°n ? ƒê√¢y l√† s·ªØa d√†nh cho em b√© u·ªëng m√† b·∫°n b√°n h√†ng th·∫•t_ƒë·ª©c th·∫ø .\n",
      "Label: 1\n",
      "\n",
      "Neighbors: \n",
      "\tƒê·∫∑t 1 h·ªôp grow plus + 4 chai pediasure th√¨ c√≥ 1_Chai ƒë√£ b·ªã khui ra v√† d√°n n·∫Øp l·∫°i b·∫±ng keo d√°n s·∫Øt . T√¥i l·∫•y ƒë·ªì c·ªë_g·∫Øng m·ªü n·∫Øp ra th√¨ th·∫•y m√†u_n∆∞·ªõc ƒë·ª•c v√† m√πi h√¥i_th·ªëi b·ªëc l√™n n·ªìng_n·∫∑c . B√°n h√†ng sao ko c√≥ l∆∞∆°ng_t√¢m v·∫≠y b·∫°n ? ƒê√¢y l√† s·ªØa d√†nh cho em b√© u·ªëng m√† b·∫°n b√°n h√†ng th·∫•t_ƒë·ª©c th·∫ø ., d=0.000, label=1\n",
      "\tƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t k√©m . G√≥i b·ªâm b·∫©n_th·ªâu kinh_kh·ªßng ƒë√£ r√°ch v√¨ b·ªã c·∫Øt tem n√™n mi·∫øng b·ªâm b√™n trong c≈©ng d√≠nh b·ª•i b·∫©n lu√¥n . M√¨nh ƒë√£ v·ª©t ƒëi coi nh∆∞ v·ª©t ti·ªÅn v√†o s·ªçt_r√°c v√¨ ch·∫£ bi·∫øt li·ªáu ƒë√£ c√≥ nh·ªØng con g√¨ chui v√†o trong g√≥i b·ªâm r·ªìi . Ch∆∞a k·ªÉ giao h√†ng ch·∫≠m . Qu√° th·∫•t_v·ªçng ! ! !, d=5.800, label=1\n",
      "\tS·∫£n_ph·∫©m b·ªã h∆∞ m√≥c kho√° . M√¨nh c√≥ li√™n_h·ªá b√™n shop ƒë·ªÉ ƒë·ªïi h√†ng nh∆∞ng shop ko ph·∫£n_h·ªìi . Qu√° th·∫•t_v·ªçng . L√†m ƒÉn ki·ªÉu n√†y s·∫Ω ko c√≤n b·∫°n n√†o ƒë·∫∑t_h√†ng onl c·ªßa shop ƒë√¢u, d=6.255, label=1\n",
      "\tƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn , Shop ph·ª•c_v·ª• r·∫•t t·ªët , Th·ªùi_gian giao h√†ng r·∫•t nhanh . M√°y_in r·∫•t ok nh√© . L·∫ßn ƒë·∫ßu m√¨nh s·ª≠_d·ª•ng loay_hoay m√£i m·ªõi in ƒë∆∞·ª£c nh∆∞ng ch·∫•t_l∆∞·ª£ng ·∫£nh s√°ng r√µ ƒë·∫πp . Nh∆∞ng k bi·∫øt d√πng c√≥ b·ªÅn k ., d=6.366, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,1,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18448d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ƒê√°ng ƒë·ªìng x√®ng . G√≥i c·∫©n_th·∫≠n . L·∫°i l·ªãch_s·ª± hihi , ƒë∆∞·ª£c t·∫∑ng 1 ƒë√¥i t·∫•t korea si√™u xinhh . 5 saooo . Th·ªùi_gian giao h√†ng r·∫•t nhanh R·∫•t ƒë√°ng ti·ªÅn Shop ph·ª•c_v·ª• r·∫•t t·ªët ƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tƒê√°ng ƒë·ªìng x√®ng . G√≥i c·∫©n_th·∫≠n . L·∫°i l·ªãch_s·ª± hihi , ƒë∆∞·ª£c t·∫∑ng 1 ƒë√¥i t·∫•t korea si√™u xinhh . 5 saooo . Th·ªùi_gian giao h√†ng r·∫•t nhanh R·∫•t ƒë√°ng ti·ªÅn Shop ph·ª•c_v·ª• r·∫•t t·ªët ƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi, d=0.000, label=0\n",
      "\tCh·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn Shop ph·ª•c_v·ª• r·∫•t t·ªët R·∫•t ƒë√°ng ti·ªÅn Th·ªùi_gian giao h√†ng r·∫•t nhanh . V·ª´a nh∆∞ in ƒë·∫πp h·∫øt n√≥i lu√¥n . Gi√° r·∫ª h∆°n c√°c shop kh√°c n·ªØa l·∫°i ƒëc t·∫∑ng 4 ƒë√¥i t·∫•t . H·∫øt_√Ω, d=6.340, label=0\n",
      "\tMua nhi·ªÅu l·∫ßn r·ªìi . D√πng Ok l·∫Øm ∆∞ng h∆°n c√°c lo·∫°i b·ªâm kh√°c . Ph√π_h·ª£p gi√° ti·ªÅn . ·ª¶ng_h·ªô shop d√†i_d√†i nh∆∞ng c√°i anh b√°n nh√© . B·∫£o kh√°ch_h√†ng ru·ªôt t·∫∑ng th√™m 1 c√°i b·ªâm h·ª©a l√™n h·ª©a xu·ªëng ch·∫£ th·∫•y t·∫∑ng k√®m j . Ch·∫£_l·∫Ω cho 4 sao ._üòÅ, d=6.591, label=0\n",
      "\tƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ch·∫Øc_ch·∫Øn . Th·ªùi_gian giao h√†ng r·∫•t nhanh . C·ª±c match v·ªõi m√†u_da c·ªßa m√¨nh . C·∫£m_∆°n shop nhi·ªÅu, d=6.650, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,2,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "907e822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: K√©m ch·∫•t_l∆∞·ª£ng . h√†ng gi·∫£ h√†ng nh√°i nh√©\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tK√©m ch·∫•t_l∆∞·ª£ng . h√†ng gi·∫£ h√†ng nh√°i nh√©, d=0.000, label=0\n",
      "\tH√†ng nh√°i th∆∞∆°ng_hi·ªáu . ch·∫•t_l∆∞·ª£ng v·∫•t ƒëi . b√°n l·ª´a h√†ng ch√≠nh h√£ng, d=6.982, label=1\n",
      "\th√†ng t·ªá . loa k√©m ch·∫•t_l∆∞·ª£ng, d=7.075, label=1\n",
      "\tShop l√†m_ƒÉn gian_d·ªëi giao thi·∫øu h√†ng c√≤n kh√¥ng c√≥ tr√°ch_nhi·ªám . Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m r·∫•t k√©m Shop ph·ª•c_v·ª• r·∫•t k√©m . M√¨nh khuy√™n kh√¥ng n√™n mua h√†ng ·ªü shop n√†y nh√©, d=7.156, label=1\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,5,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed53dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ƒê√£ nh·∫≠n ƒë∆∞·ª£c h√†ng ∆∞ng l·∫Øm nh√© . Giao h√†ng r·∫•t nhanh . C·∫£m_∆°n shop .\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tƒê√£ nh·∫≠n ƒë∆∞·ª£c h√†ng ∆∞ng l·∫Øm nh√© . Giao h√†ng r·∫•t nhanh . C·∫£m_∆°n shop ., d=0.000, label=0\n",
      "\tM√¨nh r·∫•t th√≠ch . C·∫£m_∆°n shop nh√© . Ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi . ƒê√≥ng_g√≥i s·∫£n_ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc_ch·∫Øn ., d=4.275, label=0\n",
      "\tShop ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp v√† c·∫©n_th·∫≠n . H√†ng chu·∫©n . Th·ªùi_gian giao h√†ng nhanh . L·∫°i ƒë∆∞·ª£c th√™m m√≥n qu√† nh·ªè t·ª´ shop n·ªØa . C·∫£m_∆°n shop nh√© ., d=4.295, label=0\n",
      "\tShop giao h√†ng r·∫•t nhanh . C·∫£m_∆°n shop . S·∫Ω ti·∫øp_t·ª•c ·ªßng_h·ªô shop ., d=4.443, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,14,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed927473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ƒê·∫∑t d√¢u_t√¢y m√† giao l·ªôn d∆∞a_h·∫•u : ( ( (\n",
      "Label: 1\n",
      "\n",
      "Neighbors: \n",
      "\tƒê·∫∑t d√¢u_t√¢y m√† giao l·ªôn d∆∞a_h·∫•u : ( ( (, d=0.000, label=1\n",
      "\tChi·∫øc cao chi·∫øc th·∫•p : ( ( ( (, d=4.880, label=1\n",
      "\tHuhu m·∫•t c√°i m·∫∑t t√°o r·ªìi : ( ( (, d=4.978, label=0\n",
      "\tS·∫£n_ph·∫©m kg gi·ªëng nh∆∞ h√¨nh v√† clip : ( ( (, d=5.256, label=1\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,15,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75311205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: C·∫•u_h√¨nh t·ªët , ƒë·∫πp , ti·ªán\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tC·∫•u_h√¨nh t·ªët , ƒë·∫πp , ti·ªán, d=0.000, label=0\n",
      "\tH√†ng ƒë·∫πp - t·ªët, d=7.983, label=0\n",
      "\tT·ªët gi√° ·ªïn, d=8.120, label=0\n",
      "\tKh√° l√† xinh, d=8.220, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,16,data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4fb0",
   "metadata": {},
   "source": [
    "# Sentiment classification training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40541781",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089eb60",
   "metadata": {},
   "source": [
    "## Classification using default PhoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d414ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "# from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "# from transformers.models.roberta.modeling_roberta import RobertaModel \n",
    "# from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f89e470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "522bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_init(get_hidden=False):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2,output_hidden_states=get_hidden)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780da385",
   "metadata": {},
   "source": [
    "## Define helper function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77905e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gc\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # pred: EvalPrediction object \n",
    "    # (which is a named tuple with predictions and label_ids attributes)\n",
    "    labels = pred.label_ids\n",
    "    if isinstance(pred.predictions,tuple):\n",
    "        preds = pred.predictions[0].argmax(-1)\n",
    "    else:\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1,\"accuracy\": acc}\n",
    "\n",
    "\n",
    "\n",
    "def finetune(lr,bs,wd,epochs,ddict,tokenizer=tokenizer,o_dir = './outputs',logging=False,model_init=base_model_init):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not logging:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    else:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                               logging_dir=os.path.join(o_dir, 'log'),\n",
    "                                logging_steps = len(ddict[\"train\"]) // bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd)\n",
    "\n",
    "    # instantiate trainer\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=ddict['train'],#.shard(200, 0),    # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "        eval_dataset=ddict['validation'],#.shard(100, 0), # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "#         data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a25275",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9cdad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12869\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 09:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288897</td>\n",
       "      <td>0.907470</td>\n",
       "      <td>0.908950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.298636</td>\n",
       "      <td>0.897811</td>\n",
       "      <td>0.899627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.306182</td>\n",
       "      <td>0.909048</td>\n",
       "      <td>0.910193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-403\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-403/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-403/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-403/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-806\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-806/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-806/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-806/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-1209\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-1209/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-1209/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-1209/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=32\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "o_dir = './default_phobert_finetuned_1'\n",
    "tmp = finetune(lr,bs,wd,epochs,ddict=main_dataset_tokenized,o_dir = o_dir)\n",
    "\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1\tAccuracy\n",
    "# 1\tNo log\t0.288897\t0.907470\t0.908950\n",
    "# 2\t0.298100\t0.298636\t0.897811\t0.899627\n",
    "# 3\t0.180200\t0.306182\t0.909048\t0.910193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11fc7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1209  checkpoint-403  checkpoint-537  checkpoint-806\r\n"
     ]
    }
   ],
   "source": [
    "!ls default_phobert_finetuned_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7655e9",
   "metadata": {},
   "source": [
    "# Prediction interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57401c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model_init()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.from_pretrained('./default_phobert_finetuned_1/checkpoint-806')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d78ff2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2str={0:\"Positive\",1:'Negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f951b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def label_int2str(row):\n",
    "    return idx2str[row]\n",
    "\n",
    "def forward_pass_with_label(batch,model=None):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy(),\n",
    "           'predicted_probability': torch.nn.functional.softmax(output.logits.cpu(),dim=1).numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac5b647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "main_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d9bb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ef1fec038413fb0e6afd0c1d31beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/805 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b3e8a8cb64f6cb896676357428607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute loss values\n",
    "main_dataset_tokenized[\"train\"] = main_dataset_tokenized[\"train\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)\n",
    "main_dataset_tokenized[\"validation\"] = main_dataset_tokenized[\"validation\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98453d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", 'predicted_probability',\"loss\"]\n",
    "df_val = main_dataset_tokenized[\"validation\"][:][cols]\n",
    "df_val[\"label_str\"] = df_val[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_val[\"predicted_label_str\"] = (df_val[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4047fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = main_dataset_tokenized[\"train\"][:][cols]\n",
    "df_trn[\"label_str\"] = df_trn[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_trn[\"predicted_label_str\"] = (df_trn[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31a12ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12869, 7), (3218, 7))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.shape,df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a301ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Th√¥ng_tin, chi_ti·∫øt, tr√™n, web, l√†, s·∫£n_xu·∫•t,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0779537, 0.92204636]</td>\n",
       "      <td>0.081160</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ƒê·ªìng_h·ªì, d√¢y_ƒëeo, ·ªçp_·∫πp, ., T·∫•t_nilon, r·∫ª_ti·ªÅ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04499709, 0.9550029]</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Shop, de, th∆∞∆°ng, nhiet, t√¨nh, s·∫Ω, ·ªßng, h√¥, l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99736834, 0.0026315863]</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nh√¨n, k, ch·∫Øc_ch·∫Øn, l·∫Øm]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5285818, 0.47141826]</td>\n",
       "      <td>0.637558</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ch·∫•t_l∆∞·ª£ng, s·∫£n_ph·∫©m, r·∫•t, k√©m, ., R√°ch, l·ªói,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.022633858, 0.9773662]</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [Th√¥ng_tin, chi_ti·∫øt, tr√™n, web, l√†, s·∫£n_xu·∫•t,...      1                1   \n",
       "1  [ƒê·ªìng_h·ªì, d√¢y_ƒëeo, ·ªçp_·∫πp, ., T·∫•t_nilon, r·∫ª_ti·ªÅ...      1                1   \n",
       "2  [Shop, de, th∆∞∆°ng, nhiet, t√¨nh, s·∫Ω, ·ªßng, h√¥, l...      0                0   \n",
       "3                          [Nh√¨n, k, ch·∫Øc_ch·∫Øn, l·∫Øm]      0                0   \n",
       "4  [Ch·∫•t_l∆∞·ª£ng, s·∫£n_ph·∫©m, r·∫•t, k√©m, ., R√°ch, l·ªói,...      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0     [0.0779537, 0.92204636]  0.081160  Negative            Negative  \n",
       "1     [0.04499709, 0.9550029]  0.046041  Negative            Negative  \n",
       "2  [0.99736834, 0.0026315863]  0.002635  Positive            Positive  \n",
       "3     [0.5285818, 0.47141826]  0.637558  Positive            Positive  \n",
       "4    [0.022633858, 0.9773662]  0.022894  Negative            Negative  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc7b897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C·∫ßm, kh√°, ch·∫Øc_tay, ., Tr·ª•c, u·ªën, to, ƒë√∫ng, q...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.997373, 0.0026270347]</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3G, c√≤n, t√πy, l√∫c_n√†o, v√†, khi, n√†o, v√†, ·ªü, ƒë...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99603903, 0.0039609983]</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ƒê√≥ng_g√≥i, s·∫£n_ph·∫©m, r·∫•t, ch·∫Øc_ch·∫Øn, ., Th·ªùi_g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99715614, 0.0028438682]</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[m√¨nh, x√†i, em, n√†y, b·ªã, ƒë·ª©ng, m√°y, ho√†i, √†]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.13099755, 0.8690025]</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ch·∫øt, $]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.09323443, 0.9067656]</td>\n",
       "      <td>0.097871</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [C·∫ßm, kh√°, ch·∫Øc_tay, ., Tr·ª•c, u·ªën, to, ƒë√∫ng, q...      0                0   \n",
       "1  [3G, c√≤n, t√πy, l√∫c_n√†o, v√†, khi, n√†o, v√†, ·ªü, ƒë...      0                0   \n",
       "2  [ƒê√≥ng_g√≥i, s·∫£n_ph·∫©m, r·∫•t, ch·∫Øc_ch·∫Øn, ., Th·ªùi_g...      0                0   \n",
       "3       [m√¨nh, x√†i, em, n√†y, b·ªã, ƒë·ª©ng, m√°y, ho√†i, √†]      1                1   \n",
       "4                                          [Ch·∫øt, $]      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0    [0.997373, 0.0026270347]  0.002630  Positive            Positive  \n",
       "1  [0.99603903, 0.0039609983]  0.003969  Positive            Positive  \n",
       "2  [0.99715614, 0.0028438682]  0.002848  Positive            Positive  \n",
       "3     [0.13099755, 0.8690025]  0.140409  Negative            Negative  \n",
       "4     [0.09323443, 0.9067656]  0.097871  Negative            Negative  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a29fd",
   "metadata": {},
   "source": [
    "## Classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8f10912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.96      0.96      0.96      7466\n",
      "    Negative       0.94      0.95      0.94      5403\n",
      "\n",
      "    accuracy                           0.95     12869\n",
      "   macro avg       0.95      0.95      0.95     12869\n",
      "weighted avg       0.95      0.95      0.95     12869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_trn.label.values,df_trn.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0670fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.91      0.92      0.91      1814\n",
      "    Negative       0.89      0.88      0.88      1404\n",
      "\n",
      "    accuracy                           0.90      3218\n",
      "   macro avg       0.90      0.90      0.90      3218\n",
      "weighted avg       0.90      0.90      0.90      3218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_val.label.values,df_val.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d826ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb4ac47fdf0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,10))\n",
    "ConfusionMatrixDisplay.from_predictions(df_val.label.values,df_val.predicted_label.values,\n",
    "                                        display_labels=['Positive','Negative'],xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4098",
   "metadata": {},
   "source": [
    "## Most confident prediction: right vs wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c842659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prob_match(row):\n",
    "    l = row['label']\n",
    "    prob = row['predicted_probability']\n",
    "    return prob[l]\n",
    "def _get_prob_mismatch(row):\n",
    "    prob = row['predicted_probability']\n",
    "    return np.max(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d1e4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['correct_confidence'] = df_val.loc[df_val.label==df_val.predicted_label].apply(_get_prob_match,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e765bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['incorrect_confidence'] = df_val.loc[df_val.label!=df_val.predicted_label].apply(_get_prob_mismatch,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f68965e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correct_pred(df,cols=['text','label_str','correct_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('correct_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(tmp[1:])\n",
    "        print('-'*100)\n",
    "def analyze_incorrect_pred(df,cols=['text','label_str','predicted_label_str','incorrect_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('incorrect_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(f'True label: {tmp[1]}, but predict {tmp[2]}, with confidence {tmp[3]}')\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45aa524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mua | ao | mua | bo_shop | giao | ao | mua | doi | LAM | sao | DANH_Gi√° | the | nao | day\n",
      "['Negative' 0.5087628960609436]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "B√¥ng | m·ªÅm | h∆°n | nh∆∞ng | c≈©ng | m·ªèng | h∆°n | ƒë·ª£t | tr∆∞·ªõc | d√πng\n",
      "['Positive' 0.5093692541122437]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ghi | date | T2 | / | 2019 | n√™n | m·ªõi | ƒë·∫∑t_h√†ng | ƒë·∫øn | 5 | g√≥i | nh·∫≠n | h√†ng | th√¨ | T1 | / | 2019 | th·ª±c_ph·∫©m | cho | b√© | n√™n | b√°n | ƒë√∫ng_h·∫°n | s·ª≠_d·ª•ng | gi·ªõi_thi·ªáu | ƒë·ªÉ | kh√°ch | tin_t∆∞·ªüng | & | quy·∫øt_ƒë·ªãnh | mua | h√†ng\n",
      "['Negative' 0.5200026035308838]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pin | t·ª•t | nhanh | nh∆∞ng | kh√¥ng | b·ªã | s·∫≠p | ngu·ªìn\n",
      "['Positive' 0.5242894887924194]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "M·ªü_ra | c√≥ | m·ªói | v·ªè | ko | th·∫•y | k√≠nh | c∆∞·ªùng_l·ª±c | ·ªü | trong\n",
      "['Negative' 0.5256175994873047]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh√¨n | k | ch·∫Øc_ch·∫Øn | l·∫Øm\n",
      "['Positive' 0.5285817980766296]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "V·ª´a | m·ªõi | mua | s√°ng | nay | th√¨ | 333.000 | VND | gi·ªù | v√†o | nh√¨n | l·∫°i | th·∫•y | 189.000 | VND | ! | ! | ! | !\n",
      "['Negative' 0.5287574529647827]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | . | Tuy_nhi√™n | giao | thi·∫øu | s·ªë_l∆∞·ª£ng | . | M√¨nh | ƒë·∫∑t | 2 | thanh_to√°n | 2 | nh·∫≠n | ƒë∆∞·ª£c | ch·ªâ | c√≥ | 1 | c√°i\n",
      "['Positive' 0.5362181067466736]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H√†ng | d√πng | t·∫°m | ƒë∆∞·ª£c | tuy_nhi√™n | h∆°i | l·ªèng_l·∫ªo | . | K | ch·∫Øc_ch·∫Øn | l·∫Øm | .\n",
      "['Positive' 0.538158655166626]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Giao | h√†ng | l√¢u | ∆°i | l√† | l√¢u | ch·∫Øc | do | tr·∫£ | ti·ªÅn | tr∆∞·ªõc | : | (\n",
      "['Positive' 0.5443349480628967]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | giao | h√†ng | kh | gi·ªëng | m√†u\n",
      "['Negative' 0.5501789450645447]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dao | h√†ng | l√¢u | ƒë·∫∑t | h·∫≥n | 2 | l·∫ßn | m·ªõi | dao | g·∫ßn | th√°ng | m·ªõi | c√≥ | h√†ng | ....\n",
      "['Negative' 0.5585803389549255]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "S·∫£n_ph·∫©m | ok | t·ªëc_ƒë·ªô | kh√¥ng | dc | nh∆∞ | mong_ƒë·ª£i\n",
      "['Positive' 0.5587018132209778]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "B·ªôt | m·ªãn | h·ª≠i | th√¨ | c√≥ | m√πi | th∆°m | nh∆∞ng | khi | d√πng | th√¨ | kh√¥ng | ƒë·∫≠m | m√πi | ƒë·∫≠u_n√†nh | bao_b√¨ | in | m·ªù_nh·∫°t\n",
      "['Negative' 0.5603232979774475]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng | g√≥ii | ƒëu·ªçc\n",
      "['Negative' 0.5685207843780518]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "V·∫£i | kh√° | m·ªèng | . | Ti·ªÅn | n√†o | c·ªßa | n·∫•y | . | N√≥i_chung | h∆°n | gi√†y_bata | m·ªôt | t√Ω | .\n",
      "['Negative' 0.5702218413352966]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tr∆∞·ªõc_m·∫Øt | th√¨ | th·∫•y | oke | r·ªìi | ƒë√≥ | c·ª•c | n√†y | ch·ªçi | tr√¢u | c≈©ng | ch·∫øt | üòÇ_üòÇ_üòÇ\n",
      "['Positive' 0.5918132066726685]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "V∆∞·ª£t | ngo√†i | mong_ƒë·ª£i\n",
      "['Positive' 0.5925284624099731]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "m√¨nh | mua | 4 | chai | trong | ƒë√≥ | c√≥ | 2 | chai | l∆∞ng | KHG | ƒë·ªß | dung_t√≠ch\n",
      "['Negative' 0.595950722694397]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Minh_dung | sp | nay | gan | 10 | nam | roi | do | sp | rat | tot | ..\n",
      "['Positive' 0.5961607694625854]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_correct_pred(df_val,ascending=True,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6c3381f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". | H√†ng | r·∫•t | ƒë·∫πp | <3 | !\n",
      "True label: Negative, but predict Positive, with confidence 0.9974151849746704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tam | duoc | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn\n",
      "True label: Negative, but predict Positive, with confidence 0.997407853603363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | s·∫£n_ph·∫©m | tuy·ªát_v·ªùi | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | Shop | ph·ª•c_v·ª• | r·∫•t | t·ªët | Th·ªùi_gian | giao | h√†ng | ch·∫≠m | R·∫•t | ƒë√°ng | ti·ªÅn | Th·ªùi_gian | giao | h√†ng | ch·∫≠m\n",
      "True label: Negative, but predict Positive, with confidence 0.997378945350647\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | s·∫£n_ph·∫©m | tuy·ªát_v·ªùi | nh∆∞ng | to√†n | ch·ªØ | Trung_Qu·ªëc | .\n",
      "True label: Negative, but predict Positive, with confidence 0.997368574142456\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H√†ng | y | h√¨nh | ƒë·∫πp | sang_tr·ªçng\n",
      "True label: Negative, but predict Positive, with confidence 0.9973480701446533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "chat | l∆∞·ª£ng | r·∫•t | t·ªët | ok\n",
      "True label: Negative, but predict Positive, with confidence 0.9973419308662415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "t·ªët | dong | goi | dep | nhung | the | nho | bi | hong | kh√¥ng | d√πng | duoc | ngo√†i | Ra | OK | üëå\n",
      "True label: Negative, but predict Positive, with confidence 0.9972805976867676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üò° | üò°\n",
      "True label: Negative, but predict Positive, with confidence 0.9972085356712341\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | ph·ª•c_v·ª• | r·∫•t | t·ªët | Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh | ti·ªÅn | n√†o | th√¨ | c·ªßa | ƒë√≥\n",
      "True label: Negative, but predict Positive, with confidence 0.9970918893814087\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Take | wrong | color\n",
      "True label: Negative, but predict Positive, with confidence 0.9969770908355713\n",
      "----------------------------------------------------------------------------------------------------\n",
      "·ªêp | h∆°i | c≈©_shop | nha | .\n",
      "True label: Negative, but predict Positive, with confidence 0.9968775510787964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "M√†u | qu√° | gh√™ | huhu\n",
      "True label: Negative, but predict Positive, with confidence 0.9968618154525757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | . | Tr·ª´ | th·∫≥ng | 3_* | v√¨ | ƒë·ªôi_gi√° | l√™n | qu√° | cao | so | v·ªõi | th·ª±c_t·∫ø\n",
      "True label: Negative, but predict Positive, with confidence 0.9968488812446594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn | . | H√†ng | y_·∫£nh | 100_% | m·∫´u_m√£ | ƒë·∫πp | c·∫ßm | ch·∫Øc | tay | nh·ªè | g·ªçn | mk | th·∫•y | kh√° | ti·ªán | . | ( | s·ª≠a | b√†i | khi | sd | : | s·∫°c | ch·∫≠m | nhah | t·ª•t | pin | k | h√†i | l√≤g | )\n",
      "True label: Negative, but predict Positive, with confidence 0.9968122839927673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh\n",
      "True label: Negative, but predict Positive, with confidence 0.9967789053916931\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh∆∞lon | . | üòå\n",
      "True label: Negative, but predict Positive, with confidence 0.9966498017311096\n",
      "----------------------------------------------------------------------------------------------------\n",
      "c≈©ng | ƒë·∫πp | nh∆∞ng | may | l·ªùi\n",
      "True label: Negative, but predict Positive, with confidence 0.9964595437049866\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | h√†ng | c·∫©n_th·∫≠n | S·∫£n_ph·∫©m | √≠t | b·ªçt | r·ª≠a | xong | th·∫•y | da_kh√¥ | cƒÉng | . | N√™n | d√πng | th√™m | v·ªõi | x·ªãt | kho√°ng | ƒë·ªÉ | d∆∞·ª°ng | ·∫©m\n",
      "True label: Negative, but predict Positive, with confidence 0.9962769150733948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "L·ª´a | nhau | √† | shop\n",
      "True label: Negative, but predict Positive, with confidence 0.9957062602043152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | s·∫£n_ph·∫©m | r·∫•t | kem | moi | mua | co | may | ngay | Ma_di_khong | dung | gio | moi | nguoi | dung | co | mua | hang | cua_shop | nua\n",
      "True label: Negative, but predict Positive, with confidence 0.9955772161483765\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=False,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "737caf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kh·∫©u_v·ªã | c·ªßa | m√¨nh | th√¨ | h∆°i | nh·∫°t | v√† | thi·∫øu | cay | :)\n",
      "True label: Positive, but predict Negative, with confidence 0.5006212592124939\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√≥ng_g√≥i | ƒë·∫πp | . | Nh∆∞ng | pin | ko | b·ªÅn | . | N·∫°p | s·∫°c | d·ª±_ph√≤ng | c·∫£ | ƒë√™m | m·ªõi | ƒë·∫ßy | m√† | S·∫°c | ch·ªâ | dc | 1 | l·∫ßn | l√† | h·∫øt | . | - | _ | -\n",
      "True label: Positive, but predict Negative, with confidence 0.5072213411331177\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ch·∫•t_l∆∞·ª£ng | c·ªßa | s·∫£n_ph·∫©m | ph√π_h·ª£p | v·ªõi | gi√° | ti·ªÅn | . | C√≥_th·ªÉ | h√£ng | c·∫ßn | nghi√™n_c·ª©u | sao | cho | c·ª•c | s·∫°c | √≠t | n√≥ng | trong | qu√°_tr√¨nh | s·∫°c | v√¨ | hi·ªán_nay | khi | s·∫°c | c·ª•c | s·∫°c | r·∫•t | n√≥ng | n√™n | t√¢m_l√Ω | ng∆∞·ªùi | s·ª≠_d·ª•ng | s·ª£ | n√≥ | ch√°y | n·ªï | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5155402421951294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "100 | k | th√¨ | v·∫≠y | l√† | t·ªët | r·ªìi | ... | mang | ƒëi | ch∆°i | c≈©ng | ƒë·∫πp | ph·∫øt | ... | ƒë·ª´ng | ho·∫°t_ƒë·ªông | m·∫°nh | th√¨ | ch·∫Øc | kh√¥ng | sao | ... | d√πng | ƒë∆∞·ª£c | 2 | tu·∫ßn | r·ªìi | , | gi·∫∑c | lu√¥n | r·ªìi | .... | ·ªîN | ! | ! | C√≥ | c√°i | l√† | ƒë·∫øn | khi | shiper | ƒë·∫øn | c·ªïng | m·ªõi | ƒëi·ªán_tho·∫°i | ch·ª© | ch·∫£ | b√°o | tr∆∞·ªõc | g√¨ | c·∫£ | ? | ?\n",
      "True label: Positive, but predict Negative, with confidence 0.5188581943511963\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SHOP | ƒêƒÇNG | ·∫¢NH | M·ªòT_ƒê·∫∞NG | S·∫¢N_PH·∫®M | M·ªòT | L·∫∫O | ._B√ÅN | C√ÅP | TH√å | √öP | H√åNH | C√ÅI | C√ÅP | TH√îI | CH·ª® | UP | C·∫¢ | C·ª§C | S·∫†C | G√ÇY | NH·∫¶M_L·∫™N | CHO | KH√ÅCH_H√ÄNG | L√Ä | COMBO_C√ÅP | + | D√ÇY | S·∫†C | . | T√îI | MUA | LI·ªÄN | 2 | C√ÅI | L√öC | NH·∫¨N | H√ÄNG | M·ªöI | NG·ªö | NG∆Ø·ªúI | RA | B·ªä | MUA | ƒê·∫ÆT | . | CH·∫§T_L∆Ø·ª¢NG | S·∫¢N_PH·∫®M | CH∆ØA | R√ï | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5189838409423828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | ch·∫≠m | . | doi | nhan | hang | s·ªët | ca | ru·ªôt\n",
      "True label: Negative, but predict Positive, with confidence 0.5209054946899414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "C√≥ | ai | d√πng | 6 | s | l√™n | 9.2.1 | l·∫°i | nhanh | h·∫øt | pin | nh∆∞ | t√¥i | kh√¥ng | . | Nhanh | h·∫øt | pin | qu√°\n",
      "True label: Negative, but predict Positive, with confidence 0.523202657699585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "M√¨nh | kh√¥ng | d√°m | d√πng | v√¨ | th·∫•y | m√£_v·∫°ch | 69 | : | ' | ( | . | T·∫°i | b·∫£n_th√¢n | ch∆∞a | t√¨m_hi·ªÉu | kƒ© | n√™n | c≈©ng | mua | cho | ƒë·ªß | gi√° | free | ship\n",
      "True label: Positive, but predict Negative, with confidence 0.5317555069923401\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê∆∞·ª£c | co | Th·ªÉ | mua | l√¢u | dai\n",
      "True label: Positive, but predict Negative, with confidence 0.5340108871459961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê√£ | ch√°t | th·ªëng_nh·∫•t | lo·∫°i | h√†ng | nh∆∞ng | shop | v·∫´n | g·ª≠i | h√†ng | ng·∫´u_nhi√™n | . | Ko | s·ª≠_d·ª•ng | ƒëc | ch√°n\n",
      "True label: Negative, but predict Positive, with confidence 0.5393141508102417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Th·ªùi_gian | giao | h√†ng | r·∫•t | nhanh | M√†u_s·∫Øc | kh√¥ng | ƒë√∫ng | d√π | m√¨nh | ƒë√£ | d·∫∑n | tr∆∞·ªõc\n",
      "True label: Positive, but predict Negative, with confidence 0.5393228530883789\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qu·∫°t | m·ªõi | mua | nh∆∞ng | kh√¥ng | qu·∫°t | ƒë∆∞·ª£c\n",
      "True label: Negative, but predict Positive, with confidence 0.5425977110862732\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H∆°i | r·ªông | so | vs | size | b√¨nh_th∆∞·ªùng | . | H√†ng | i | h√¨nh\n",
      "True label: Positive, but predict Negative, with confidence 0.5448225736618042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ƒê·∫∑t | c√≥ | 8 | mon | √† | giao | thi·∫øu | 2 | m√≥n | r·ªìi | l√†m_ƒÉn | th·∫≠t | qu√°_ƒë√°ng | m√†\n",
      "True label: Negative, but predict Positive, with confidence 0.5459489822387695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | oi | mua | nhi·ªÅu | h∆°n | sao | l·∫°i | ti·ªÅn | ship | ƒë·∫Øt | h∆°n\n",
      "True label: Positive, but predict Negative, with confidence 0.5503779649734497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh·∫°t_nh·∫Ωo | ch·ªâ | c√≥ | m·ªói | b√°nh_ph·ªü | dai | l·∫°t | v√† | n∆∞·ªõc | gia_v·ªã | trong_veo | . | ƒÇn | hao_hao | h·ªß_ti·∫øu | lu·ªôc | v·ªõi | n∆∞·ªõc | Knor | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5527167916297913\n",
      "----------------------------------------------------------------------------------------------------\n",
      "H√†ng | nh√°i | . | From | gi·∫ßy | kg | ƒë·∫πp | . | Ph√π_h·ª£p | v·ªõi | gi√° | ti·ªÅn | ƒë√£ | gi·∫£m | . | Ch·ª© | kg | nh∆∞ | h√†ng | qu√£ng | c√°o | . | ƒê√≥ng_g√≥i | s·∫£n_ph·∫©m | r·∫•t | ƒë·∫πp | v√† | ch·∫Øc_ch·∫Øn\n",
      "True label: Positive, but predict Negative, with confidence 0.5576053261756897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nh∆∞ | shit | ngta | v·∫≠y | √° | .._Hg | l√†m | ƒë∆∞·ª£c | tr√≤ | ch·ªëng | g√¨ | hit\n",
      "True label: Negative, but predict Positive, with confidence 0.5576322674751282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "kh√≥ | u·ªëng | m√¨nh | c·ª© | ng·ª° | l√† | ƒëang | u·ªëng | v√† | ƒë·∫Øp | b·ªôt | ngh·ªá | m√† | m√¨nh | mua | v·ªÅ | ƒë·ªÉ | n·∫•u_ƒÉn | üò≠\n",
      "True label: Negative, but predict Positive, with confidence 0.5652170181274414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Em | ƒë√£ | mua | gi√†y | m√† | em | kh√¥ng | th·∫•y | d√¢y | gi√†y | ƒë√¢u | h·∫øt | shop | ·∫°\n",
      "True label: Negative, but predict Positive, with confidence 0.5652967691421509\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=True,top_n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "10px",
    "top": "150px",
    "width": "196px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
