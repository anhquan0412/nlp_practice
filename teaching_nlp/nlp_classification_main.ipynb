{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf68ba4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-lib\" data-toc-modified-id=\"Load-lib-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load lib</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-crash-files-(do-once)\" data-toc-modified-id=\"Convert-crash-files-(do-once)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Convert crash files (do once)</a></span></li><li><span><a href=\"#Load-csv\" data-toc-modified-id=\"Load-csv-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load csv</a></span></li></ul></li><li><span><a href=\"#Vietnamese-tokenization\" data-toc-modified-id=\"Vietnamese-tokenization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vietnamese tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Pretokenization-(Vietnamese-word-tokenization)\" data-toc-modified-id=\"Step-1:-Pretokenization-(Vietnamese-word-tokenization)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Step 1: Pretokenization (Vietnamese word tokenization)</a></span></li><li><span><a href=\"#Step-2:-Model's-tokenization\" data-toc-modified-id=\"Step-2:-Model's-tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Step 2: Model's tokenization</a></span></li></ul></li><li><span><a href=\"#Use-HuggingFace-Dataset-to-store-and-tokenize-corpus\" data-toc-modified-id=\"Use-HuggingFace-Dataset-to-store-and-tokenize-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Use HuggingFace Dataset to store and tokenize corpus</a></span></li><li><span><a href=\"#Define-dataset-dict-and-perform-train/val-split\" data-toc-modified-id=\"Define-dataset-dict-and-perform-train/val-split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define dataset dict and perform train/val split</a></span></li><li><span><a href=\"#Model-Understanding\" data-toc-modified-id=\"Model-Understanding-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Understanding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-hidden-states-for-sentence-similarity\" data-toc-modified-id=\"Extract-hidden-states-for-sentence-similarity-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Extract hidden states for sentence similarity</a></span></li></ul></li><li><span><a href=\"#Sentiment-classification-training\" data-toc-modified-id=\"Sentiment-classification-training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Sentiment classification training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-using-default-PhoBert\" data-toc-modified-id=\"Classification-using-default-PhoBert-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Classification using default PhoBert</a></span></li><li><span><a href=\"#Define-helper-function-for-training\" data-toc-modified-id=\"Define-helper-function-for-training-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Define helper function for training</a></span></li><li><span><a href=\"#Start-training\" data-toc-modified-id=\"Start-training-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Start training</a></span></li></ul></li><li><span><a href=\"#Prediction-interpretation\" data-toc-modified-id=\"Prediction-interpretation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Prediction interpretation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-report-and-confusion-matrix\" data-toc-modified-id=\"Classification-report-and-confusion-matrix-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Classification report and confusion matrix</a></span></li><li><span><a href=\"#Most-confident-prediction:-right-vs-wrong\" data-toc-modified-id=\"Most-confident-prediction:-right-vs-wrong-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Most confident prediction: right vs wrong</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dc9b6",
   "metadata": {},
   "source": [
    "# Load lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ada2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10c8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71235da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c8a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4ec21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af5dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel # body only\n",
    "#inherit this to load pretrained weight\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6d3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data_crash')\n",
    "RAW_PATH = Path('raw_crash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01542e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346f326",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83962f99",
   "metadata": {},
   "source": [
    "## Convert crash files (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696ba470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if not os.path.exists(DATA_PATH):\n",
    "#     os.mkdir(DATA_PATH)\n",
    "\n",
    "# ### Cleaning training file\n",
    "\n",
    "# train = open(RAW_PATH/\"train.crash\").readlines()\n",
    "# id_locations = []\n",
    "# label_locations = []\n",
    "# for idx, line in tqdm(enumerate(train)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"train_\"):\n",
    "#         id_locations.append(idx)\n",
    "#     elif line == \"0\" or line == \"1\":\n",
    "#         label_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for id_loc, l_loc in tqdm(zip(id_locations, label_locations)):\n",
    "#     line_id = train[id_loc].strip()\n",
    "#     label = train[l_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(train[id_loc + 1: l_loc])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\\t{label}\")\n",
    "\n",
    "# with open(DATA_PATH/\"train.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\tlabel\\n\")\n",
    "#     f.write(\"\\n\".join(data))\n",
    "\n",
    "# ### Cleaning test file\n",
    "\n",
    "# test = open(RAW_PATH/\"test.crash\").readlines()\n",
    "# id_locations = []\n",
    "# for idx, line in tqdm(enumerate(test)):\n",
    "#     line = line.strip()\n",
    "#     if line.startswith(\"test_\"):\n",
    "#         id_locations.append(idx)\n",
    "# data = []\n",
    "\n",
    "# for i, id_loc in tqdm(enumerate(id_locations)):\n",
    "#     if i >= len(id_locations) - 1:\n",
    "#         end = len(test)\n",
    "#     else:\n",
    "#         end = id_locations[i + 1]\n",
    "#     line_id = test[id_loc].strip()\n",
    "#     text = re.sub('\\s+', ' ', ' '.join(test[id_loc + 1:end])).strip()[1:-1].strip()\n",
    "#     data.append(f\"{line_id}\\t{text}\")\n",
    "\n",
    "# with open(DATA_PATH/\"test.csv\", \"wt\") as f:\n",
    "#     f.write(\"id\\ttext\\n\")\n",
    "#     f.write(\"\\n\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe363",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f367e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {str(DATA_PATH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv',sep='\\t').fillna(\"###\")\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv',sep='\\t').fillna(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5cb72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9602</th>\n",
       "      <td>San pham dung nhu tren hinh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13611</th>\n",
       "      <td>ga giường mỏng như lá lúa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13163</th>\n",
       "      <td>Đế giày còn nhiều lỗi 🙂</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13740</th>\n",
       "      <td>Do vchuyen len bị rớt bên ngoài chút ạ nhưng k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời😘 Đóng gói sản ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13362</th>\n",
       "      <td>- giày rất đẹp sau này sẽ ủng hộ shop tiếp nữa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>Mua 2 cái sạc iphone thì giao 2 cái sạc samsun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>Nó tệ hơn so với những gì mình tưởng. mua nó m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7944</th>\n",
       "      <td>Giao hang rất nhanh. Dong goi rất can than.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>Lần sau ủng hộ shop nữa. Ngon.Đóng gói sản phẩ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Thời gian giao hàng rất nhanh Shop phục vụ rất...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>Đặt 10 món mà giao thiếu không báo thiếu không...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>Sản phẩm mùi rất thơm mùi nước hoa hồng rất th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>Sp k giống hình. Hàng giao có 1 cái bị rách đổ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Shop giao hàng kh giống màu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13132</th>\n",
       "      <td>cảm ơn shop đã giao đúng mẫu mình rất ưng sẽ ủ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9029</th>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời Đóng gói sản phẩ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>san pham khac so voi hinh mau.quan din dau nho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8456</th>\n",
       "      <td>Chất lượng sản phẩm tuyệt vời Đóng gói sản phẩ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Giấy quá mỏng đọc khá là đau đầu. Tên riêng dị...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "9602                         San pham dung nhu tren hinh      0\n",
       "13611                          ga giường mỏng như lá lúa      1\n",
       "13163                            Đế giày còn nhiều lỗi 🙂      1\n",
       "13740  Do vchuyen len bị rớt bên ngoài chút ạ nhưng k...      0\n",
       "1799   Chất lượng sản phẩm tuyệt vời😘 Đóng gói sản ph...      0\n",
       "13362     - giày rất đẹp sau này sẽ ủng hộ shop tiếp nữa      0\n",
       "9671   Mua 2 cái sạc iphone thì giao 2 cái sạc samsun...      1\n",
       "10343  Nó tệ hơn so với những gì mình tưởng. mua nó m...      1\n",
       "7944       Giao hang rất nhanh. Dong goi rất can than.      0\n",
       "2296   Lần sau ủng hộ shop nữa. Ngon.Đóng gói sản phẩ...      0\n",
       "4997   Thời gian giao hàng rất nhanh Shop phục vụ rất...      0\n",
       "2026   Đặt 10 món mà giao thiếu không báo thiếu không...      1\n",
       "10569  Sản phẩm mùi rất thơm mùi nước hoa hồng rất th...      0\n",
       "6667   Sp k giống hình. Hàng giao có 1 cái bị rách đổ...      1\n",
       "33                           Shop giao hàng kh giống màu      1\n",
       "13132  cảm ơn shop đã giao đúng mẫu mình rất ưng sẽ ủ...      0\n",
       "9029   Chất lượng sản phẩm tuyệt vời Đóng gói sản phẩ...      0\n",
       "154    san pham khac so voi hinh mau.quan din dau nho...      1\n",
       "8456   Chất lượng sản phẩm tuyệt vời Đóng gói sản phẩ...      0\n",
       "220    Giấy quá mỏng đọc khá là đau đầu. Tên riêng dị...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['text','label']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4a826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chất lượng sản phẩm rất kém. Mình mua 2sp 1sp dùng được 1 sp bị hỏng'\n",
      "  1]\n",
      " ['Chất lượng sản phẩm tuyệt vời thanks shop đã lên deal!' 0]\n",
      " ['Giày chiếc to chiếc nhỏ' 1]\n",
      " ['Quá xấu đường may quá thưa chất vải quá xấu ko phải chất quần jieans'\n",
      "  1]\n",
      " ['Ối zời ơi, ngon quá xá luôn Ngon ngoài sức tưởng tượng Thik nhất sấu và mơ chua cay'\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['text','label']].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377a4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Không giống ảnh ở nhiều chi tiết']\n",
      " ['Hàng y hình rất đẹp và chắc chắn.']\n",
      " ['dễ thương ko có j để bàn cãi chỉ mỗi tội bị nhỏ em giành mất ko có xài TT.TT']\n",
      " ['kem không thấm khó tán đều sau khoảng 1-2h lại bết vào nếp da trắng bệt nhìn không tự nhiên chút nào cả khó trôi kể cả dùng xà phòng. phải dùng muối biển tẩy da mới hết. tác dụng chống nắng cảm giác không đạt spf 50 đâu. nói chung là xài mới có vài lần là bỏ luôn cả type. uổng tiền!!!']\n",
      " ['Rất phù hợp giá tiền. Ck mình mặc cái nào cũng đẹp. Có 6sao thì cho 6 sao luôn.']]\n"
     ]
    }
   ],
   "source": [
    "print(test_df[['text']].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1ceee",
   "metadata": {},
   "source": [
    "# Vietnamese tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4c606",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098136789/files/assets/nlpt_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a346",
   "metadata": {},
   "source": [
    "Jack Sparrow loves New York!\n",
    "\n",
    "\n",
    "1. Normalization\n",
    "    - set of operations you apply to a raw string to **make it “cleaner”**, e.g. stripping whitespace, rm accented chars, lowercasing, Unicode normalization (unify various ways to write the same character)\n",
    "\n",
    "=> jack sparrow loves new york!\n",
    "\n",
    "2. Pretokenization\n",
    "    - splits a text into smaller objects (can be words) that **give an upper bound to what your tokens will be at the end of training; your final tokens will be parts of these smaller objects**\n",
    "    - Sometimes splitting into 'words' is not always trivial (Chinese, Japanese, Korean). In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization.\n",
    "\n",
    "=> [\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]\n",
    "\n",
    "3. Tokenizer model\n",
    "    - tokenizer applies a **subword splitting model** on the words. This is the part of the pipeline that **needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer)**\n",
    "    -  to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens\n",
    "    - Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece\n",
    "    \n",
    "=> [jack, spa, rrow, loves, new, york, !]\n",
    "\n",
    "NOTE: at this point we no longer have a list of strings but a list of integers (input IDs)\n",
    "\n",
    "4. Postprocessing\n",
    "    - some additional transformations can be applied on the list of tokens\n",
    "    - e.g. adding special tokens at the beginning or the end\n",
    "    - This is the last step, and the sequence of integers can be fed to the model\n",
    "=> a BERT-style tokenizer would add classifications and separator tokens: [CLS, jack, spa, rrow, loves, new, york, !, SEP]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be97150",
   "metadata": {},
   "source": [
    "## Step 1: Pretokenization (Vietnamese word tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b9e4",
   "metadata": {},
   "source": [
    "https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378087",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> from underthesea import word_tokenize\n",
    ">>> sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
    "\n",
    ">>> word_tokenize(sentence)\n",
    "['Chàng trai', '9X', 'Quảng Trị', 'khởi nghiệp', 'từ', 'nấm', 'sò']\n",
    "\n",
    ">>> word_tokenize(sentence, format=\"text\")\n",
    "'Chàng_trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43cefc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_tokenize(sen,split_word=False):\n",
    "    # optional step: fix the whitespace between words\n",
    "    sen = \" \".join(sen.split())\n",
    "    sens = sent_tokenize(sen)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen+=word_tokenize(sen,format='text' if not split_word else None)\n",
    "    \n",
    "    if not split_word:\n",
    "        return ''.join(tokenized_sen)\n",
    "    return ['_'.join(words.split(' ')) for words in tokenized_sen]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63754082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '?', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời? Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f171898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '😌_😌', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "# print(apply_word_tokenize(_tmp,False))\n",
    "print(apply_word_tokenize(_tmp,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c089cc",
   "metadata": {},
   "source": [
    "Apply on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32166f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [apply_word_tokenize(s[0],True) for s in train_df[['text']].values]\n",
    "train_label = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90044db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [apply_word_tokenize(s[0],True) for s in test_df[['text']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc34d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'tuyệt_vời',\n",
       "  '.',\n",
       "  'y',\n",
       "  'hình',\n",
       "  'chụp',\n",
       "  '.',\n",
       "  'đáng',\n",
       "  'tiền'],\n",
       " ['Hjhj_shop',\n",
       "  'giao',\n",
       "  'hàng',\n",
       "  'nhanh',\n",
       "  'quá',\n",
       "  '.',\n",
       "  'Đẹp',\n",
       "  'lắm',\n",
       "  'ạ',\n",
       "  'bé',\n",
       "  'nhà',\n",
       "  'm',\n",
       "  'rất',\n",
       "  'thích'],\n",
       " ['nhìn', 'đẹp', 'phết', 'nhỉ', '..'],\n",
       " ['Đóng_gói',\n",
       "  'rất',\n",
       "  'đẹp',\n",
       "  '.',\n",
       "  'Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'rất',\n",
       "  'tốt',\n",
       "  'Chất_lượng',\n",
       "  'sản_phẩm',\n",
       "  'tuyệt_vời'],\n",
       " ['Săn', 'đc', 'với', 'giá', '11', 'k', '.', 'Toẹt', 'vời']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2f6f",
   "metadata": {},
   "source": [
    "## Step 2: Model's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed8d0",
   "metadata": {},
   "source": [
    "To use pretrained language model such as BERT, GPT, Roberta... We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n",
    "\n",
    "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their hub. In this notebook, we use PhoBERT-base (vinai/phobert-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f842db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e1bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_explain(inp,split_word):\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()\n",
    "\n",
    "\n",
    "def two_step_tokenization_explain(inp,split_word=False):\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Pretokenization ---')\n",
    "    tok = apply_word_tokenize(inp,split_word)\n",
    "    print(tok)\n",
    "    print()\n",
    "    tokenizer_explain(tok,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 4, 7079, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', ',', 'phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời, phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời, phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7978be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "Chất_lượng sản_phẩm tuyệt_vời .Phấn mịn , đóng_gói sản_phẩm rất đẹp và chắc_chắn\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 2586, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.@@', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời.Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời. Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7f2dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw sentence ---\n",
      "Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn\n",
      "\n",
      "--- Pretokenization ---\n",
      "['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '😌_😌', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 6869, 265, 1819, 3, 1751, 3, 5, 11459, 5451, 4, 8179, 265, 59, 258, 6, 994, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens\n",
      "['<s>', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '<unk>', '_@@', '<unk>', '.', 'Phấn', 'mịn', ',', 'đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Chất_lượng sản_phẩm tuyệt_vời <unk> _<unk>. Phấn mịn, đóng_gói sản_phẩm rất đẹp và chắc_chắn </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_tmp = 'Chất lượng sản phẩm tuyệt vời 😌😌. Phấn mịn, đóng gói sản phẩm rất đẹp và chắc chắn'\n",
    "two_step_tokenization_explain(_tmp,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0c4f",
   "metadata": {},
   "source": [
    "# Use HuggingFace Dataset to store and tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2481f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f388127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # pad to model's allowed max length, which is max_sequence_length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87c4b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text[:4],\n",
    "                        'label':train_label[:4],\n",
    "                        }\n",
    "                    )\n",
    "# test_dataset = Dataset.from_dict(\n",
    "#                         {'text': test_text,\n",
    "#                         }\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1572e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8b3fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e055d34efa4d8ca9456242a6bef98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10db51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013debff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dung', 'dc', 'sp', 'tot', 'cam', 'on_shop', 'Đóng_gói', 'sản_phẩm', 'rất', 'đẹp', 'và', 'chắc_chắn', 'Chất_lượng', 'sản_phẩm', 'tuyệt_vời'], ['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', '.', 'Son', 'mịn', 'nhưng', 'khi', 'đánh', 'lên', 'không', 'như', 'màu', 'trên', 'ảnh'], ['Chất_lượng', 'sản_phẩm', 'tuyệt_vời', 'nhưng', 'k', 'có', 'hộp', 'k', 'có', 'dây', 'giày', 'đen', 'k', 'có', 'tất'], [':', '(', '(', 'Mình', 'hơi', 'thất_vọng', '1', 'chút', 'vì', 'mình', 'đã', 'kỳ_vọng', 'cuốn', 'sách', 'khá', 'nhiều', 'hi_vọng', 'nó', 'sẽ', 'nói', 'về', 'việc', 'học_tập', 'của', 'cách', 'sinh_viên', 'trường', 'Harvard', 'ra_sao', 'những', 'nỗ_lực', 'của', 'họ', 'như', 'thế_nào', '4', 'h', 'sáng', '?', 'tại_sao', 'họ', 'lại', 'phải', 'thức', 'dậy', 'vào', 'thời_khắc', 'đấy', '?', 'sau', 'đó', 'là', 'cả', 'một', 'câu_chuyện', 'ra_sao', '.', 'Cái', 'mình', 'thực_sự', 'cần', 'ở', 'đây', 'là', 'câu_chuyện', 'ẩn', 'dấu', 'trong', 'đó', 'để', 'tự', 'bản_thân', 'mỗi', 'người', 'cảm_nhận', 'và', 'đi', 'sâu', 'vào', 'lòng', 'người', 'hơn', '.', 'Còn', 'cuốn', 'sách', 'này', 'chỉ', 'đơn_thuần', 'là', 'cuốn', 'sách', 'dạy', 'kĩ_năng', 'mà', 'hầu_như', 'sách', 'nào', 'cũng', 'đã', 'có', '.', 'BUồn', '...']]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f6c2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3556, 1236, 1894, 36150, 2225, 1204, 2947, 1672, 20811, 54922, 55662, 1685, 265, 59, 258, 6, 994, 6869, 265, 1819, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 5, 16332, 5451, 51, 26, 480, 72, 17, 42, 412, 34, 284, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6869, 265, 1819, 51, 1947, 10, 2275, 1947, 10, 1747, 2466, 989, 1947, 10, 7328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 27, 20, 20, 3000, 1329, 2804, 99, 2013, 90, 68, 14, 2109, 1088, 713, 281, 36, 4876, 231, 38, 96, 28, 49, 1227, 7, 139, 649, 212, 9913, 57964, 3075, 21, 773, 7, 86, 42, 1279, 163, 1664, 298, 114, 2393, 86, 44, 41, 2908, 1764, 33, 9171, 1582, 114, 53, 37, 8, 94, 16, 876, 57964, 3075, 5, 2510, 68, 742, 115, 25, 97, 8, 876, 4592, 3309, 12, 37, 24, 385, 744, 205, 18, 2601, 6, 57, 808, 33, 605, 18, 48, 5, 631, 1088, 713, 23, 66, 5284, 8, 1088, 713, 940, 10685, 64, 2903, 713, 142, 32, 14, 10, 5, 924, 1878, 5460, 135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1e74a",
   "metadata": {},
   "source": [
    "# Define dataset dict and perform train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1925dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1215ad4a192441790921b1574c3d5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cef9a2ae974bc2a8414156499e9625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "                        {'text': train_text,\n",
    "                        'label':train_label,\n",
    "                        }\n",
    "                    )\n",
    "test_dataset = Dataset.from_dict(\n",
    "                        {'text': test_text,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True)\n",
    "test_dataset_tokenized= test_dataset.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d637faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "train_dataset_tokenized = train_dataset_tokenized.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2184388",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized = DatasetDict()\n",
    "main_dataset_tokenized['train'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8)))\n",
    "main_dataset_tokenized['validation'] = train_dataset_tokenized.select(range(int(train_dataset_tokenized.num_rows*0.8),train_dataset_tokenized.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed94e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3218\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ebcace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.580154\n",
       "1    0.419846\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['train']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d049de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.563704\n",
       "1    0.436296\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(main_dataset_tokenized['validation']['label']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e7953",
   "metadata": {},
   "source": [
    "# Model Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224bed4",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "646a3340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ad9f6",
   "metadata": {},
   "source": [
    "## Extract hidden states for sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18f42a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch,model=None):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).hidden_states[-1]\n",
    "\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "def extract_hidden_states_concat(batch,model=None):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last 4 hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(**inputs).hidden_states[-4:]\n",
    "        last_hidden_states = [hs[:,0] for hs in last_hidden_states]\n",
    "        concat_hidden_states = torch.cat(last_hidden_states,1)\n",
    "\n",
    "    return {\"hidden_state\": concat_hidden_states.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "333a3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def raw_model_init():\n",
    "    model = AutoModel.from_pretrained(\"vinai/phobert-base\",output_hidden_states=True,output_attentions=True)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "raw_model = raw_model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfa93af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d20ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "train_sample = train_dataset_tokenized.shuffle().select(range(int(train_dataset_tokenized.num_rows*1)))\n",
    "sample_dataset_tokenized = DatasetDict()\n",
    "trn_sz = 1\n",
    "sample_dataset_tokenized['train'] = train_sample.select(range(int(train_sample.num_rows*trn_sz)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48cb6c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1178ceff9dd64753913809e2fe2bafdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8044 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "sample_dataset_tokenized['train'] = sample_dataset_tokenized['train'].map(partial(extract_hidden_states_concat,model=raw_model), batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ec66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c0b0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def concat_str(inp):\n",
    "    return ' '.join(inp)\n",
    "def show_neighbors(neigh,idx,data=data,metadata=sample_dataset_tokenized['train'],n_neighbors=5):\n",
    "    print(f\"Sentence: {concat_str(metadata['text'][idx])}\\nLabel: {metadata['label'][idx]}\")\n",
    "    distances,nbors = neigh.kneighbors([data[idx]])\n",
    "\n",
    "    print('\\nNeighbors: ')\n",
    "    for d,n_idx in zip(distances[0],nbors[0]):\n",
    "        print(f\"\\t{concat_str(metadata['text'][n_idx])}, d={d:.3f}, label={metadata['label'][n_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06760021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sample_dataset_tokenized['train']['hidden_state'].cpu().numpy()\n",
    "neigh = NearestNeighbors(n_neighbors=4)\n",
    "neigh.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53482b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Đặt 1 hộp grow plus + 4 chai pediasure thì có 1_Chai đã bị khui ra và dán nắp lại bằng keo dán sắt . Tôi lấy đồ cố_gắng mở nắp ra thì thấy màu_nước đục và mùi hôi_thối bốc lên nồng_nặc . Bán hàng sao ko có lương_tâm vậy bạn ? Đây là sữa dành cho em bé uống mà bạn bán hàng thất_đức thế .\n",
      "Label: 1\n",
      "\n",
      "Neighbors: \n",
      "\tĐặt 1 hộp grow plus + 4 chai pediasure thì có 1_Chai đã bị khui ra và dán nắp lại bằng keo dán sắt . Tôi lấy đồ cố_gắng mở nắp ra thì thấy màu_nước đục và mùi hôi_thối bốc lên nồng_nặc . Bán hàng sao ko có lương_tâm vậy bạn ? Đây là sữa dành cho em bé uống mà bạn bán hàng thất_đức thế ., d=0.000, label=1\n",
      "\tĐóng_gói sản_phẩm rất kém . Gói bỉm bẩn_thỉu kinh_khủng đã rách vì bị cắt tem nên miếng bỉm bên trong cũng dính bụi bẩn luôn . Mình đã vứt đi coi như vứt tiền vào sọt_rác vì chả biết liệu đã có những con gì chui vào trong gói bỉm rồi . Chưa kể giao hàng chậm . Quá thất_vọng ! ! !, d=5.800, label=1\n",
      "\tSản_phẩm bị hư móc khoá . Mình có liên_hệ bên shop để đổi hàng nhưng shop ko phản_hồi . Quá thất_vọng . Làm ăn kiểu này sẽ ko còn bạn nào đặt_hàng onl của shop đâu, d=6.255, label=1\n",
      "\tĐóng_gói sản_phẩm rất đẹp và chắc_chắn , Shop phục_vụ rất tốt , Thời_gian giao hàng rất nhanh . Máy_in rất ok nhé . Lần đầu mình sử_dụng loay_hoay mãi mới in được nhưng chất_lượng ảnh sáng rõ đẹp . Nhưng k biết dùng có bền k ., d=6.366, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,1,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18448d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Đáng đồng xèng . Gói cẩn_thận . Lại lịch_sự hihi , được tặng 1 đôi tất korea siêu xinhh . 5 saooo . Thời_gian giao hàng rất nhanh Rất đáng tiền Shop phục_vụ rất tốt Đóng_gói sản_phẩm rất đẹp và chắc_chắn Chất_lượng sản_phẩm tuyệt_vời\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tĐáng đồng xèng . Gói cẩn_thận . Lại lịch_sự hihi , được tặng 1 đôi tất korea siêu xinhh . 5 saooo . Thời_gian giao hàng rất nhanh Rất đáng tiền Shop phục_vụ rất tốt Đóng_gói sản_phẩm rất đẹp và chắc_chắn Chất_lượng sản_phẩm tuyệt_vời, d=0.000, label=0\n",
      "\tChất_lượng sản_phẩm tuyệt_vời Đóng_gói sản_phẩm rất đẹp và chắc_chắn Shop phục_vụ rất tốt Rất đáng tiền Thời_gian giao hàng rất nhanh . Vừa như in đẹp hết nói luôn . Giá rẻ hơn các shop khác nữa lại đc tặng 4 đôi tất . Hết_ý, d=6.340, label=0\n",
      "\tMua nhiều lần rồi . Dùng Ok lắm ưng hơn các loại bỉm khác . Phù_hợp giá tiền . Ủng_hộ shop dài_dài nhưng cái anh bán nhé . Bảo khách_hàng ruột tặng thêm 1 cái bỉm hứa lên hứa xuống chả thấy tặng kèm j . Chả_lẽ cho 4 sao ._😁, d=6.591, label=0\n",
      "\tĐóng_gói sản_phẩm rất chắc_chắn . Thời_gian giao hàng rất nhanh . Cực match với màu_da của mình . Cảm_ơn shop nhiều, d=6.650, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,2,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "907e822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Kém chất_lượng . hàng giả hàng nhái nhé\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tKém chất_lượng . hàng giả hàng nhái nhé, d=0.000, label=0\n",
      "\tHàng nhái thương_hiệu . chất_lượng vất đi . bán lừa hàng chính hãng, d=6.982, label=1\n",
      "\thàng tệ . loa kém chất_lượng, d=7.075, label=1\n",
      "\tShop làm_ăn gian_dối giao thiếu hàng còn không có trách_nhiệm . Chất_lượng sản_phẩm rất kém Shop phục_vụ rất kém . Mình khuyên không nên mua hàng ở shop này nhé, d=7.156, label=1\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,5,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed53dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Đã nhận được hàng ưng lắm nhé . Giao hàng rất nhanh . Cảm_ơn shop .\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tĐã nhận được hàng ưng lắm nhé . Giao hàng rất nhanh . Cảm_ơn shop ., d=0.000, label=0\n",
      "\tMình rất thích . Cảm_ơn shop nhé . Chất_lượng sản_phẩm tuyệt_vời . Đóng_gói sản_phẩm rất đẹp và chắc_chắn ., d=4.275, label=0\n",
      "\tShop đóng_gói sản_phẩm đẹp và cẩn_thận . Hàng chuẩn . Thời_gian giao hàng nhanh . Lại được thêm món quà nhỏ từ shop nữa . Cảm_ơn shop nhé ., d=4.295, label=0\n",
      "\tShop giao hàng rất nhanh . Cảm_ơn shop . Sẽ tiếp_tục ủng_hộ shop ., d=4.443, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,14,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed927473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Đặt dâu_tây mà giao lộn dưa_hấu : ( ( (\n",
      "Label: 1\n",
      "\n",
      "Neighbors: \n",
      "\tĐặt dâu_tây mà giao lộn dưa_hấu : ( ( (, d=0.000, label=1\n",
      "\tChiếc cao chiếc thấp : ( ( ( (, d=4.880, label=1\n",
      "\tHuhu mất cái mặt táo rồi : ( ( (, d=4.978, label=0\n",
      "\tSản_phẩm kg giống như hình và clip : ( ( (, d=5.256, label=1\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,15,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75311205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Cấu_hình tốt , đẹp , tiện\n",
      "Label: 0\n",
      "\n",
      "Neighbors: \n",
      "\tCấu_hình tốt , đẹp , tiện, d=0.000, label=0\n",
      "\tHàng đẹp - tốt, d=7.983, label=0\n",
      "\tTốt giá ổn, d=8.120, label=0\n",
      "\tKhá là xinh, d=8.220, label=0\n"
     ]
    }
   ],
   "source": [
    "show_neighbors(neigh,16,data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b4fb0",
   "metadata": {},
   "source": [
    "# Sentiment classification training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40541781",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089eb60",
   "metadata": {},
   "source": [
    "## Classification using default PhoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d414ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "# from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "# from transformers.models.roberta.modeling_roberta import RobertaModel \n",
    "# from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f89e470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "522bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_init(get_hidden=False):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2,output_hidden_states=get_hidden)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780da385",
   "metadata": {},
   "source": [
    "## Define helper function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77905e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gc\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # pred: EvalPrediction object \n",
    "    # (which is a named tuple with predictions and label_ids attributes)\n",
    "    labels = pred.label_ids\n",
    "    if isinstance(pred.predictions,tuple):\n",
    "        preds = pred.predictions[0].argmax(-1)\n",
    "    else:\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1,\"accuracy\": acc}\n",
    "\n",
    "\n",
    "\n",
    "def finetune(lr,bs,wd,epochs,ddict,tokenizer=tokenizer,o_dir = './outputs',logging=False,model_init=base_model_init):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not logging:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    else:\n",
    "        training_args = TrainingArguments(o_dir, \n",
    "                                 learning_rate=lr, \n",
    "                                 warmup_ratio=0.1, \n",
    "                                 lr_scheduler_type='cosine', \n",
    "                                 fp16=True,\n",
    "                                do_train=True,\n",
    "                                 do_eval=True,\n",
    "                                 evaluation_strategy=\"epoch\", \n",
    "                                 save_strategy=\"epoch\",\n",
    "                                 overwrite_output_dir=True,\n",
    "                                gradient_accumulation_steps=1,\n",
    "                                 per_device_train_batch_size=bs, \n",
    "                                 per_device_eval_batch_size=bs,\n",
    "                               logging_dir=os.path.join(o_dir, 'log'),\n",
    "                                logging_steps = len(ddict[\"train\"]) // bs,\n",
    "                                num_train_epochs=epochs, weight_decay=wd)\n",
    "\n",
    "    # instantiate trainer\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=ddict['train'],#.shard(200, 0),    # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "        eval_dataset=ddict['validation'],#.shard(100, 0), # Only use subset of the dataset for a quick training. Remove shard for full training\n",
    "#         data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a25275",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9cdad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "loading configuration file config.json from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/quan1080/.cache/huggingface/hub/models--vinai--phobert-base/snapshots/667b55927a1571811539f27c0f374429a1c75759/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/quan1080/anaconda3/envs/fastai_v2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12869\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 09:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288897</td>\n",
       "      <td>0.907470</td>\n",
       "      <td>0.908950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.298636</td>\n",
       "      <td>0.897811</td>\n",
       "      <td>0.899627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.306182</td>\n",
       "      <td>0.909048</td>\n",
       "      <td>0.910193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-403\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-403/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-403/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-403/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-403/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-806\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-806/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-806/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-806/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-806/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3218\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./default_phobert_finetuned_1/checkpoint-1209\n",
      "Configuration saved in ./default_phobert_finetuned_1/checkpoint-1209/config.json\n",
      "Model weights saved in ./default_phobert_finetuned_1/checkpoint-1209/pytorch_model.bin\n",
      "tokenizer config file saved in ./default_phobert_finetuned_1/checkpoint-1209/tokenizer_config.json\n",
      "Special tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/special_tokens_map.json\n",
      "added tokens file saved in ./default_phobert_finetuned_1/checkpoint-1209/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=32\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "o_dir = './default_phobert_finetuned_1'\n",
    "tmp = finetune(lr,bs,wd,epochs,ddict=main_dataset_tokenized,o_dir = o_dir)\n",
    "\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1\tAccuracy\n",
    "# 1\tNo log\t0.288897\t0.907470\t0.908950\n",
    "# 2\t0.298100\t0.298636\t0.897811\t0.899627\n",
    "# 3\t0.180200\t0.306182\t0.909048\t0.910193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11fc7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1209  checkpoint-403  checkpoint-537  checkpoint-806\r\n"
     ]
    }
   ],
   "source": [
    "!ls default_phobert_finetuned_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7655e9",
   "metadata": {},
   "source": [
    "# Prediction interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57401c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model_init()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.from_pretrained('./default_phobert_finetuned_1/checkpoint-806')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d78ff2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2str={0:\"Positive\",1:'Negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f951b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def label_int2str(row):\n",
    "    return idx2str[row]\n",
    "\n",
    "def forward_pass_with_label(batch,model=None):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy(),\n",
    "           'predicted_probability': torch.nn.functional.softmax(output.logits.cpu(),dim=1).numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac5b647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "main_dataset_tokenized.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d9bb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ef1fec038413fb0e6afd0c1d31beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/805 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b3e8a8cb64f6cb896676357428607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute loss values\n",
    "main_dataset_tokenized[\"train\"] = main_dataset_tokenized[\"train\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)\n",
    "main_dataset_tokenized[\"validation\"] = main_dataset_tokenized[\"validation\"].map(\n",
    "    partial(forward_pass_with_label,model=model), batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98453d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_tokenized.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", 'predicted_probability',\"loss\"]\n",
    "df_val = main_dataset_tokenized[\"validation\"][:][cols]\n",
    "df_val[\"label_str\"] = df_val[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_val[\"predicted_label_str\"] = (df_val[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4047fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = main_dataset_tokenized[\"train\"][:][cols]\n",
    "df_trn[\"label_str\"] = df_trn[\"label\"].apply(lambda x: idx2str[x])\n",
    "df_trn[\"predicted_label_str\"] = (df_trn[\"predicted_label\"]\n",
    "                              .apply(lambda x: idx2str[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31a12ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12869, 7), (3218, 7))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.shape,df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a301ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Thông_tin, chi_tiết, trên, web, là, sản_xuất,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0779537, 0.92204636]</td>\n",
       "      <td>0.081160</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Đồng_hồ, dây_đeo, ọp_ẹp, ., Tất_nilon, rẻ_tiề...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04499709, 0.9550029]</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Shop, de, thương, nhiet, tình, sẽ, ủng, hô, l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99736834, 0.0026315863]</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nhìn, k, chắc_chắn, lắm]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5285818, 0.47141826]</td>\n",
       "      <td>0.637558</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Chất_lượng, sản_phẩm, rất, kém, ., Rách, lỗi,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.022633858, 0.9773662]</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [Thông_tin, chi_tiết, trên, web, là, sản_xuất,...      1                1   \n",
       "1  [Đồng_hồ, dây_đeo, ọp_ẹp, ., Tất_nilon, rẻ_tiề...      1                1   \n",
       "2  [Shop, de, thương, nhiet, tình, sẽ, ủng, hô, l...      0                0   \n",
       "3                          [Nhìn, k, chắc_chắn, lắm]      0                0   \n",
       "4  [Chất_lượng, sản_phẩm, rất, kém, ., Rách, lỗi,...      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0     [0.0779537, 0.92204636]  0.081160  Negative            Negative  \n",
       "1     [0.04499709, 0.9550029]  0.046041  Negative            Negative  \n",
       "2  [0.99736834, 0.0026315863]  0.002635  Positive            Positive  \n",
       "3     [0.5285818, 0.47141826]  0.637558  Positive            Positive  \n",
       "4    [0.022633858, 0.9773662]  0.022894  Negative            Negative  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc7b897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_probability</th>\n",
       "      <th>loss</th>\n",
       "      <th>label_str</th>\n",
       "      <th>predicted_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Cầm, khá, chắc_tay, ., Trục, uốn, to, đúng, q...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.997373, 0.0026270347]</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3G, còn, tùy, lúc_nào, và, khi, nào, và, ở, đ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99603903, 0.0039609983]</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Đóng_gói, sản_phẩm, rất, chắc_chắn, ., Thời_g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99715614, 0.0028438682]</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[mình, xài, em, này, bị, đứng, máy, hoài, à]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.13099755, 0.8690025]</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Chết, $]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.09323443, 0.9067656]</td>\n",
       "      <td>0.097871</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label  \\\n",
       "0  [Cầm, khá, chắc_tay, ., Trục, uốn, to, đúng, q...      0                0   \n",
       "1  [3G, còn, tùy, lúc_nào, và, khi, nào, và, ở, đ...      0                0   \n",
       "2  [Đóng_gói, sản_phẩm, rất, chắc_chắn, ., Thời_g...      0                0   \n",
       "3       [mình, xài, em, này, bị, đứng, máy, hoài, à]      1                1   \n",
       "4                                          [Chết, $]      1                1   \n",
       "\n",
       "        predicted_probability      loss label_str predicted_label_str  \n",
       "0    [0.997373, 0.0026270347]  0.002630  Positive            Positive  \n",
       "1  [0.99603903, 0.0039609983]  0.003969  Positive            Positive  \n",
       "2  [0.99715614, 0.0028438682]  0.002848  Positive            Positive  \n",
       "3     [0.13099755, 0.8690025]  0.140409  Negative            Negative  \n",
       "4     [0.09323443, 0.9067656]  0.097871  Negative            Negative  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a29fd",
   "metadata": {},
   "source": [
    "## Classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8f10912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.96      0.96      0.96      7466\n",
      "    Negative       0.94      0.95      0.94      5403\n",
      "\n",
      "    accuracy                           0.95     12869\n",
      "   macro avg       0.95      0.95      0.95     12869\n",
      "weighted avg       0.95      0.95      0.95     12869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_trn.label.values,df_trn.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0670fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.91      0.92      0.91      1814\n",
      "    Negative       0.89      0.88      0.88      1404\n",
      "\n",
      "    accuracy                           0.90      3218\n",
      "   macro avg       0.90      0.90      0.90      3218\n",
      "weighted avg       0.90      0.90      0.90      3218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_val.label.values,df_val.predicted_label.values,target_names = ['Positive','Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d826ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb4ac47fdf0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,10))\n",
    "ConfusionMatrixDisplay.from_predictions(df_val.label.values,df_val.predicted_label.values,\n",
    "                                        display_labels=['Positive','Negative'],xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f4098",
   "metadata": {},
   "source": [
    "## Most confident prediction: right vs wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c842659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prob_match(row):\n",
    "    l = row['label']\n",
    "    prob = row['predicted_probability']\n",
    "    return prob[l]\n",
    "def _get_prob_mismatch(row):\n",
    "    prob = row['predicted_probability']\n",
    "    return np.max(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d1e4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['correct_confidence'] = df_val.loc[df_val.label==df_val.predicted_label].apply(_get_prob_match,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e765bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['incorrect_confidence'] = df_val.loc[df_val.label!=df_val.predicted_label].apply(_get_prob_mismatch,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f68965e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correct_pred(df,cols=['text','label_str','correct_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('correct_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(tmp[1:])\n",
    "        print('-'*100)\n",
    "def analyze_incorrect_pred(df,cols=['text','label_str','predicted_label_str','incorrect_confidence'],ascending=False,top_n=5):\n",
    "    raw_tmp = df.sort_values('incorrect_confidence',ascending=ascending)[cols].head(top_n).values\n",
    "    for tmp in raw_tmp:\n",
    "        print(' | '.join(tmp[0]))\n",
    "        print(f'True label: {tmp[1]}, but predict {tmp[2]}, with confidence {tmp[3]}')\n",
    "        print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45aa524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mua | ao | mua | bo_shop | giao | ao | mua | doi | LAM | sao | DANH_Giá | the | nao | day\n",
      "['Negative' 0.5087628960609436]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bông | mềm | hơn | nhưng | cũng | mỏng | hơn | đợt | trước | dùng\n",
      "['Positive' 0.5093692541122437]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ghi | date | T2 | / | 2019 | nên | mới | đặt_hàng | đến | 5 | gói | nhận | hàng | thì | T1 | / | 2019 | thực_phẩm | cho | bé | nên | bán | đúng_hạn | sử_dụng | giới_thiệu | để | khách | tin_tưởng | & | quyết_định | mua | hàng\n",
      "['Negative' 0.5200026035308838]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pin | tụt | nhanh | nhưng | không | bị | sập | nguồn\n",
      "['Positive' 0.5242894887924194]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mở_ra | có | mỗi | vỏ | ko | thấy | kính | cường_lực | ở | trong\n",
      "['Negative' 0.5256175994873047]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nhìn | k | chắc_chắn | lắm\n",
      "['Positive' 0.5285817980766296]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vừa | mới | mua | sáng | nay | thì | 333.000 | VND | giờ | vào | nhìn | lại | thấy | 189.000 | VND | ! | ! | ! | !\n",
      "['Negative' 0.5287574529647827]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | . | Tuy_nhiên | giao | thiếu | số_lượng | . | Mình | đặt | 2 | thanh_toán | 2 | nhận | được | chỉ | có | 1 | cái\n",
      "['Positive' 0.5362181067466736]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hàng | dùng | tạm | được | tuy_nhiên | hơi | lỏng_lẻo | . | K | chắc_chắn | lắm | .\n",
      "['Positive' 0.538158655166626]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Giao | hàng | lâu | ơi | là | lâu | chắc | do | trả | tiền | trước | : | (\n",
      "['Positive' 0.5443349480628967]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | giao | hàng | kh | giống | màu\n",
      "['Negative' 0.5501789450645447]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dao | hàng | lâu | đặt | hẳn | 2 | lần | mới | dao | gần | tháng | mới | có | hàng | ....\n",
      "['Negative' 0.5585803389549255]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sản_phẩm | ok | tốc_độ | không | dc | như | mong_đợi\n",
      "['Positive' 0.5587018132209778]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bột | mịn | hửi | thì | có | mùi | thơm | nhưng | khi | dùng | thì | không | đậm | mùi | đậu_nành | bao_bì | in | mờ_nhạt\n",
      "['Negative' 0.5603232979774475]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng | góii | đuọc\n",
      "['Negative' 0.5685207843780518]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vải | khá | mỏng | . | Tiền | nào | của | nấy | . | Nói_chung | hơn | giày_bata | một | tý | .\n",
      "['Negative' 0.5702218413352966]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Trước_mắt | thì | thấy | oke | rồi | đó | cục | này | chọi | trâu | cũng | chết | 😂_😂_😂\n",
      "['Positive' 0.5918132066726685]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vượt | ngoài | mong_đợi\n",
      "['Positive' 0.5925284624099731]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mình | mua | 4 | chai | trong | đó | có | 2 | chai | lưng | KHG | đủ | dung_tích\n",
      "['Negative' 0.595950722694397]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Minh_dung | sp | nay | gan | 10 | nam | roi | do | sp | rat | tot | ..\n",
      "['Positive' 0.5961607694625854]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_correct_pred(df_val,ascending=True,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6c3381f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". | Hàng | rất | đẹp | <3 | !\n",
      "True label: Negative, but predict Positive, with confidence 0.9974151849746704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tam | duoc | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn\n",
      "True label: Negative, but predict Positive, with confidence 0.997407853603363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | sản_phẩm | tuyệt_vời | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | Shop | phục_vụ | rất | tốt | Thời_gian | giao | hàng | chậm | Rất | đáng | tiền | Thời_gian | giao | hàng | chậm\n",
      "True label: Negative, but predict Positive, with confidence 0.997378945350647\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | sản_phẩm | tuyệt_vời | nhưng | toàn | chữ | Trung_Quốc | .\n",
      "True label: Negative, but predict Positive, with confidence 0.997368574142456\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hàng | y | hình | đẹp | sang_trọng\n",
      "True label: Negative, but predict Positive, with confidence 0.9973480701446533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "chat | lượng | rất | tốt | ok\n",
      "True label: Negative, but predict Positive, with confidence 0.9973419308662415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tốt | dong | goi | dep | nhung | the | nho | bi | hong | không | dùng | duoc | ngoài | Ra | OK | 👌\n",
      "True label: Negative, but predict Positive, with confidence 0.9972805976867676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "😡 | 😡\n",
      "True label: Negative, but predict Positive, with confidence 0.9972085356712341\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | phục_vụ | rất | tốt | Thời_gian | giao | hàng | rất | nhanh | tiền | nào | thì | của | đó\n",
      "True label: Negative, but predict Positive, with confidence 0.9970918893814087\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Take | wrong | color\n",
      "True label: Negative, but predict Positive, with confidence 0.9969770908355713\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ốp | hơi | cũ_shop | nha | .\n",
      "True label: Negative, but predict Positive, with confidence 0.9968775510787964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Màu | quá | ghê | huhu\n",
      "True label: Negative, but predict Positive, with confidence 0.9968618154525757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | rất | nhanh | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | . | Trừ | thẳng | 3_* | vì | đội_giá | lên | quá | cao | so | với | thực_tế\n",
      "True label: Negative, but predict Positive, with confidence 0.9968488812446594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn | . | Hàng | y_ảnh | 100_% | mẫu_mã | đẹp | cầm | chắc | tay | nhỏ | gọn | mk | thấy | khá | tiện | . | ( | sửa | bài | khi | sd | : | sạc | chậm | nhah | tụt | pin | k | hài | lòg | )\n",
      "True label: Negative, but predict Positive, with confidence 0.9968122839927673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | rất | nhanh\n",
      "True label: Negative, but predict Positive, with confidence 0.9967789053916931\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nhưlon | . | 😌\n",
      "True label: Negative, but predict Positive, with confidence 0.9966498017311096\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cũng | đẹp | nhưng | may | lời\n",
      "True label: Negative, but predict Positive, with confidence 0.9964595437049866\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | hàng | cẩn_thận | Sản_phẩm | ít | bọt | rửa | xong | thấy | da_khô | căng | . | Nên | dùng | thêm | với | xịt | khoáng | để | dưỡng | ẩm\n",
      "True label: Negative, but predict Positive, with confidence 0.9962769150733948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lừa | nhau | à | shop\n",
      "True label: Negative, but predict Positive, with confidence 0.9957062602043152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | sản_phẩm | rất | kem | moi | mua | co | may | ngay | Ma_di_khong | dung | gio | moi | nguoi | dung | co | mua | hang | cua_shop | nua\n",
      "True label: Negative, but predict Positive, with confidence 0.9955772161483765\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=False,top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "737caf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khẩu_vị | của | mình | thì | hơi | nhạt | và | thiếu | cay | :)\n",
      "True label: Positive, but predict Negative, with confidence 0.5006212592124939\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đóng_gói | đẹp | . | Nhưng | pin | ko | bền | . | Nạp | sạc | dự_phòng | cả | đêm | mới | đầy | mà | Sạc | chỉ | dc | 1 | lần | là | hết | . | - | _ | -\n",
      "True label: Positive, but predict Negative, with confidence 0.5072213411331177\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chất_lượng | của | sản_phẩm | phù_hợp | với | giá | tiền | . | Có_thể | hãng | cần | nghiên_cứu | sao | cho | cục | sạc | ít | nóng | trong | quá_trình | sạc | vì | hiện_nay | khi | sạc | cục | sạc | rất | nóng | nên | tâm_lý | người | sử_dụng | sợ | nó | cháy | nổ | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5155402421951294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "100 | k | thì | vậy | là | tốt | rồi | ... | mang | đi | chơi | cũng | đẹp | phết | ... | đừng | hoạt_động | mạnh | thì | chắc | không | sao | ... | dùng | được | 2 | tuần | rồi | , | giặc | luôn | rồi | .... | ỔN | ! | ! | Có | cái | là | đến | khi | shiper | đến | cổng | mới | điện_thoại | chứ | chả | báo | trước | gì | cả | ? | ?\n",
      "True label: Positive, but predict Negative, with confidence 0.5188581943511963\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SHOP | ĐĂNG | ẢNH | MỘT_ĐẰNG | SẢN_PHẨM | MỘT | LẺO | ._BÁN | CÁP | THÌ | ÚP | HÌNH | CÁI | CÁP | THÔI | CHỨ | UP | CẢ | CỤC | SẠC | GÂY | NHẦM_LẪN | CHO | KHÁCH_HÀNG | LÀ | COMBO_CÁP | + | DÂY | SẠC | . | TÔI | MUA | LIỀN | 2 | CÁI | LÚC | NHẬN | HÀNG | MỚI | NGỚ | NGƯỜI | RA | BỊ | MUA | ĐẮT | . | CHẤT_LƯỢNG | SẢN_PHẨM | CHƯA | RÕ | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5189838409423828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | chậm | . | doi | nhan | hang | sốt | ca | ruột\n",
      "True label: Negative, but predict Positive, with confidence 0.5209054946899414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Có | ai | dùng | 6 | s | lên | 9.2.1 | lại | nhanh | hết | pin | như | tôi | không | . | Nhanh | hết | pin | quá\n",
      "True label: Negative, but predict Positive, with confidence 0.523202657699585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mình | không | dám | dùng | vì | thấy | mã_vạch | 69 | : | ' | ( | . | Tại | bản_thân | chưa | tìm_hiểu | kĩ | nên | cũng | mua | cho | đủ | giá | free | ship\n",
      "True label: Positive, but predict Negative, with confidence 0.5317555069923401\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Được | co | Thể | mua | lâu | dai\n",
      "True label: Positive, but predict Negative, with confidence 0.5340108871459961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đã | chát | thống_nhất | loại | hàng | nhưng | shop | vẫn | gửi | hàng | ngẫu_nhiên | . | Ko | sử_dụng | đc | chán\n",
      "True label: Negative, but predict Positive, with confidence 0.5393141508102417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thời_gian | giao | hàng | rất | nhanh | Màu_sắc | không | đúng | dù | mình | đã | dặn | trước\n",
      "True label: Positive, but predict Negative, with confidence 0.5393228530883789\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Quạt | mới | mua | nhưng | không | quạt | được\n",
      "True label: Negative, but predict Positive, with confidence 0.5425977110862732\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hơi | rộng | so | vs | size | bình_thường | . | Hàng | i | hình\n",
      "True label: Positive, but predict Negative, with confidence 0.5448225736618042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Đặt | có | 8 | mon | à | giao | thiếu | 2 | món | rồi | làm_ăn | thật | quá_đáng | mà\n",
      "True label: Negative, but predict Positive, with confidence 0.5459489822387695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shop | oi | mua | nhiều | hơn | sao | lại | tiền | ship | đắt | hơn\n",
      "True label: Positive, but predict Negative, with confidence 0.5503779649734497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Nhạt_nhẽo | chỉ | có | mỗi | bánh_phở | dai | lạt | và | nước | gia_vị | trong_veo | . | Ăn | hao_hao | hủ_tiếu | luộc | với | nước | Knor | .\n",
      "True label: Negative, but predict Positive, with confidence 0.5527167916297913\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hàng | nhái | . | From | giầy | kg | đẹp | . | Phù_hợp | với | giá | tiền | đã | giảm | . | Chứ | kg | như | hàng | quãng | cáo | . | Đóng_gói | sản_phẩm | rất | đẹp | và | chắc_chắn\n",
      "True label: Positive, but predict Negative, with confidence 0.5576053261756897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Như | shit | ngta | vậy | á | .._Hg | làm | được | trò | chống | gì | hit\n",
      "True label: Negative, but predict Positive, with confidence 0.5576322674751282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "khó | uống | mình | cứ | ngỡ | là | đang | uống | và | đắp | bột | nghệ | mà | mình | mua | về | để | nấu_ăn | 😭\n",
      "True label: Negative, but predict Positive, with confidence 0.5652170181274414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Em | đã | mua | giày | mà | em | không | thấy | dây | giày | đâu | hết | shop | ạ\n",
      "True label: Negative, but predict Positive, with confidence 0.5652967691421509\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "analyze_incorrect_pred(df_val,ascending=True,top_n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "10px",
    "top": "150px",
    "width": "196px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
