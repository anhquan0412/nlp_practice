{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Language-model\" data-toc-modified-id=\"Language-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Language model</a></span></li><li><span><a href=\"#Machine-translation\" data-toc-modified-id=\"Machine-translation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Machine translation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Statistical-machine-translation\" data-toc-modified-id=\"Statistical-machine-translation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Statistical machine translation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learn-the-translation-model\" data-toc-modified-id=\"Learn-the-translation-model-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Learn the translation model</a></span></li><li><span><a href=\"#How-to-compute-argmax?\" data-toc-modified-id=\"How-to-compute-argmax?-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>How to compute argmax?</a></span></li><li><span><a href=\"#Cons-of-statistical-machine-translation\" data-toc-modified-id=\"Cons-of-statistical-machine-translation-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Cons of statistical machine translation</a></span></li></ul></li><li><span><a href=\"#Neural-machine-translation\" data-toc-modified-id=\"Neural-machine-translation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Neural machine translation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Definition</a></span></li><li><span><a href=\"#A-CONDITIONAL-LANGUAGE-MODEL\" data-toc-modified-id=\"A-CONDITIONAL-LANGUAGE-MODEL-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>A CONDITIONAL LANGUAGE MODEL</a></span></li><li><span><a href=\"#How-to-train-these-2-RNNs\" data-toc-modified-id=\"How-to-train-these-2-RNNs-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>How to train these 2 RNNs</a></span></li><li><span><a href=\"#Beam-search-for-neural-translation-inference-(test-time)\" data-toc-modified-id=\"Beam-search-for-neural-translation-inference-(test-time)-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Beam search for neural translation inference (test time)</a></span></li><li><span><a href=\"#Stopping-criterion-for-beam-search-+-pick-the-best-hypotheses\" data-toc-modified-id=\"Stopping-criterion-for-beam-search-+-pick-the-best-hypotheses-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Stopping criterion for beam search + pick the best hypotheses</a></span></li></ul></li><li><span><a href=\"#Pros-and-cons-of-neural-MT-over-statistical-MT\" data-toc-modified-id=\"Pros-and-cons-of-neural-MT-over-statistical-MT-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Pros and cons of neural MT over statistical MT</a></span></li></ul></li><li><span><a href=\"#Attention\" data-toc-modified-id=\"Attention-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Definition</a></span></li><li><span><a href=\"#Steps\" data-toc-modified-id=\"Steps-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Steps</a></span></li><li><span><a href=\"#math-equation\" data-toc-modified-id=\"math-equation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>math equation</a></span></li><li><span><a href=\"#Pros-of-using-attention\" data-toc-modified-id=\"Pros-of-using-attention-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Pros of using attention</a></span></li><li><span><a href=\"#General-definition-of-attention\" data-toc-modified-id=\"General-definition-of-attention-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>General definition of attention</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learn the translation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- From parallel corpus/data (same text with multiple translations)\n",
    "- Break down P(x|y) to P(x,a|y), with a is **alignment** (correspondence b/t French sentence x and English sentence y, at word level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Alignment can be many-to-one (>1 words in E are represented by 1 word in F) or one-to-many (often called fertile word), for example 1 single F word **entarte** means: **hit someone with a pie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Even many-to-many translation (phrase-level)\n",
    "\n",
    "![](images/translation_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How to compute argmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example of a decoding approach:\n",
    "https://youtu.be/XXtpJxZBa2c?t=830"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cons of statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do machine translation with a single neural network\n",
    "\n",
    "The **architecture** is called: **sequence-to-sequence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other uses of sequence-to-sequence (beside NMT):\n",
    "- Summarization (long text -> short text)\n",
    "- Dialogue (previous utterances -> next utterance)\n",
    "- Parsing (input text -> output parse as sentence)\n",
    "- Code generation (natural language -> Python code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This is test time behavior**. We don't know how to train this yet\n",
    "- The final hidden state of encoder RNN (orange box) become the initial hidden state of decoder RNN\n",
    "- We fed ENGLISH WORD EMBEDDINGS to encoder RNN, and FRENCH WORD EMBEDDINGS to decoder RNN. Aka we have 2 different sets of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### A CONDITIONAL LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train these 2 RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The encoder RNN is just a language model (predicting the next English word), but there are no loss yet.\n",
    "\n",
    "- For decoder RNN: use the final hidden state of encoder RNN to initialize the hidden state, then train it as if it's a language model, on French words (with losses and all)\n",
    "\n",
    "![](images/translation_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **End-to-end backprop**: so BOTH the hidden state of decoder RNN and encoder RNN are updated during training. Even further, you can unfreeze the word embeddings of the 2 systems and finetune them as well.\n",
    "\n",
    "- Though of course you can use pretraining language models for encoder RNN and/or decoder RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Beam search for neural translation inference (test time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example: https://youtu.be/XXtpJxZBa2c?t=2132\n",
    "\n",
    "A finished beam search with 2 hypotheses with same length (k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that normally beam search should result in k hypotheses (from beam search), and each hypotheses ideally will have same size. That's why we don't  normalize the score for each of them (see more below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stopping criterion for beam search + pick the best hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since now we have specific stop criterions, thus considered hypotheses won't have same length anymore\n",
    "\n",
    "![](images/translation_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pros and cons of neural MT over statistical MT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/translation_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Disadvantages:\n",
    "- less interpretable\n",
    "    - hard to interpret the neurons and weights of the RNN\n",
    "    - in contrast NMT has subcomponents, which is understandable since human design them\n",
    "- hard to control\n",
    "    - hard to put or reinforce hard-coded linguistic rule\n",
    "    - hard to put safety guidelines (safety concern: controversial translation, swear words ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with seq2seq model when you only use the final hidden state vector of encoder RNN to feed in decoder RNN, aka the **information bottleneck**\n",
    "\n",
    "![](images/translation_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On each step of the decoder, use **direct connection to the encoder** to **focus on particular part** of **source sentence**\n",
    "- More formal: a content-based, memory retrieval mechanism\n",
    "    - You have a vector in decoder to attend to all this content ( representations/hidden states) from your encoder to decide what information to absorb, based on how similar this content is to every position in the memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why 'dot product'? Since hidden state of decoder tries to predict next English word, it will contain all the information to do so, and **with a high dot product (high similarities based on cosine similarity formula)** of that hidden state to the hidden state of encoder (of some French word), **it will say: \"oh this English word I am trying to predict might be highly associated with this French word\"**\n",
    "- note that dot product is **basic** to calculate the attention scores, which assumes 2 vectors have the same size (hidden states of encoder and decoder)\n",
    "- there is also **multiplicative attention** way to calculate the scores\n",
    "    - use a learnable weight matrix to learn the best way to calculate those scores\n",
    "    ![](images/translation_20.png)\n",
    "- also, **additive attention**, see more here: https://youtu.be/XXtpJxZBa2c?t=4517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...continue to attention steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## math equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last step, **'proceed as in the non-attention seq2seq model'** = basically treat the [at;st] as the concat[embedding,old_h] => orange arrow (matmul weight + tanh + dropout) => blue arrow (matmul weight + softmax) to have vocab probability and do cross entropy loss on actual y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](s4/translation_attention_archi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros of using attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Solving the bottleneck problem**: instead of counting on only last hidden state of encoder RNN, **with attention, the decoder RNN can look at all the hidden states of encoder and make decision on which to focus on** (based on attention distribution)\n",
    "\n",
    "- **direct connection between encoder and decoder**, similar to shortcut connection (skip connection): as hidden state of decoder will connect to multiple hs of encoders => lots of connection => gradient flowing back easier since there is no bottleneck => **reduce vanishing gradient**\n",
    "\n",
    "- **a sense of interpretability** by looking at the attention distribution for each word. Similar to hard alignment of SMT, but this time the neural net learns the alignment by itself. TODO: reproduce this\n",
    "\n",
    "![](images/translation_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General definition of attention\n",
    "\n",
    "https://youtu.be/XXtpJxZBa2c?t=4273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaning the previous img\n",
    "\n",
    "- The query determines which values to focus on and HOW MUCH it is going to select from the values, which results in the weighted sum (weighted sum = sum(Wi * Xi) with W is the weights and X is the values)\n",
    "- Attention will turn an artibtary set of representations (like 100 values, 1000 values ...) into 1 single fixed-sized representation which summarizes them (1 single vector after the weighted sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/translation_23.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "237.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
