{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What is Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To FIND and CLASSIFY names in text\n",
    "\n",
    "![](images/ner_intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A hard task because\n",
    "- An official name can sound like typical words: \"First National Bank\", \"Future School\"\n",
    "- Hard to determine class of entity: \"Charles Schwab\" can be referred to a person or an organization\n",
    "\n",
    "=> ambiguous, dependent on context\n",
    "\n",
    "Overall, single word classification is always hard, since words can have double meaning, such as \"sanction\" or \"Paris\" (Paris city, or Paris Hilton?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Window classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use a window containing neighboring words to classify the center word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Steps:\n",
    "- Use word2vector to convert each word in a FIXED SIZE window to a vector\n",
    "- Concatenate those vectors. x_window will be a vector of size (windows size * word vector length,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/ner_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Binary classifier with unnormalized score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Still use a fixed window and concat vector x_window size size (windows size * word vector length,)\n",
    "\n",
    "Task: classify whether center word is a location or not (binary classification)\n",
    "- Will go over all positions in corpus, but only correct position will get high score. Correct position = position that has actual NER Location at center\n",
    "\n",
    "E.g.: Not all museums in Paris are amazing\n",
    "- One true window: 'museums in **Paris** are amazing'\n",
    "- Wrong window: 'all museums **in** Paris are'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Neural net feed forward for this architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assuming word-vector-length is 4. We will have 3-layer neural net as described below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/ner_neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "a is R(8x1). Output of s is R(2x1), or binary outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Cons of updating word vectors using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we are doing binary classification for movie review for example, and feeding a pre-trained w2v to start with, some words representation will be updated (thanks to SGD) in a way that break the original word relationship\n",
    "- Words like: \"TV\",\"telly\",\"television\" are close to each other, but if 'television' is in test set, it will be further away from 'TV' and 'telly' in train set which got updated. Things will got worst if TV and telly are key words to determine negativity of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/ner_updatew2v_or_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: \n",
    "- use transfer learning (w2v pre-trained?) Yes, but if we already have big corpuses around +100 millions of words of data (probably from machine translation tasks), then I guess it's fine to start with random initialization of word vectors\n",
    "\n",
    "about fine-tuning w2v pre-trained: \n",
    "- if small data (~ 100k words) => don't, since small data is easy to overfit, can't generalize well\n",
    "- large dataset (>1 mil words): yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dependency_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dependency_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dependency_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dependency_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dependency_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/dependency_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy transition-based parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dependency_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dependency_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are only 3 actions: shift (pop the buffer and push to stack), left arc (reduce the top stack's left word, and top stack's word -> second to top's word in term of dependency), right arc (reduce the top stack itself, second to top -> top)\n",
    "\n",
    "Finish when buffer is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use machine learning to predict action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joakim Nirve idea (**MaltParser**): build a machine learning classifier to predict next action\n",
    "\n",
    "Each action is predicted by a discriminative classifier (e.g.,\n",
    "softmax classifier) over each legal move\n",
    "\n",
    "- Max of 3 untyped choices; max of |R| × 2 + 1 when typed\n",
    "- Features: top of stack word, POS; first in buffer word, POS; etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros: \n",
    "- **There is NO search (in the simplest form).** But you can profitably do a beam search if you wish (slower but better). You keep k good parse prefixes at each time step\n",
    "- The model’s accuracy is fractionally below the state of the art in dependency parsing\n",
    "- It provides very fast linear time parsing, with great performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dependency_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dependency_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have\n",
    "- first word of stack (d dimensional vector)\n",
    "- POS of the first word (d dimensional vector)\n",
    "- dependency label of that word(d dimensional vector): this is a small set and you can extract similarities from them\n",
    "\n",
    "Then concat them.\n",
    "\n",
    "This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dependency_11.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
