{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword models: models below the word level\n",
    "- In some languages, build word embedding models will be hard b/c of large and open vocabulary\n",
    "    - Rich morphology (lots of prefixes and suffixes)\n",
    "    - New slangs/informal words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Generates embeddings for unknown words\n",
    "- Similar spellings share similar embeddings\n",
    "- Solves out-of-vocabulary problem\n",
    "- **Comparable results versus word-level model with FEWER PARAMETERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Comparing char model with BPE model (seq2seq machine translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/subword_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(2 graphs on left side) Char model works better for rich morphology language such as Czech rather than French, though the gain is small if you use a more complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-word models: two trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same architecture as for word-level model:\n",
    "    - But use smaller units: “word pieces”\n",
    "    - [Sennrich, Haddow, Birch, ACL’16a],[Chung, Cho, Bengio, ACL’16].\n",
    "    - BPE\n",
    "- Hybrid architectures:\n",
    "    - Main model has words; something else for characters\n",
    "    - [Costa-Jussà & Fonollosa, ACL’16], [Luong & Manning, ACL’16]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A word segmentation algorithm:\n",
    "    - Though done as bottom up clustering\n",
    "    - Start with a unigram vocabulary of all (Unicode) characters in data\n",
    "    - Most frequent ngram pairs ↦ a new ngram\n",
    "    - Basically the SentencePieceTokenizer in fastai2 library (which is from Google's SentencePiece library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Automatically decides vocab for system**\n",
    "- No longer strongly “word” based in conventional way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with all characters in vocab\n",
    "- Add a pair (e, s) with freq 9\n",
    "- Add a pair (es, t) with freq 9\n",
    "- Add a pair (l, o) with freq 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencepiece model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentencepiece model works from raw text\n",
    "- Whitespace is retained as special token (_) and grouped normally\n",
    "- You can reverse things at end by joining pieces and recoding them to spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: \"discover discovering discovered disco disc disk discord disconnect disconnected disconneting\"\n",
    "\n",
    "```python\n",
    "subword(1000)\n",
    "\n",
    "['▁dis', 'co', 'ver', '▁dis', 'co', 'ver', 'ing', '▁dis', 'co', 'ver', 'ed', '▁dis', 'co', '▁dis', 'c', '▁dis', 'k', '▁dis', 'c', 'or', 'd', '▁dis', 'c', 'on', 'n', 'ect', '▁dis', 'c', 'on', 'n', 'ect', 'ed', '▁dis', 'c', 'on', 'ne', 'ting']\n",
    "\n",
    "# (note that these are not vocab, these are the original text got tokenized using BPE)\n",
    "\n",
    "\n",
    "subword(10000)\n",
    "\n",
    "['▁discover', '▁discover', 'ing', '▁discovered', '▁disco', '▁disc', '▁dis', 'k', '▁disco', 'rd', '▁disco', 'n', 'n', 'ect', '▁disco', 'n', 'n', 'ect', 'ed', '▁disco', 'nne', 'ting']\n",
    "\n",
    "```\n",
    "\"Picking a subword vocab size represents a compromise: a larger vocab means fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordpiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wordpiece model tokenizes inside words\n",
    "\n",
    "\n",
    "-  BERT uses a variant of the wordpiece model\n",
    "    - (Relatively) common words are in the vocabulary: at, fairfax, 1910s\n",
    "- Other words are built from wordpieces:\n",
    "    - hypatia = h ##yp ##ati ##a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use character-level model for word-level model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cnn + maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/novel_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run **character-level** biLSTM and concatenate 2 final hidden states (known as 'outward representation')\n",
    "- Use that representation for another LSTM LM model that works along **sequence of words** (TODO: need details on this)\n",
    "- Train the whole thing **end-to-end** to **update character embeddings** that **can produce good words vector** (TODO: how?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Aware Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/9oTHFx0Gg3Q?t=3280\n",
    "\n",
    "http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture12-subwords.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word similarity between char-level word embedding and word-level word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LSTM-Char (b4 highway): since we initially doing conv layer + maxpool, the word embedding results from char-level model still remember things about character\n",
    "- All words with 'le' or 'ile' (conv filter size 2 or 3) are similar to 'while'\n",
    "- Words start with 'rich' are similar to 'richard', but they are not necessarily names\n",
    "\n",
    "But for LSTM-Char after highway), things are a bit better\n",
    "- Even with 'richard', the similar names coming out of this model are still name, unlike the b4 highway one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For out-of-vocabulary similarity (note that LSTM-Word can't handle OOV problem, hence the first blank row)\n",
    "\n",
    "![](images/subword_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining word-level and character-level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A best-of-both-worlds architecture:\n",
    "    - Translate mostly at the word level\n",
    "    - Only go to the character level when needed (unknown words - words not in vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/9oTHFx0Gg3Q?t=3753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that e.g. at UNK output, there will be 2 losses, 1 at the word-level and 1 at the small upper LSTM char-level\n",
    "\n",
    "At UNK, We feed the hidden representation of the word-level model as starting hidden representation of the char-level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_8.png)\n",
    "\n",
    "- by using the current hidden representation, the char-level model don't have much representation further back => not good at capturing context for accurate translation such as names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aim: a next generation efficient word2vec-like word representation library, but **better for rare words and languages with lots of morphology**\n",
    "- An extension of the w2v skip-gram model with character n-grams\n",
    "\n",
    "https://youtu.be/9oTHFx0Gg3Q?t=4330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/subword_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A word will be broken down using n-grams, for example the word 'where' will be represented by 6 tokens. Note that \"<\" and \">\" are boundary symbols\n",
    "- Using the same w2v mechanism, calculate *similarity* by doing the **dot product between context (neighbor) vector and center-word vector, but now the center-word vector will correspond all 6 of those tokens' vector**. We will sum them together (there's a hashing trick for this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
