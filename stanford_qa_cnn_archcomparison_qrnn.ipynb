{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Lecture: https://www.youtube.com/watch?v=yIdF-17HwSk&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=11&t=0s\n",
    "\n",
    "- Slide: http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture10-QA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/question_answering_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- SQUAD and SQUAD2.0 dataset (question answering) and its limitation (span answer)\n",
    "- Stanford Attentive Reader (SAR) and SAR++\n",
    "- BiDAF from AI institute (self attention?)\n",
    "- So many way to calculate and use attention\n",
    "- FusionNet: combine all those attention usages and stack 5 of them together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slide: http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture11-convnets.pdf\n",
    "\n",
    "Video: https://youtu.be/EAJoRA0KX7I?t=1407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why RNN to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN: \n",
    "    - have to go through the **entire sequence** (e.g. the entire sentence in classification task) **from one end to another** for a final vector (hidden vector h), and **normally capture too much of last words inside this vector** (assuming no LSTM or attention tweak)\n",
    "    - RNN always go through prefix context before getting to a subsequent short phrases, e.g. the country of my birth ..., **RNN cannot capture the phrase \"my birth\" independently without considering \"the country of\"**\n",
    "    \n",
    "    => No explicit modeling of long and short range dependencies\n",
    "    \n",
    "- CNN: can compute \"vectors\" for every possible \"word subsequence\" of a certain length\n",
    "    - e.g: for the example above: the country, country of, of my, my birth ...\n",
    "    - Regardless of whether phrase is grammatical\n",
    "    - Exploits local dependencies (short-range)\n",
    "    - However, long-distance dependencies require many layers (similarly to CNN in computer vision, deeper is better at capture \"big picture\" information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add padding 1 (start word and end word with embedding 0s) to maintain number of outputs (# of rows) and add more filters (2 more) to increase dimension of output matrix (# of columns)\n",
    "\n",
    "Also if you want to increase number of outputs (# of rows), you can increase padding (padding = 2), aka wide convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary to computer vision, each filter should be able to learn (by SGD) to specialize different thing, such as filter 1 can specialize if a phrase is \"polite\" (produce high value) or \"rude\" (low value)\n",
    "\n",
    "In this sentence, for each phrase, is it in a polite tone and talk about food and blabla\n",
    "\n",
    "=> In a sense, we can summarize the whole sentence with respect to these features (produced by filters), by doing **max pooling** (to be precise: **global max pooling**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the first 2 features is \"polite\" and \"food\", then this whole text is not polite (0.3 politeness) but is talking about food (1.6 food)\n",
    "\n",
    "You can also do an **average pooling** to show \"what is the average amount of politeness in this text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imporant**: Sometimes **max pooling** is better, because of the nature of natural language:\n",
    "- signals in language are often sparse (you will express politeness in some words, not every word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local max pooling (similar to CNN: using another filter (pool) to compute max pooling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for each column, you can just extract the max k values, in order. \n",
    "\n",
    "Global max pooling jus just k-max pooling with k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of convolution: dilate convolution\n",
    "- add a second cnn layer by using a second cnn filter after previous padding 1 stride 1 kernel size 3 filter\n",
    "- help see a bigger spread of the sentence without having too many parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple cnn architecture for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: In the paper, the number of each region sizes (2,3,4) is 100 instead of 2\n",
    "- Also he used 'multi channel input' idea of\n",
    "    - Initialize with pre-trained word vectors (word2vec or Glove)\n",
    "    - **Start with two copies**\n",
    "    - **Backprop into only one set, keep other “static”** => keep both the original version and the updated version, in which the idea is somewhat similar to **skip connection** (in skip connection you add the original and the processed output)\n",
    "    - Both channel sets are added to ci before max-pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advices on using different architectures for NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNN on character-level to build word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used in assignment 5 to build better machine translation model\n",
    "\n",
    "TODO: remove! (move to submodel notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/novel_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep CNN model for NLP classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each convo block contains 2 convo sub-block, each sub-block contain (in order)\n",
    "- Conv layer: kernel size 3. Padded to preserve dimension\n",
    "- batchnorm\n",
    "- RELU for nonlinearity\n",
    "\n",
    "For this architecture, skip connection is not inside this block. It should be between each block, as shown in pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/EAJoRA0KX7I?t=4679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QRNN addresses some of the problems CNNs and RNNs have; \n",
    "- convolutions being time invariance and \n",
    "- LSTMs being non-parallelized. \n",
    "\n",
    "We can say that QRNN combines best of two worlds: parallel nature of convolutions and time dependencies of LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cnn_12.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
